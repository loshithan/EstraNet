{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EstraNet Training on ASCADf Dataset\n",
    "\n",
    "This notebook sets up and trains the EstraNet model on the ASCADf (ASCAD with fixed key) dataset.\n",
    "\n",
    "**Paper**: [EstraNet: An Efficient Shift-Invariant Transformer Network for Side-Channel Analysis](https://tches.iacr.org/index.php/TCHES/article/view/11255)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Setup Checklist\n",
    "- âœ… Install dependencies\n",
    "- âœ… Download ASCADf dataset from Google Drive\n",
    "- âœ… Apply TensorFlow 2.13+ compatibility fixes\n",
    "- âœ… Configure training parameters\n",
    "- âœ… Train the model\n",
    "- âœ… Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Environment Setup\n",
    "\n",
    "First, let's check if we're running on Google Colab and set up GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Running on Google Colab\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "ðŸš€ GPU detected! Training will be accelerated.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"ðŸ“ Running on local Jupyter\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"ðŸš€ GPU detected! Training will be accelerated.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Training will be slower on CPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"ðŸ’¡ Enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Clone Repository and Install Dependencies\n",
    "\n",
    "If running on Colab, we need to clone the repository first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EstraNet'...\n",
      "remote: Enumerating objects: 47, done.\u001b[K\n",
      "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 47 (delta 23), reused 40 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (47/47), 24.88 KiB | 8.29 MiB/s, done.\n",
      "Resolving deltas: 100% (23/23), done.\n",
      "âœ… Clean! Directory: /content/EstraNet\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "if os.path.exists('EstraNet'):\n",
    "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
    "    \n",
    "!git clone https://github.com/suvadeep-iitb/EstraNet.git\n",
    "os.chdir('EstraNet')\n",
    "print(f\"âœ… Clean! Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing dependencies...\n",
      "\n",
      "\n",
      "âœ… All dependencies installed!\n",
      "Using TensorFlow 2.19.0 (pre-installed)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "print(\"ðŸ“¦ Installing dependencies...\\n\")\n",
    "# %pip install -q absl-py==2.3.1 numpy==1.24.3 scipy==1.10.1 h5py==3.11.0\n",
    "\n",
    "# Install gdown for downloading from Google Drive\n",
    "%pip install -q gdown\n",
    "\n",
    "# Note: Using TensorFlow version pre-installed in Colab (2.16+ / 2.19+)\n",
    "# The compatibility fixes in Section 3 work with all TensorFlow 2.13+ versions\n",
    "print(\"\\nâœ… All dependencies installed!\")\n",
    "print(f\"Using TensorFlow {tf.__version__} (pre-installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Apply TensorFlow 2.13+ Compatibility Fixes\n",
    "\n",
    "âš ï¸ **IMPORTANT: You MUST run this cell before training!**\n",
    "\n",
    "The original code has compatibility issues with TensorFlow 2.13+. This cell fixes them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Applying ALL TensorFlow 2.13+ compatibility fixes...\n",
      "\n",
      "ðŸ“ Fixing transformer.py...\n",
      "  âœ… transformer.py fixed!\n",
      "\n",
      "ðŸ“ Fixing train_trans.py...\n",
      "  âœ… train_trans.py fixed!\n",
      "\n",
      "ðŸ“ Fixing fast_attention.py...\n",
      "  âœ… fast_attention.py fixed!\n",
      "\n",
      "ðŸš€ ALL FIXES APPLIED! Run training now.\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”§ Applying ALL TensorFlow 2.13+ compatibility fixes...\\n\")\n",
    "\n",
    "# FIX 1: transformer.py\n",
    "print(\"ðŸ“ Fixing transformer.py...\")\n",
    "with open('transformer.py', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "content = content.replace(\n",
    "    'def call(self, inp, softmax_attn_smoothing=1, training=False):',\n",
    "    'def call(self, inputs, softmax_attn_smoothing=1, training=False):'\n",
    ")\n",
    "content = content.replace(\n",
    "    'inp = tf.expand_dims(inp, axis=-1)',\n",
    "    'inp = tf.expand_dims(inputs, axis=-1)',\n",
    "    1\n",
    ")\n",
    "content = content.replace(\n",
    "    'pos_ft, pos_ft_slopes = self.pos_feature(slen, bsz)',\n",
    "    'pos_ft, pos_ft_slopes = self.pos_feature(slen=slen, bsz=bsz)'\n",
    ")\n",
    "content = content.replace(\n",
    "    'from tensorflow.keras.layers.experimental import SyncBatchNormalization',\n",
    "    'from tensorflow.keras.layers import BatchNormalization as SyncBatchNormalization'\n",
    ")\n",
    "content = content.replace('if l is 0 else', 'if l == 0 else')\n",
    "\n",
    "# FIX DIVISION BY ZERO\n",
    "content = content.replace(\n",
    "    'normalized_slopes = (1. / float(slen-1)) * self.slopes',\n",
    "    'normalized_slopes = (1. / max(float(slen-1), 1.0)) * self.slopes'\n",
    ")\n",
    "\n",
    "with open('transformer.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  âœ… transformer.py fixed!\")\n",
    "\n",
    "# FIX 2: train_trans.py\n",
    "print(\"\\nðŸ“ Fixing train_trans.py...\")\n",
    "with open('train_trans.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "content = content.replace('.reset_states()', '.reset_state()')\n",
    "content = content.replace(\n",
    "    'logits = model(inps, softmax_attn_smoothing, training=True)[0]',\n",
    "    'logits = model(inputs=inps, softmax_attn_smoothing=softmax_attn_smoothing, training=True)[0]'\n",
    ")\n",
    "content = content.replace(\n",
    "    'logits = model(inps)[0]',\n",
    "    'logits = model(inputs=inps)[0]'\n",
    ")\n",
    "\n",
    "with open('train_trans.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  âœ… train_trans.py fixed!\")\n",
    "\n",
    "# FIX 3: fast_attention.py\n",
    "print(\"\\nðŸ“ Fixing fast_attention.py...\")\n",
    "import re\n",
    "with open('fast_attention.py', 'r') as f:\n",
    "    content = f.read()\n",
    "pattern = r'self\\.add_weight\\(\\s*\"([^\"]+)\"\\s*,'\n",
    "content = re.sub(pattern, r'self.add_weight(name=\"\\1\",', content)\n",
    "with open('fast_attention.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  âœ… fast_attention.py fixed!\")\n",
    "\n",
    "print(\"\\nðŸš€ ALL FIXES APPLIED! Run training now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Download ASCADf Dataset\n",
    "\n",
    "Download the ASCAD dataset with fixed key from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset already exists\n",
      "\n",
      "ðŸ“Š Dataset info:\n",
      "  Keys: ['Attack_traces', 'Profiling_traces']\n",
      "  Profiling traces shape: (50000, 700)\n",
      "  Attack traces shape: (10000, 700)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ASCADf dataset configuration\n",
    "file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
    "DATASET_PATH = \"data/ASCAD.h5\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"ðŸ“¥ Downloading ASCADf dataset from Google Drive...\")\n",
    "    print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
    "    \n",
    "    # Download using gdown\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
    "    \n",
    "    print(\"\\nâœ… Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"âœ… Dataset already exists\")\n",
    "\n",
    "# Verify dataset\n",
    "import h5py\n",
    "with h5py.File(DATASET_PATH, 'r') as f:\n",
    "    print(f\"\\nðŸ“Š Dataset info:\")\n",
    "    print(f\"  Keys: {list(f.keys())}\")\n",
    "    if 'Profiling_traces' in f:\n",
    "        print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
    "    if 'Attack_traces' in f:\n",
    "        print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Configure Training Parameters\n",
    "\n",
    "Set up the training configuration. You can modify these parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Configuration:\n",
      "  Dataset: ASCAD\n",
      "  Input length: 10000\n",
      "  Batch size: 16\n",
      "  Training steps: 4,000,000\n",
      "  Model layers: 2\n",
      "  Model dimension: 128\n",
      "  Attention heads: 8\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data config\n",
    "    'data_path': DATASET_PATH,\n",
    "    'dataset': 'ASCAD',\n",
    "    'input_length': 10000,  # or 40000 for full traces\n",
    "    'data_desync': 200,     # 400 for input_length=40000\n",
    "    \n",
    "    # Training config\n",
    "    'train_batch_size': 16,\n",
    "    'eval_batch_size': 16,\n",
    "    'train_steps': 4000000,\n",
    "    'warmup_steps': 1000000,\n",
    "    'iterations': 20000,\n",
    "    'save_steps': 40000,\n",
    "    \n",
    "    # Optimization config\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'clip': 0.25,\n",
    "    'min_lr_ratio': 0.004,\n",
    "    \n",
    "    # Model architecture\n",
    "    'n_layer': 2,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'n_head': 8,\n",
    "    'd_inner': 256,\n",
    "    'n_head_softmax': 8,\n",
    "    'd_head_softmax': 16,\n",
    "    'dropout': 0.05,\n",
    "    'conv_kernel_size': 3,\n",
    "    'n_conv_layer': 2,\n",
    "    'pool_size': 20,\n",
    "    'd_kernel_map': 512,\n",
    "    'beta_hat_2': 150,\n",
    "    'model_normalization': 'preLC',\n",
    "    'head_initialization': 'forward',\n",
    "    'softmax_attn': True,\n",
    "    \n",
    "    # Checkpoint config\n",
    "    'checkpoint_dir': './',\n",
    "    'result_path': 'results',\n",
    "    'warm_start': False,\n",
    "    'use_tpu': False,\n",
    "    'max_eval_batch': 100,\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ Training Configuration:\")\n",
    "print(f\"  Dataset: {config['dataset']}\")\n",
    "print(f\"  Input length: {config['input_length']}\")\n",
    "print(f\"  Batch size: {config['train_batch_size']}\")\n",
    "print(f\"  Training steps: {config['train_steps']:,}\")\n",
    "print(f\"  Model layers: {config['n_layer']}\")\n",
    "print(f\"  Model dimension: {config['d_model']}\")\n",
    "print(f\"  Attention heads: {config['n_head']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Train the Model\n",
    "\n",
    "Now let's train the EstraNet model. This will take several hours depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training...\n",
      "\n",
      "Arguments: --use_tpu=False --data_path=data/ASCAD.h5 --dataset=ASCAD --checkpoint_dir=./ --warm_start=False --r...\n",
      "\n",
      "2026-02-08 10:30:52.855334: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770546652.873738   53513 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770546652.880355   53513 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770546652.895195   53513 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770546652.895239   53513 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770546652.895243   53513 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770546652.895249   53513 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "INFO:tensorflow:\n",
      "I0208 10:30:58.330739 134503972279424 train_trans.py:382] \n",
      "INFO:tensorflow:\n",
      "I0208 10:30:58.330991 134503972279424 train_trans.py:383] \n",
      "INFO:tensorflow:use_tpu               : False\n",
      "I0208 10:30:58.331094 134503972279424 train_trans.py:384] use_tpu               : False\n",
      "INFO:tensorflow:data_path             : data/ASCAD.h5\n",
      "I0208 10:30:58.331162 134503972279424 train_trans.py:385] data_path             : data/ASCAD.h5\n",
      "INFO:tensorflow:dataset               : ASCAD\n",
      "I0208 10:30:58.331221 134503972279424 train_trans.py:386] dataset               : ASCAD\n",
      "INFO:tensorflow:checkpoint_dir        : ./\n",
      "I0208 10:30:58.331276 134503972279424 train_trans.py:387] checkpoint_dir        : ./\n",
      "INFO:tensorflow:checkpoint_idx        : 0\n",
      "I0208 10:30:58.331329 134503972279424 train_trans.py:388] checkpoint_idx        : 0\n",
      "INFO:tensorflow:warm_start            : False\n",
      "I0208 10:30:58.331379 134503972279424 train_trans.py:389] warm_start            : False\n",
      "INFO:tensorflow:result_path           : results\n",
      "I0208 10:30:58.331427 134503972279424 train_trans.py:390] result_path           : results\n",
      "INFO:tensorflow:do_train              : True\n",
      "I0208 10:30:58.331476 134503972279424 train_trans.py:391] do_train              : True\n",
      "INFO:tensorflow:learning_rate         : 0.00025\n",
      "I0208 10:30:58.331528 134503972279424 train_trans.py:392] learning_rate         : 0.00025\n",
      "INFO:tensorflow:clip                  : 0.25\n",
      "I0208 10:30:58.331580 134503972279424 train_trans.py:393] clip                  : 0.25\n",
      "INFO:tensorflow:min_lr_ratio          : 0.004\n",
      "I0208 10:30:58.331631 134503972279424 train_trans.py:394] min_lr_ratio          : 0.004\n",
      "INFO:tensorflow:warmup_steps          : 1000000\n",
      "I0208 10:30:58.331680 134503972279424 train_trans.py:395] warmup_steps          : 1000000\n",
      "INFO:tensorflow:input_length          : 10000\n",
      "I0208 10:30:58.331728 134503972279424 train_trans.py:396] input_length          : 10000\n",
      "INFO:tensorflow:data_desync           : 200\n",
      "I0208 10:30:58.331777 134503972279424 train_trans.py:397] data_desync           : 200\n",
      "INFO:tensorflow:train_batch_size      : 16\n",
      "I0208 10:30:58.331825 134503972279424 train_trans.py:398] train_batch_size      : 16\n",
      "INFO:tensorflow:eval_batch_size       : 16\n",
      "I0208 10:30:58.331873 134503972279424 train_trans.py:399] eval_batch_size       : 16\n",
      "INFO:tensorflow:train_steps           : 4000000\n",
      "I0208 10:30:58.331921 134503972279424 train_trans.py:400] train_steps           : 4000000\n",
      "INFO:tensorflow:iterations            : 20000\n",
      "I0208 10:30:58.332031 134503972279424 train_trans.py:401] iterations            : 20000\n",
      "INFO:tensorflow:save_steps            : 40000\n",
      "I0208 10:30:58.332087 134503972279424 train_trans.py:402] save_steps            : 40000\n",
      "INFO:tensorflow:n_layer               : 2\n",
      "I0208 10:30:58.332143 134503972279424 train_trans.py:403] n_layer               : 2\n",
      "INFO:tensorflow:d_model               : 128\n",
      "I0208 10:30:58.332199 134503972279424 train_trans.py:404] d_model               : 128\n",
      "INFO:tensorflow:d_head                : 32\n",
      "I0208 10:30:58.332255 134503972279424 train_trans.py:405] d_head                : 32\n",
      "INFO:tensorflow:n_head                : 8\n",
      "I0208 10:30:58.332312 134503972279424 train_trans.py:406] n_head                : 8\n",
      "INFO:tensorflow:d_inner               : 256\n",
      "I0208 10:30:58.332368 134503972279424 train_trans.py:407] d_inner               : 256\n",
      "INFO:tensorflow:n_head_softmax        : 8\n",
      "I0208 10:30:58.332426 134503972279424 train_trans.py:408] n_head_softmax        : 8\n",
      "INFO:tensorflow:d_head_softmax        : 16\n",
      "I0208 10:30:58.332486 134503972279424 train_trans.py:409] d_head_softmax        : 16\n",
      "INFO:tensorflow:dropout               : 0.05\n",
      "I0208 10:30:58.332545 134503972279424 train_trans.py:410] dropout               : 0.05\n",
      "INFO:tensorflow:conv_kernel_size      : 3\n",
      "I0208 10:30:58.332602 134503972279424 train_trans.py:411] conv_kernel_size      : 3\n",
      "INFO:tensorflow:n_conv_layer          : 2\n",
      "I0208 10:30:58.332659 134503972279424 train_trans.py:412] n_conv_layer          : 2\n",
      "INFO:tensorflow:pool_size             : 20\n",
      "I0208 10:30:58.332716 134503972279424 train_trans.py:413] pool_size             : 20\n",
      "INFO:tensorflow:d_kernel_map          : 512\n",
      "I0208 10:30:58.332773 134503972279424 train_trans.py:414] d_kernel_map          : 512\n",
      "INFO:tensorflow:beta_hat_2            : 150\n",
      "I0208 10:30:58.332830 134503972279424 train_trans.py:415] beta_hat_2            : 150\n",
      "INFO:tensorflow:model_normalization   : preLC\n",
      "I0208 10:30:58.332888 134503972279424 train_trans.py:416] model_normalization   : preLC\n",
      "INFO:tensorflow:head_initialization   : forward\n",
      "I0208 10:30:58.333015 134503972279424 train_trans.py:417] head_initialization   : forward\n",
      "INFO:tensorflow:softmax_attn          : True\n",
      "I0208 10:30:58.333085 134503972279424 train_trans.py:418] softmax_attn          : True\n",
      "INFO:tensorflow:max_eval_batch        : 100\n",
      "I0208 10:30:58.333153 134503972279424 train_trans.py:419] max_eval_batch        : 100\n",
      "INFO:tensorflow:output_attn           : False\n",
      "I0208 10:30:58.333216 134503972279424 train_trans.py:420] output_attn           : False\n",
      "INFO:tensorflow:\n",
      "I0208 10:30:58.333267 134503972279424 train_trans.py:421] \n",
      "INFO:tensorflow:\n",
      "I0208 10:30:58.333318 134503972279424 train_trans.py:422] \n",
      "INFO:tensorflow:Number of accelerators: 1\n",
      "I0208 10:31:25.352763 134503972279424 train_trans.py:462] Number of accelerators: 1\n",
      "INFO:tensorflow:num of train batches 3125\n",
      "I0208 10:31:25.352973 134503972279424 train_trans.py:475] num of train batches 3125\n",
      "INFO:tensorflow:num of test batches 625\n",
      "I0208 10:31:25.353069 134503972279424 train_trans.py:476] num of test batches 625\n",
      "2026-02-08 10:31:25.464929: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1770546685.465114   53513 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0208 10:31:26.795044 134503972279424 functional_saver.py:440] Sharding callback duration: 9 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-1\n",
      "I0208 10:31:26.808923 134503972279424 train_trans.py:226] Model saved in path: ./trans_long-1\n",
      "INFO:tensorflow:Starting training ... \n",
      "I0208 10:31:26.809665 134503972279424 train_trans.py:284] Starting training ... \n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:1474: UserWarning: Layer 'layers_._0' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling SelfAttention.call().\n",
      "\n",
      "\u001b[1mDimensions must be equal, but are 8 and 32 for '{{node self_attention_1/Mul_2}} = Mul[T=DT_FLOAT](self_attention_1/Mul_2/ReadVariableOp, self_attention_1/Mul_1)' with input shapes: [1,1,8,8], [16,1,8,32].\u001b[0m\n",
      "\n",
      "Arguments received by SelfAttention.call():\n",
      "  â€¢ source_input=tf.Tensor(shape=(16, 1, 128), dtype=float32)\n",
      "  â€¢ pos_ft=tf.Tensor(shape=(16, 1, 128), dtype=float32)\n",
      "  â€¢ pos_ft_slopes=tf.Tensor(shape=(16, 1, 128), dtype=float32)\n",
      "  â€¢ training=True''\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'layers_._0', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/EstraNet/train_trans.py\", line 530, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "             ^^^^^^^^^^\n",
      "  File \"/content/EstraNet/train_trans.py\", line 478, in main\n",
      "    train(train_data.GetTFRecords(FLAGS.train_batch_size, training=True), \\\n",
      "  File \"/content/EstraNet/train_trans.py\", line 289, in train\n",
      "    train_steps(train_iter, tf.convert_to_tensor(FLAGS.iterations), \\\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_fileyb5mhj58.py\", line 66, in tf__train_steps\n",
      "    ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(steps),), None, fscope), None, loop_body, get_state_1, set_state_1, ('global_step',), {'iterate_names': '_'})\n",
      "  File \"/tmp/__autograph_generated_fileyb5mhj58.py\", line 62, in loop_body\n",
      "    ag__.converted_call(ag__.ld(strategy).run, (ag__.ld(step_fn),), dict(args=(ag__.ld(inps), ag__.ld(lbls), ag__.ld(global_step))), fscope)\n",
      "  File \"/tmp/__autograph_generated_fileyb5mhj58.py\", line 23, in step_fn\n",
      "    logits = ag__.converted_call(ag__.ld(model), (), dict(inputs=ag__.ld(inps), softmax_attn_smoothing=ag__.ld(softmax_attn_smoothing), training=True), fscope_1)[0]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/content/EstraNet/transformer.py\", line 315, in call\n",
      "    all_out = layer([core_out, pos_ft, pos_ft_slopes], training=training)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/EstraNet/transformer.py\", line 130, in call\n",
      "    attn_outputs = self.self_attn(attn_in, pos_ft, pos_ft_slopes,\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/EstraNet/fast_attention.py\", line 196, in call\n",
      "    key_pos_ft = query_pos_ft + self.pos_ft_offsets*slope_pos\n",
      "                                ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/content/EstraNet/train_trans.py\", line 243, in step_fn  *\n",
      "        logits = model(inputs=inps, softmax_attn_smoothing=softmax_attn_smoothing, training=True)[0]\n",
      "    File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/content/EstraNet/transformer.py\", line 315, in call\n",
      "        all_out = layer([core_out, pos_ft, pos_ft_slopes], training=training)\n",
      "    File \"/content/EstraNet/transformer.py\", line 130, in call\n",
      "        attn_outputs = self.self_attn(attn_in, pos_ft, pos_ft_slopes,\n",
      "    File \"/content/EstraNet/fast_attention.py\", line 196, in call\n",
      "        key_pos_ft = query_pos_ft + self.pos_ft_offsets*slope_pos\n",
      "\n",
      "    ValueError: Exception encountered when calling SelfAttention.call().\n",
      "    \n",
      "    \u001b[1mDimensions must be equal, but are 8 and 32 for '{{node while/transformer_1/layers_._0_1/self_attention_1/Mul_2}} = Mul[T=DT_FLOAT](while/transformer_1/layers_._0_1/self_attention_1/Mul_2/ReadVariableOp, while/transformer_1/layers_._0_1/self_attention_1/Mul_1)' with input shapes: [1,1,8,8], [16,1,8,32].\u001b[0m\n",
      "    \n",
      "    Arguments received by SelfAttention.call():\n",
      "      â€¢ source_input=tf.Tensor(shape=(16, 1, 128), dtype=float32)\n",
      "      â€¢ pos_ft=tf.Tensor(shape=(16, 1, 128), dtype=float32)\n",
      "      â€¢ pos_ft_slopes=tf.Tensor(shape=(16, 1, 128), dtype=float32)\n",
      "      â€¢ training=True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the command arguments\n",
    "args = [\n",
    "    f'--use_tpu={config[\"use_tpu\"]}',\n",
    "    f'--data_path={config[\"data_path\"]}',\n",
    "    f'--dataset={config[\"dataset\"]}',\n",
    "    f'--checkpoint_dir={config[\"checkpoint_dir\"]}',\n",
    "    f'--warm_start={config[\"warm_start\"]}',\n",
    "    f'--result_path={config[\"result_path\"]}',\n",
    "    f'--learning_rate={config[\"learning_rate\"]}',\n",
    "    f'--clip={config[\"clip\"]}',\n",
    "    f'--min_lr_ratio={config[\"min_lr_ratio\"]}',\n",
    "    f'--warmup_steps={config[\"warmup_steps\"]}',\n",
    "    f'--input_length={config[\"input_length\"]}',\n",
    "    f'--data_desync={config[\"data_desync\"]}',\n",
    "    f'--train_batch_size={config[\"train_batch_size\"]}',\n",
    "    f'--eval_batch_size={config[\"eval_batch_size\"]}',\n",
    "    f'--train_steps={config[\"train_steps\"]}',\n",
    "    f'--iterations={config[\"iterations\"]}',\n",
    "    f'--save_steps={config[\"save_steps\"]}',\n",
    "    f'--n_layer={config[\"n_layer\"]}',\n",
    "    f'--d_model={config[\"d_model\"]}',\n",
    "    f'--d_head={config[\"d_head\"]}',\n",
    "    f'--n_head={config[\"n_head\"]}',\n",
    "    f'--d_inner={config[\"d_inner\"]}',\n",
    "    f'--n_head_softmax={config[\"n_head_softmax\"]}',\n",
    "    f'--d_head_softmax={config[\"d_head_softmax\"]}',\n",
    "    f'--dropout={config[\"dropout\"]}',\n",
    "    f'--conv_kernel_size={config[\"conv_kernel_size\"]}',\n",
    "    f'--n_conv_layer={config[\"n_conv_layer\"]}',\n",
    "    f'--pool_size={config[\"pool_size\"]}',\n",
    "    f'--d_kernel_map={config[\"d_kernel_map\"]}',\n",
    "    f'--beta_hat_2={config[\"beta_hat_2\"]}',\n",
    "    f'--model_normalization={config[\"model_normalization\"]}',\n",
    "    f'--head_initialization={config[\"head_initialization\"]}',\n",
    "    f'--softmax_attn={config[\"softmax_attn\"]}',\n",
    "    f'--max_eval_batch={config[\"max_eval_batch\"]}',\n",
    "    '--do_train=True'\n",
    "]\n",
    "\n",
    "args_str = ' '.join(args)\n",
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "print(f\"Arguments: {args_str[:100]}...\\n\")\n",
    "\n",
    "# Execute training - output will be displayed automatically\n",
    "!python train_trans.py {args_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Evaluate the Model (Run After Training Completes)\n",
    "\n",
    "âš ï¸ **Run this cell only after training completes!**\n",
    "\n",
    "This cell evaluates the trained model on the test set. You can run it multiple times to check different checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 1 checkpoint(s)\n",
      "   Latest checkpoint: trans_long-1.index\n",
      "\n",
      "ðŸ“Š Starting evaluation...\n",
      "\n",
      "2026-02-08 09:56:01.530341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770544561.559862   44596 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770544561.568885   44596 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770544561.590714   44596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770544561.590757   44596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770544561.590765   44596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770544561.590772   44596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "INFO:tensorflow:\n",
      "I0208 09:56:07.428153 132652938204288 train_trans.py:382] \n",
      "INFO:tensorflow:\n",
      "I0208 09:56:07.428373 132652938204288 train_trans.py:383] \n",
      "INFO:tensorflow:use_tpu               : False\n",
      "I0208 09:56:07.428464 132652938204288 train_trans.py:384] use_tpu               : False\n",
      "INFO:tensorflow:data_path             : data/ASCAD.h5\n",
      "I0208 09:56:07.428532 132652938204288 train_trans.py:385] data_path             : data/ASCAD.h5\n",
      "INFO:tensorflow:dataset               : ASCAD\n",
      "I0208 09:56:07.428591 132652938204288 train_trans.py:386] dataset               : ASCAD\n",
      "INFO:tensorflow:checkpoint_dir        : ./\n",
      "I0208 09:56:07.428647 132652938204288 train_trans.py:387] checkpoint_dir        : ./\n",
      "INFO:tensorflow:checkpoint_idx        : 0\n",
      "I0208 09:56:07.428699 132652938204288 train_trans.py:388] checkpoint_idx        : 0\n",
      "INFO:tensorflow:warm_start            : False\n",
      "I0208 09:56:07.428748 132652938204288 train_trans.py:389] warm_start            : False\n",
      "INFO:tensorflow:result_path           : results\n",
      "I0208 09:56:07.428797 132652938204288 train_trans.py:390] result_path           : results\n",
      "INFO:tensorflow:do_train              : False\n",
      "I0208 09:56:07.428845 132652938204288 train_trans.py:391] do_train              : False\n",
      "INFO:tensorflow:learning_rate         : 0.00025\n",
      "I0208 09:56:07.428897 132652938204288 train_trans.py:392] learning_rate         : 0.00025\n",
      "INFO:tensorflow:clip                  : 0.25\n",
      "I0208 09:56:07.428986 132652938204288 train_trans.py:393] clip                  : 0.25\n",
      "INFO:tensorflow:min_lr_ratio          : 0.004\n",
      "I0208 09:56:07.429044 132652938204288 train_trans.py:394] min_lr_ratio          : 0.004\n",
      "INFO:tensorflow:warmup_steps          : 1000000\n",
      "I0208 09:56:07.429095 132652938204288 train_trans.py:395] warmup_steps          : 1000000\n",
      "INFO:tensorflow:input_length          : 10000\n",
      "I0208 09:56:07.429148 132652938204288 train_trans.py:396] input_length          : 10000\n",
      "INFO:tensorflow:data_desync           : 0\n",
      "I0208 09:56:07.429199 132652938204288 train_trans.py:397] data_desync           : 0\n",
      "INFO:tensorflow:train_batch_size      : 16\n",
      "I0208 09:56:07.429250 132652938204288 train_trans.py:398] train_batch_size      : 16\n",
      "INFO:tensorflow:eval_batch_size       : 16\n",
      "I0208 09:56:07.429302 132652938204288 train_trans.py:399] eval_batch_size       : 16\n",
      "INFO:tensorflow:train_steps           : 4000000\n",
      "I0208 09:56:07.429353 132652938204288 train_trans.py:400] train_steps           : 4000000\n",
      "INFO:tensorflow:iterations            : 20000\n",
      "I0208 09:56:07.429405 132652938204288 train_trans.py:401] iterations            : 20000\n",
      "INFO:tensorflow:save_steps            : 40000\n",
      "I0208 09:56:07.429456 132652938204288 train_trans.py:402] save_steps            : 40000\n",
      "INFO:tensorflow:n_layer               : 2\n",
      "I0208 09:56:07.429508 132652938204288 train_trans.py:403] n_layer               : 2\n",
      "INFO:tensorflow:d_model               : 128\n",
      "I0208 09:56:07.429559 132652938204288 train_trans.py:404] d_model               : 128\n",
      "INFO:tensorflow:d_head                : 32\n",
      "I0208 09:56:07.429611 132652938204288 train_trans.py:405] d_head                : 32\n",
      "INFO:tensorflow:n_head                : 8\n",
      "I0208 09:56:07.429663 132652938204288 train_trans.py:406] n_head                : 8\n",
      "INFO:tensorflow:d_inner               : 256\n",
      "I0208 09:56:07.429715 132652938204288 train_trans.py:407] d_inner               : 256\n",
      "INFO:tensorflow:n_head_softmax        : 8\n",
      "I0208 09:56:07.429769 132652938204288 train_trans.py:408] n_head_softmax        : 8\n",
      "INFO:tensorflow:d_head_softmax        : 16\n",
      "I0208 09:56:07.429825 132652938204288 train_trans.py:409] d_head_softmax        : 16\n",
      "INFO:tensorflow:dropout               : 0.05\n",
      "I0208 09:56:07.429879 132652938204288 train_trans.py:410] dropout               : 0.05\n",
      "INFO:tensorflow:conv_kernel_size      : 3\n",
      "I0208 09:56:07.429946 132652938204288 train_trans.py:411] conv_kernel_size      : 3\n",
      "INFO:tensorflow:n_conv_layer          : 2\n",
      "I0208 09:56:07.430010 132652938204288 train_trans.py:412] n_conv_layer          : 2\n",
      "INFO:tensorflow:pool_size             : 20\n",
      "I0208 09:56:07.430063 132652938204288 train_trans.py:413] pool_size             : 20\n",
      "INFO:tensorflow:d_kernel_map          : 512\n",
      "I0208 09:56:07.430117 132652938204288 train_trans.py:414] d_kernel_map          : 512\n",
      "INFO:tensorflow:beta_hat_2            : 150\n",
      "I0208 09:56:07.430170 132652938204288 train_trans.py:415] beta_hat_2            : 150\n",
      "INFO:tensorflow:model_normalization   : preLC\n",
      "I0208 09:56:07.430224 132652938204288 train_trans.py:416] model_normalization   : preLC\n",
      "INFO:tensorflow:head_initialization   : forward\n",
      "I0208 09:56:07.430277 132652938204288 train_trans.py:417] head_initialization   : forward\n",
      "INFO:tensorflow:softmax_attn          : True\n",
      "I0208 09:56:07.430333 132652938204288 train_trans.py:418] softmax_attn          : True\n",
      "INFO:tensorflow:max_eval_batch        : 100\n",
      "I0208 09:56:07.430397 132652938204288 train_trans.py:419] max_eval_batch        : 100\n",
      "INFO:tensorflow:output_attn           : False\n",
      "I0208 09:56:07.430448 132652938204288 train_trans.py:420] output_attn           : False\n",
      "INFO:tensorflow:\n",
      "I0208 09:56:07.430497 132652938204288 train_trans.py:421] \n",
      "INFO:tensorflow:\n",
      "I0208 09:56:07.430547 132652938204288 train_trans.py:422] \n",
      "INFO:tensorflow:Number of accelerators: 1\n",
      "I0208 09:56:35.290364 132652938204288 train_trans.py:462] Number of accelerators: 1\n",
      "INFO:tensorflow:num of test batches 625\n",
      "I0208 09:56:35.290539 132652938204288 train_trans.py:484] num of test batches 625\n",
      "2026-02-08 09:56:35.387713: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1770544595.387878   44596 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "INFO:tensorflow:Restoring checkpoint: ./trans_long-1\n",
      "I0208 09:56:36.653517 132652938204288 train_trans.py:366] Restoring checkpoint: ./trans_long-1\n",
      "INFO:tensorflow:Restored checkpoint: ./trans_long-1\n",
      "I0208 09:56:36.663431 132652938204288 train_trans.py:369] Restored checkpoint: ./trans_long-1\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/train_trans.py\", line 530, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "             ^^^^^^^^^^\n",
      "  File \"/content/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/train_trans.py\", line 486, in main\n",
      "    output = evaluate(test_data.GetTFRecords(FLAGS.eval_batch_size, training=False), \n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/train_trans.py\", line 377, in evaluate\n",
      "    output = model.predict(data)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/content/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/EstraNet/transformer.py\", line 311, in call\n",
      "    pos_ft, pos_ft_slopes = self.pos_feature(slen, bsz)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: Exception encountered when calling Transformer.call().\n",
      "\n",
      "\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 1 (of type <class 'int'>)\u001b[0m\n",
      "\n",
      "Arguments received by Transformer.call():\n",
      "  â€¢ inp=tf.Tensor(shape=(16, 700), dtype=float32)\n",
      "  â€¢ softmax_attn_smoothing=1\n",
      "  â€¢ training=False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check if checkpoints exist\n",
    "checkpoint_files = glob.glob('*.index')\n",
    "if not checkpoint_files:\n",
    "    print(\"âš ï¸ No checkpoint files found!\")\n",
    "    print(\"   Make sure training has created at least one checkpoint.\")\n",
    "    print(\"   Checkpoints are saved every\", config['save_steps'], \"steps.\")\n",
    "else:\n",
    "    print(f\"âœ… Found {len(checkpoint_files)} checkpoint(s)\")\n",
    "    print(f\"   Latest checkpoint: {sorted(checkpoint_files)[-1]}\\n\")\n",
    "    \n",
    "    # Build evaluation arguments\n",
    "    eval_args = [\n",
    "        f'--use_tpu={config[\"use_tpu\"]}',\n",
    "        f'--data_path={config[\"data_path\"]}',\n",
    "        f'--dataset={config[\"dataset\"]}',\n",
    "        f'--checkpoint_dir={config[\"checkpoint_dir\"]}',\n",
    "        '--checkpoint_idx=0',\n",
    "        f'--warm_start={config[\"warm_start\"]}',\n",
    "        f'--result_path={config[\"result_path\"]}',\n",
    "        f'--learning_rate={config[\"learning_rate\"]}',\n",
    "        f'--clip={config[\"clip\"]}',\n",
    "        f'--min_lr_ratio={config[\"min_lr_ratio\"]}',\n",
    "        f'--warmup_steps={config[\"warmup_steps\"]}',\n",
    "        f'--input_length={config[\"input_length\"]}',\n",
    "        f'--train_batch_size={config[\"train_batch_size\"]}',\n",
    "        f'--eval_batch_size={config[\"eval_batch_size\"]}',\n",
    "        f'--train_steps={config[\"train_steps\"]}',\n",
    "        f'--iterations={config[\"iterations\"]}',\n",
    "        f'--save_steps={config[\"save_steps\"]}',\n",
    "        f'--n_layer={config[\"n_layer\"]}',\n",
    "        f'--d_model={config[\"d_model\"]}',\n",
    "        f'--d_head={config[\"d_head\"]}',\n",
    "        f'--n_head={config[\"n_head\"]}',\n",
    "        f'--d_inner={config[\"d_inner\"]}',\n",
    "        f'--n_head_softmax={config[\"n_head_softmax\"]}',\n",
    "        f'--d_head_softmax={config[\"d_head_softmax\"]}',\n",
    "        f'--dropout={config[\"dropout\"]}',\n",
    "        f'--conv_kernel_size={config[\"conv_kernel_size\"]}',\n",
    "        f'--n_conv_layer={config[\"n_conv_layer\"]}',\n",
    "        f'--pool_size={config[\"pool_size\"]}',\n",
    "        f'--d_kernel_map={config[\"d_kernel_map\"]}',\n",
    "        f'--beta_hat_2={config[\"beta_hat_2\"]}',\n",
    "        f'--model_normalization={config[\"model_normalization\"]}',\n",
    "        f'--head_initialization={config[\"head_initialization\"]}',\n",
    "        f'--softmax_attn={config[\"softmax_attn\"]}',\n",
    "        f'--max_eval_batch={config[\"max_eval_batch\"]}',\n",
    "        '--output_attn=False',\n",
    "        '--do_train=False'\n",
    "    ]\n",
    "    \n",
    "    eval_args_str = ' '.join(eval_args)\n",
    "    print(\"ðŸ“Š Starting evaluation...\\n\")\n",
    "    \n",
    "    # Execute evaluation\n",
    "    !python train_trans.py {eval_args_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ View Results\n",
    "\n",
    "Check the results directory for evaluation metrics and guessing entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Results directory not found\n",
      "\n",
      "ðŸ’¾ Checkpoint files:\n",
      "  - checkpoint\n",
      "  - trans_long-1.data-00000-of-00001\n",
      "  - trans_long-1.index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in results directory\n",
    "if os.path.exists('results'):\n",
    "    print(\"ðŸ“ Results directory contents:\")\n",
    "    for file in os.listdir('results'):\n",
    "        filepath = os.path.join('results', file)\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"  - {file} ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"âš ï¸ Results directory not found\")\n",
    "\n",
    "# List checkpoint files\n",
    "print(\"\\nðŸ’¾ Checkpoint files:\")\n",
    "checkpoint_files = [f for f in os.listdir('.') if 'checkpoint' in f or f.endswith('.index') or f.endswith('.data-00000-of-00001')]\n",
    "if checkpoint_files:\n",
    "    for file in sorted(checkpoint_files)[:10]:  # Show first 10\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"  No checkpoints found yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Notes\n",
    "\n",
    "### Training Tips:\n",
    "- **GPU Acceleration**: Make sure GPU is enabled in Colab (Runtime > Change runtime type > GPU)\n",
    "- **Training Time**: Full training with 4M steps will take many hours. Consider reducing `train_steps` for testing.\n",
    "- **Checkpoints**: Models are saved every 40,000 steps. You can resume training from checkpoints.\n",
    "- **Memory**: If you run out of memory, try reducing `train_batch_size` or `input_length`.\n",
    "\n",
    "### Quick Test Run:\n",
    "For a quick test, modify the config:\n",
    "```python\n",
    "config['train_steps'] = 100000  # Reduced from 4M\n",
    "config['warmup_steps'] = 10000  # Reduced from 1M\n",
    "config['save_steps'] = 10000    # Save more frequently\n",
    "```\n",
    "\n",
    "### Evaluation:\n",
    "- **When to run**: Only run the evaluation cell (Section 7) after training has created at least one checkpoint\n",
    "- **Checkpoints**: The evaluation will use the latest checkpoint automatically\n",
    "- **Re-run**: You can re-run the evaluation cell anytime to check the latest checkpoint\n",
    "\n",
    "### Compatibility Fixes Applied:\n",
    "This notebook automatically fixes TensorFlow 2.13+ / Keras 3 compatibility issues:\n",
    "- âœ… Replaced SyncBatchNormalization with BatchNormalization (doesn't exist in TF 2.13+)\n",
    "- âœ… Fixed integer comparison syntax (changed 'is' to '==')\n",
    "- âœ… Fixed add_weight() method calls in fast_attention.py\n",
    "- âœ… Fixed reset_states() method calls in train_trans.py (renamed to reset_state())\n",
    "- âœ… Fixed model call signature for Keras 3 (keyword arguments required)\n",
    "- âœ… Fixed PositionalFeature call signature for Keras 3 (ALL arguments as keywords)\n",
    "\n",
    "**Note**: These fixes work with TensorFlow 2.13, 2.19, and other Keras 3-based versions.\n",
    "\n",
    "### References:\n",
    "- **Paper**: [IACR TCHES 2024](https://tches.iacr.org/index.php/TCHES/article/view/11255)\n",
    "- **GitHub**: [suvadeep-iitb/EstraNet](https://github.com/suvadeep-iitb/EstraNet)\n",
    "- **ASCAD Dataset**: [ANSSI-FR/ASCAD](https://github.com/ANSSI-FR/ASCAD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EstraNet Training on ASCADf Dataset\n",
    "\n",
    "This notebook sets up and trains the EstraNet model on the ASCADf (ASCAD with fixed key) dataset.\n",
    "\n",
    "**Paper**: [EstraNet: An Efficient Shift-Invariant Transformer Network for Side-Channel Analysis](https://tches.iacr.org/index.php/TCHES/article/view/11255)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Setup Checklist\n",
    "- ‚úÖ Install dependencies\n",
    "- ‚úÖ Download ASCADf dataset from Google Drive\n",
    "- ‚úÖ Apply TensorFlow 2.13+ compatibility fixes\n",
    "- ‚úÖ Configure training parameters\n",
    "- ‚úÖ Train the model\n",
    "- ‚úÖ Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup\n",
    "\n",
    "First, let's check if we're running on Google Colab and set up GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Running on Google Colab\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "üöÄ GPU detected! Training will be accelerated.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"üìù Running on local Jupyter\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"üöÄ GPU detected! Training will be accelerated.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"üí° Enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone Repository and Install Dependencies\n",
    "\n",
    "If running on Colab, we need to clone the repository first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EstraNet'...\n",
      "remote: Enumerating objects: 69, done.\u001b[K\n",
      "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
      "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
      "remote: Total 69 (delta 36), reused 66 (delta 33), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (69/69), 46.36 KiB | 930.00 KiB/s, done.\n",
      "Resolving deltas: 100% (36/36), done.\n",
      "‚úÖ Clean! Directory: /content/EstraNet\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "if os.path.exists('EstraNet'):\n",
    "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
    "    \n",
    "!git clone https://github.com/loshithan/EstraNet.git\n",
    "os.chdir('EstraNet')\n",
    "print(f\"‚úÖ Clean! Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "\n",
      "\n",
      "‚úÖ All dependencies installed!\n",
      "Using TensorFlow 2.19.0 (pre-installed)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "# %pip install -q absl-py==2.3.1 numpy==1.24.3 scipy==1.10.1 h5py==3.11.0\n",
    "\n",
    "# Install gdown for downloading from Google Drive\n",
    "%pip install -q gdown\n",
    "\n",
    "# Note: Using TensorFlow version pre-installed in Colab (2.16+ / 2.19+)\n",
    "# The compatibility fixes in Section 3 work with all TensorFlow 2.13+ versions\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "print(f\"Using TensorFlow {tf.__version__} (pre-installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Apply TensorFlow 2.13+ Compatibility Fixes\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT: You MUST run this cell before training!**\n",
    "\n",
    "The original code has compatibility issues with TensorFlow 2.13+. This cell fixes them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying ALL TensorFlow 2.13+ compatibility fixes...\n",
      "\n",
      "üìù Fixing transformer.py...\n",
      "  ‚úÖ transformer.py fixed!\n",
      "\n",
      "üìù Fixing train_trans.py...\n",
      "  ‚úÖ train_trans.py fixed!\n",
      "\n",
      "üìù Fixing fast_attention.py...\n",
      "  ‚úÖ fast_attention.py fixed!\n",
      "\n",
      "üöÄ ALL FIXES APPLIED! Run training now.\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Applying ALL TensorFlow 2.13+ compatibility fixes...\\n\")\n",
    "\n",
    "# FIX 1: transformer.py\n",
    "print(\"üìù Fixing transformer.py...\")\n",
    "with open('transformer.py', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "content = content.replace(\n",
    "    'def call(self, inp, softmax_attn_smoothing=1, training=False):',\n",
    "    'def call(self, inputs, softmax_attn_smoothing=1, training=False):'\n",
    ")\n",
    "content = content.replace(\n",
    "    'inp = tf.expand_dims(inp, axis=-1)',\n",
    "    'inp = tf.expand_dims(inputs, axis=-1)',\n",
    "    1\n",
    ")\n",
    "content = content.replace(\n",
    "    'pos_ft, pos_ft_slopes = self.pos_feature(slen, bsz)',\n",
    "    'pos_ft, pos_ft_slopes = self.pos_feature(slen=slen, bsz=bsz)'\n",
    ")\n",
    "content = content.replace(\n",
    "    'from tensorflow.keras.layers.experimental import SyncBatchNormalization',\n",
    "    'from tensorflow.keras.layers import BatchNormalization as SyncBatchNormalization'\n",
    ")\n",
    "content = content.replace('if l is 0 else', 'if l == 0 else')\n",
    "\n",
    "# FIX DIVISION BY ZERO\n",
    "content = content.replace(\n",
    "    'normalized_slopes = (1. / float(slen-1)) * self.slopes',\n",
    "    'normalized_slopes = (1. / max(float(slen-1), 1.0)) * self.slopes'\n",
    ")\n",
    "\n",
    "with open('transformer.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  ‚úÖ transformer.py fixed!\")\n",
    "\n",
    "# FIX 2: train_trans.py\n",
    "print(\"\\nüìù Fixing train_trans.py...\")\n",
    "with open('train_trans.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "content = content.replace('.reset_states()', '.reset_state()')\n",
    "content = content.replace(\n",
    "    'logits = model(inps, softmax_attn_smoothing, training=True)[0]',\n",
    "    'logits = model(inputs=inps, softmax_attn_smoothing=softmax_attn_smoothing, training=True)[0]'\n",
    ")\n",
    "content = content.replace(\n",
    "    'logits = model(inps)[0]',\n",
    "    'logits = model(inputs=inps)[0]'\n",
    ")\n",
    "\n",
    "with open('train_trans.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  ‚úÖ train_trans.py fixed!\")\n",
    "\n",
    "# FIX 3: fast_attention.py\n",
    "print(\"\\nüìù Fixing fast_attention.py...\")\n",
    "import re\n",
    "with open('fast_attention.py', 'r') as f:\n",
    "    content = f.read()\n",
    "pattern = r'self\\.add_weight\\(\\s*\"([^\"]+)\"\\s*,'\n",
    "content = re.sub(pattern, r'self.add_weight(name=\"\\1\",', content)\n",
    "with open('fast_attention.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  ‚úÖ fast_attention.py fixed!\")\n",
    "\n",
    "print(\"\\nüöÄ ALL FIXES APPLIED! Run training now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Download ASCADf Dataset\n",
    "\n",
    "Download the ASCAD dataset with fixed key from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading ASCADf dataset from Google Drive...\n",
      "   This may take a few minutes (~1.5 GB)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\n",
      "To: /content/EstraNet/data/ASCAD.h5\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46.6M/46.6M [00:00<00:00, 177MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset downloaded successfully!\n",
      "\n",
      "üìä Dataset info:\n",
      "  Keys: ['Attack_traces', 'Profiling_traces']\n",
      "  Profiling traces shape: (50000, 700)\n",
      "  Attack traces shape: (10000, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ASCADf dataset configuration\n",
    "file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
    "DATASET_PATH = \"data/ASCAD.h5\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"üì• Downloading ASCADf dataset from Google Drive...\")\n",
    "    print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
    "    \n",
    "    # Download using gdown\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists\")\n",
    "\n",
    "# Verify dataset\n",
    "import h5py\n",
    "with h5py.File(DATASET_PATH, 'r') as f:\n",
    "    print(f\"\\nüìä Dataset info:\")\n",
    "    print(f\"  Keys: {list(f.keys())}\")\n",
    "    if 'Profiling_traces' in f:\n",
    "        print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
    "    if 'Attack_traces' in f:\n",
    "        print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Configure Training Parameters\n",
    "\n",
    "Set up the training configuration. You can modify these parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Training Configuration:\n",
      "  Dataset: ASCAD\n",
      "  Input length: 10000\n",
      "  Batch size: 16\n",
      "  Training steps: 4,000,000\n",
      "  Model layers: 2\n",
      "  Model dimension: 128\n",
      "  Attention heads: 8\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data config\n",
    "    'data_path': DATASET_PATH,\n",
    "    'dataset': 'ASCAD',\n",
    "    'input_length': 10000,  # or 40000 for full traces\n",
    "    'data_desync': 200,     # 400 for input_length=40000\n",
    "    \n",
    "    # Training config\n",
    "    'train_batch_size': 16,\n",
    "    'eval_batch_size': 16,\n",
    "    'train_steps': 4000000,\n",
    "    'warmup_steps': 1000000,\n",
    "    'iterations': 20000,\n",
    "    'save_steps': 40000,\n",
    "    \n",
    "    # Optimization config\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'clip': 0.25,\n",
    "    'min_lr_ratio': 0.004,\n",
    "    \n",
    "    # Model architecture\n",
    "    'n_layer': 2,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'n_head': 8,\n",
    "    'd_inner': 256,\n",
    "    'n_head_softmax': 8,\n",
    "    'd_head_softmax': 16,\n",
    "    'dropout': 0.05,\n",
    "    'conv_kernel_size': 3,\n",
    "    'n_conv_layer': 2,\n",
    "    'pool_size': 20,\n",
    "    'd_kernel_map': 512,\n",
    "    'beta_hat_2': 150,\n",
    "    'model_normalization': 'preLC',\n",
    "    'head_initialization': 'forward',\n",
    "    'softmax_attn': True,\n",
    "    \n",
    "    # Checkpoint config\n",
    "    'checkpoint_dir': './',\n",
    "    'result_path': 'results',\n",
    "    'warm_start': False,\n",
    "    'use_tpu': False,\n",
    "    'max_eval_batch': 100,\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"  Dataset: {config['dataset']}\")\n",
    "print(f\"  Input length: {config['input_length']}\")\n",
    "print(f\"  Batch size: {config['train_batch_size']}\")\n",
    "print(f\"  Training steps: {config['train_steps']:,}\")\n",
    "print(f\"  Model layers: {config['n_layer']}\")\n",
    "print(f\"  Model dimension: {config['d_model']}\")\n",
    "print(f\"  Attention heads: {config['n_head']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Train the Model\n",
    "\n",
    "Now let's train the EstraNet model. This will take several hours depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "\n",
      "Arguments: --use_tpu=False --data_path=data/ASCAD.h5 --dataset=ASCAD --checkpoint_dir=./ --warm_start=False --r...\n",
      "\n",
      "2026-02-08 16:11:08.068364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770567068.085835    2837 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770567068.091195    2837 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770567068.104658    2837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770567068.104692    2837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770567068.104696    2837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770567068.104700    2837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚úÖ File replaced with fixed version\n",
      "‚úÖ Checkpoints deleted\n",
      "\n",
      "‚ö†Ô∏è  NOW RESTART RUNTIME: Runtime ‚Üí Restart runtime\n",
      "Then run your training again\n",
      "INFO:tensorflow:\n",
      "I0208 16:11:13.793531 138663375429632 train_trans.py:382] \n",
      "INFO:tensorflow:\n",
      "I0208 16:11:13.793760 138663375429632 train_trans.py:383] \n",
      "INFO:tensorflow:use_tpu               : False\n",
      "I0208 16:11:13.793844 138663375429632 train_trans.py:384] use_tpu               : False\n",
      "INFO:tensorflow:data_path             : data/ASCAD.h5\n",
      "I0208 16:11:13.793909 138663375429632 train_trans.py:385] data_path             : data/ASCAD.h5\n",
      "INFO:tensorflow:dataset               : ASCAD\n",
      "I0208 16:11:13.793967 138663375429632 train_trans.py:386] dataset               : ASCAD\n",
      "INFO:tensorflow:checkpoint_dir        : ./\n",
      "I0208 16:11:13.794023 138663375429632 train_trans.py:387] checkpoint_dir        : ./\n",
      "INFO:tensorflow:checkpoint_idx        : 0\n",
      "I0208 16:11:13.794077 138663375429632 train_trans.py:388] checkpoint_idx        : 0\n",
      "INFO:tensorflow:warm_start            : False\n",
      "I0208 16:11:13.794129 138663375429632 train_trans.py:389] warm_start            : False\n",
      "INFO:tensorflow:result_path           : results\n",
      "I0208 16:11:13.794180 138663375429632 train_trans.py:390] result_path           : results\n",
      "INFO:tensorflow:do_train              : True\n",
      "I0208 16:11:13.794238 138663375429632 train_trans.py:391] do_train              : True\n",
      "INFO:tensorflow:learning_rate         : 0.00025\n",
      "I0208 16:11:13.794294 138663375429632 train_trans.py:392] learning_rate         : 0.00025\n",
      "INFO:tensorflow:clip                  : 0.25\n",
      "I0208 16:11:13.794367 138663375429632 train_trans.py:393] clip                  : 0.25\n",
      "INFO:tensorflow:min_lr_ratio          : 0.004\n",
      "I0208 16:11:13.794423 138663375429632 train_trans.py:394] min_lr_ratio          : 0.004\n",
      "INFO:tensorflow:warmup_steps          : 1000000\n",
      "I0208 16:11:13.794474 138663375429632 train_trans.py:395] warmup_steps          : 1000000\n",
      "INFO:tensorflow:input_length          : 10000\n",
      "I0208 16:11:13.794526 138663375429632 train_trans.py:396] input_length          : 10000\n",
      "INFO:tensorflow:data_desync           : 200\n",
      "I0208 16:11:13.794576 138663375429632 train_trans.py:397] data_desync           : 200\n",
      "INFO:tensorflow:train_batch_size      : 16\n",
      "I0208 16:11:13.794626 138663375429632 train_trans.py:398] train_batch_size      : 16\n",
      "INFO:tensorflow:eval_batch_size       : 16\n",
      "I0208 16:11:13.794675 138663375429632 train_trans.py:399] eval_batch_size       : 16\n",
      "INFO:tensorflow:train_steps           : 4000000\n",
      "I0208 16:11:13.794724 138663375429632 train_trans.py:400] train_steps           : 4000000\n",
      "INFO:tensorflow:iterations            : 20000\n",
      "I0208 16:11:13.794773 138663375429632 train_trans.py:401] iterations            : 20000\n",
      "INFO:tensorflow:save_steps            : 40000\n",
      "I0208 16:11:13.794822 138663375429632 train_trans.py:402] save_steps            : 40000\n",
      "INFO:tensorflow:n_layer               : 2\n",
      "I0208 16:11:13.794871 138663375429632 train_trans.py:403] n_layer               : 2\n",
      "INFO:tensorflow:d_model               : 128\n",
      "I0208 16:11:13.794920 138663375429632 train_trans.py:404] d_model               : 128\n",
      "INFO:tensorflow:d_head                : 32\n",
      "I0208 16:11:13.794969 138663375429632 train_trans.py:405] d_head                : 32\n",
      "INFO:tensorflow:n_head                : 8\n",
      "I0208 16:11:13.795018 138663375429632 train_trans.py:406] n_head                : 8\n",
      "INFO:tensorflow:d_inner               : 256\n",
      "I0208 16:11:13.795067 138663375429632 train_trans.py:407] d_inner               : 256\n",
      "INFO:tensorflow:n_head_softmax        : 8\n",
      "I0208 16:11:13.795118 138663375429632 train_trans.py:408] n_head_softmax        : 8\n",
      "INFO:tensorflow:d_head_softmax        : 16\n",
      "I0208 16:11:13.795170 138663375429632 train_trans.py:409] d_head_softmax        : 16\n",
      "INFO:tensorflow:dropout               : 0.05\n",
      "I0208 16:11:13.795228 138663375429632 train_trans.py:410] dropout               : 0.05\n",
      "INFO:tensorflow:conv_kernel_size      : 3\n",
      "I0208 16:11:13.795279 138663375429632 train_trans.py:411] conv_kernel_size      : 3\n",
      "INFO:tensorflow:n_conv_layer          : 2\n",
      "I0208 16:11:13.795329 138663375429632 train_trans.py:412] n_conv_layer          : 2\n",
      "INFO:tensorflow:pool_size             : 20\n",
      "I0208 16:11:13.795396 138663375429632 train_trans.py:413] pool_size             : 20\n",
      "INFO:tensorflow:d_kernel_map          : 512\n",
      "I0208 16:11:13.795448 138663375429632 train_trans.py:414] d_kernel_map          : 512\n",
      "INFO:tensorflow:beta_hat_2            : 150\n",
      "I0208 16:11:13.795500 138663375429632 train_trans.py:415] beta_hat_2            : 150\n",
      "INFO:tensorflow:model_normalization   : preLC\n",
      "I0208 16:11:13.795552 138663375429632 train_trans.py:416] model_normalization   : preLC\n",
      "INFO:tensorflow:head_initialization   : forward\n",
      "I0208 16:11:13.795603 138663375429632 train_trans.py:417] head_initialization   : forward\n",
      "INFO:tensorflow:softmax_attn          : True\n",
      "I0208 16:11:13.795653 138663375429632 train_trans.py:418] softmax_attn          : True\n",
      "INFO:tensorflow:max_eval_batch        : 100\n",
      "I0208 16:11:13.795704 138663375429632 train_trans.py:419] max_eval_batch        : 100\n",
      "INFO:tensorflow:output_attn           : False\n",
      "I0208 16:11:13.795754 138663375429632 train_trans.py:420] output_attn           : False\n",
      "INFO:tensorflow:\n",
      "I0208 16:11:13.795803 138663375429632 train_trans.py:421] \n",
      "INFO:tensorflow:\n",
      "I0208 16:11:13.795854 138663375429632 train_trans.py:422] \n",
      "INFO:tensorflow:Number of accelerators: 1\n",
      "I0208 16:11:40.592192 138663375429632 train_trans.py:462] Number of accelerators: 1\n",
      "INFO:tensorflow:num of train batches 3125\n",
      "I0208 16:11:40.592384 138663375429632 train_trans.py:475] num of train batches 3125\n",
      "INFO:tensorflow:num of test batches 625\n",
      "I0208 16:11:40.592458 138663375429632 train_trans.py:476] num of test batches 625\n",
      "2026-02-08 16:11:40.789910: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1770567100.790065    2837 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0208 16:11:42.047371 138663375429632 functional_saver.py:440] Sharding callback duration: 7 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-1\n",
      "I0208 16:11:42.057680 138663375429632 train_trans.py:226] Model saved in path: ./trans_long-1\n",
      "INFO:tensorflow:Starting training ... \n",
      "I0208 16:11:42.058207 138663375429632 train_trans.py:284] Starting training ... \n",
      "I0000 00:00:1770567107.805567    2988 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "INFO:tensorflow:[ 20000] | gnorm 10.01 lr  0.000005 | loss  5.89\n",
      "I0208 16:14:57.623369 138663375429632 train_trans.py:298] [ 20000] | gnorm 10.01 lr  0.000005 | loss  5.89\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.55\n",
      "I0208 16:14:58.408573 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.55\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:14:58.636935 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "INFO:tensorflow:[ 40000] | gnorm  5.77 lr  0.000010 | loss  5.56\n",
      "I0208 16:18:06.303855 138663375429632 train_trans.py:298] [ 40000] | gnorm  5.77 lr  0.000010 | loss  5.56\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:18:06.648223 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:18:06.900446 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:18:07.027218 138663375429632 functional_saver.py:440] Sharding callback duration: 37 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-2\n",
      "I0208 16:18:07.040001 138663375429632 train_trans.py:334] Model saved in path: ./trans_long-2\n",
      "INFO:tensorflow:[ 60000] | gnorm  4.29 lr  0.000015 | loss  5.55\n",
      "I0208 16:21:16.693605 138663375429632 train_trans.py:298] [ 60000] | gnorm  4.29 lr  0.000015 | loss  5.55\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:21:17.072980 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:21:17.314545 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "INFO:tensorflow:[ 80000] | gnorm  3.28 lr  0.000020 | loss  5.55\n",
      "I0208 16:24:24.638275 138663375429632 train_trans.py:298] [ 80000] | gnorm  3.28 lr  0.000020 | loss  5.55\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:24:24.967963 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:24:25.198540 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:24:25.299814 138663375429632 functional_saver.py:440] Sharding callback duration: 55 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-3\n",
      "I0208 16:24:25.308842 138663375429632 train_trans.py:334] Model saved in path: ./trans_long-3\n",
      "INFO:tensorflow:[100000] | gnorm  2.52 lr  0.000025 | loss  5.54\n",
      "I0208 16:27:31.643542 138663375429632 train_trans.py:298] [100000] | gnorm  2.52 lr  0.000025 | loss  5.54\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:27:31.996441 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:27:32.228181 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "INFO:tensorflow:[120000] | gnorm  1.92 lr  0.000030 | loss  5.54\n",
      "I0208 16:30:39.445691 138663375429632 train_trans.py:298] [120000] | gnorm  1.92 lr  0.000030 | loss  5.54\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:30:39.778062 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:30:40.023090 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:30:40.125535 138663375429632 functional_saver.py:440] Sharding callback duration: 41 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-4\n",
      "I0208 16:30:40.133964 138663375429632 train_trans.py:334] Model saved in path: ./trans_long-4\n",
      "INFO:tensorflow:[140000] | gnorm  1.65 lr  0.000035 | loss  5.54\n",
      "I0208 16:33:46.819703 138663375429632 train_trans.py:298] [140000] | gnorm  1.65 lr  0.000035 | loss  5.54\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:33:47.165271 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:33:47.403992 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "INFO:tensorflow:[160000] | gnorm  2.09 lr  0.000040 | loss  5.54\n",
      "I0208 16:36:53.207674 138663375429632 train_trans.py:298] [160000] | gnorm  2.09 lr  0.000040 | loss  5.54\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:36:53.563026 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:36:53.792744 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:36:53.902073 138663375429632 functional_saver.py:440] Sharding callback duration: 68 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-5\n",
      "I0208 16:36:53.914103 138663375429632 train_trans.py:334] Model saved in path: ./trans_long-5\n",
      "INFO:tensorflow:[180000] | gnorm  2.39 lr  0.000045 | loss  5.54\n",
      "I0208 16:40:00.110551 138663375429632 train_trans.py:298] [180000] | gnorm  2.39 lr  0.000045 | loss  5.54\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:40:00.444231 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:40:00.674786 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "INFO:tensorflow:[200000] | gnorm  2.75 lr  0.000050 | loss  5.54\n",
      "I0208 16:43:09.794284 138663375429632 train_trans.py:298] [200000] | gnorm  2.75 lr  0.000050 | loss  5.54\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:43:10.144052 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:43:10.372326 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:43:10.496788 138663375429632 functional_saver.py:440] Sharding callback duration: 37 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-6\n",
      "I0208 16:43:10.507785 138663375429632 train_trans.py:334] Model saved in path: ./trans_long-6\n",
      "INFO:tensorflow:[220000] | gnorm  3.78 lr  0.000055 | loss  5.54\n",
      "I0208 16:46:19.332629 138663375429632 train_trans.py:298] [220000] | gnorm  3.78 lr  0.000055 | loss  5.54\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.53\n",
      "I0208 16:46:19.659151 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.53\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.55\n",
      "I0208 16:46:19.905090 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.55\n",
      "INFO:tensorflow:[240000] | gnorm  4.29 lr  0.000060 | loss  5.54\n",
      "I0208 16:49:26.528300 138663375429632 train_trans.py:298] [240000] | gnorm  4.29 lr  0.000060 | loss  5.54\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.54\n",
      "I0208 16:49:27.036620 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.54\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.56\n",
      "I0208 16:49:27.373439 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.56\n",
      "I0208 16:49:27.494638 138663375429632 functional_saver.py:440] Sharding callback duration: 54 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-7\n",
      "I0208 16:49:27.507069 138663375429632 train_trans.py:334] Model saved in path: ./trans_long-7\n",
      "INFO:tensorflow:[260000] | gnorm  4.76 lr  0.000065 | loss  5.53\n",
      "I0208 16:52:34.573231 138663375429632 train_trans.py:298] [260000] | gnorm  4.76 lr  0.000065 | loss  5.53\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.53\n",
      "I0208 16:52:48.777001 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.53\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.56\n",
      "I0208 16:52:49.020476 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.56\n",
      "INFO:tensorflow:[280000] | gnorm  5.31 lr  0.000070 | loss  5.53\n",
      "I0208 16:55:55.827662 138663375429632 train_trans.py:298] [280000] | gnorm  5.31 lr  0.000070 | loss  5.53\n",
      "INFO:tensorflow:Train batches[  100]                | loss  5.52\n",
      "I0208 16:55:56.187529 138663375429632 train_trans.py:314] Train batches[  100]                | loss  5.52\n",
      "INFO:tensorflow:Eval  batches[  100]                | loss  5.57\n",
      "I0208 16:55:56.412590 138663375429632 train_trans.py:324] Eval  batches[  100]                | loss  5.57\n",
      "I0208 16:55:56.515421 138663375429632 functional_saver.py:440] Sharding callback duration: 61 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-8\n",
      "I0208 16:55:56.524368 138663375429632 train_trans.py:334] Model saved in path: ./trans_long-8\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Build the command arguments\n",
    "args = [\n",
    "    f'--use_tpu={config[\"use_tpu\"]}',\n",
    "    f'--data_path={config[\"data_path\"]}',\n",
    "    f'--dataset={config[\"dataset\"]}',\n",
    "    f'--checkpoint_dir={config[\"checkpoint_dir\"]}',\n",
    "    f'--warm_start={config[\"warm_start\"]}',\n",
    "    f'--result_path={config[\"result_path\"]}',\n",
    "    f'--learning_rate={config[\"learning_rate\"]}',\n",
    "    f'--clip={config[\"clip\"]}',\n",
    "    f'--min_lr_ratio={config[\"min_lr_ratio\"]}',\n",
    "    f'--warmup_steps={config[\"warmup_steps\"]}',\n",
    "    f'--input_length={config[\"input_length\"]}',\n",
    "    f'--data_desync={config[\"data_desync\"]}',\n",
    "    f'--train_batch_size={config[\"train_batch_size\"]}',\n",
    "    f'--eval_batch_size={config[\"eval_batch_size\"]}',\n",
    "    f'--train_steps={config[\"train_steps\"]}',\n",
    "    f'--iterations={config[\"iterations\"]}',\n",
    "    f'--save_steps={config[\"save_steps\"]}',\n",
    "    f'--n_layer={config[\"n_layer\"]}',\n",
    "    f'--d_model={config[\"d_model\"]}',\n",
    "    f'--d_head={config[\"d_head\"]}',\n",
    "    f'--n_head={config[\"n_head\"]}',\n",
    "    f'--d_inner={config[\"d_inner\"]}',\n",
    "    f'--n_head_softmax={config[\"n_head_softmax\"]}',\n",
    "    f'--d_head_softmax={config[\"d_head_softmax\"]}',\n",
    "    f'--dropout={config[\"dropout\"]}',\n",
    "    f'--conv_kernel_size={config[\"conv_kernel_size\"]}',\n",
    "    f'--n_conv_layer={config[\"n_conv_layer\"]}',\n",
    "    f'--pool_size={config[\"pool_size\"]}',\n",
    "    f'--d_kernel_map={config[\"d_kernel_map\"]}',\n",
    "    f'--beta_hat_2={config[\"beta_hat_2\"]}',\n",
    "    f'--model_normalization={config[\"model_normalization\"]}',\n",
    "    f'--head_initialization={config[\"head_initialization\"]}',\n",
    "    f'--softmax_attn={config[\"softmax_attn\"]}',\n",
    "    f'--max_eval_batch={config[\"max_eval_batch\"]}',\n",
    "    '--do_train=True'\n",
    "]\n",
    "\n",
    "args_str = ' '.join(args)\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "print(f\"Arguments: {args_str[:100]}...\\n\")\n",
    "\n",
    "# Execute training - output will be displayed automatically\n",
    "!python train_trans.py {args_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Evaluate the Model (Run After Training Completes)\n",
    "\n",
    "‚ö†Ô∏è **Run this cell only after training completes!**\n",
    "\n",
    "This cell evaluates the trained model on the test set. You can run it multiple times to check different checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Created backup folder: /content/drive/MyDrive/EstraNet_Backup_20260208_165724\n",
      "Found 17 checkpoint files.\n",
      "‚úÖ Copied trans_long-5.index\n",
      "‚úÖ Copied trans_long-2.data-00000-of-00001\n",
      "‚úÖ Copied trans_long-8.data-00000-of-00001\n",
      "‚úÖ Copied trans_long-3.index\n",
      "‚úÖ Copied trans_long-7.data-00000-of-00001\n",
      "‚úÖ Copied trans_long-2.index\n",
      "‚úÖ Copied trans_long-1.index\n",
      "‚úÖ Copied trans_long-4.data-00000-of-00001\n",
      "‚úÖ Copied trans_long-6.data-00000-of-00001\n",
      "‚úÖ Copied trans_long-3.data-00000-of-00001\n",
      "‚úÖ Copied trans_long-5.data-00000-of-00001\n",
      "‚úÖ Copied trans_long-4.index\n",
      "‚úÖ Copied trans_long-8.index\n",
      "‚úÖ Copied trans_long-7.index\n",
      "‚úÖ Copied trans_long-1.data-00000-of-00001\n",
      "‚úÖ Copied trans_long-6.index\n",
      "‚úÖ Copied checkpoint\n",
      "‚úÖ Copied transformer.py\n",
      "‚úÖ Copied train_trans.py\n",
      "‚úÖ Copied fast_attention.py\n",
      "\n",
      "üéâ BACKUP COMPLETE! You can safely disconnect now.\n"
     ]
    }
   ],
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Create a backup folder with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_dir = f\"/content/drive/MyDrive/EstraNet_Backup_{timestamp}\"\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "print(f\"Created backup folder: {backup_dir}\")\n",
    "\n",
    "# 3. Copy Checkpoints and Configuration\n",
    "# Copy all checkpoint files (index, data, checkpoint file)\n",
    "checkpoint_files = glob.glob(\"trans_long*\") + glob.glob(\"checkpoint\")\n",
    "print(f\"Found {len(checkpoint_files)} checkpoint files.\")\n",
    "\n",
    "for file in checkpoint_files:\n",
    "    try:\n",
    "        shutil.copy2(file, backup_dir)\n",
    "        print(f\"‚úÖ Copied {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to copy {file}: {e}\")\n",
    "\n",
    "# 4. Copy Results (if any)\n",
    "if os.path.exists(\"results\"):\n",
    "    try:\n",
    "        shutil.copytree(\"results\", f\"{backup_dir}/results\")\n",
    "        print(\"‚úÖ Copied results folder\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to copy results: {e}\")\n",
    "\n",
    "# 5. Copy your modified code (Important!)\n",
    "# Use the correct paths for your modified files\n",
    "code_files = ['transformer.py', 'train_trans.py', 'fast_attention.py']\n",
    "for file in code_files:\n",
    "    if os.path.exists(file):\n",
    "        shutil.copy2(file, backup_dir)\n",
    "        print(f\"‚úÖ Copied {file}\")\n",
    "\n",
    "print(\"\\nüéâ BACKUP COMPLETE! You can safely disconnect now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 8 checkpoint(s)\n",
      "   Latest checkpoint: trans_long-8.index\n",
      "\n",
      "üìä Starting evaluation...\n",
      "\n",
      "2026-02-08 16:57:40.608349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770569860.629738   14566 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770569860.636348   14566 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770569860.654040   14566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770569860.654069   14566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770569860.654073   14566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770569860.654079   14566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/EstraNet/train_trans.py\", line 14, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\", line 468, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/__init__.py\", line 7, in <module>\n",
      "    from keras import _tf_keras as _tf_keras\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/_tf_keras/__init__.py\", line 1, in <module>\n",
      "    from keras._tf_keras import keras\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/_tf_keras/keras/__init__.py\", line 7, in <module>\n",
      "    from keras import activations as activations\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/activations/__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize as deserialize\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/__init__.py\", line 13, in <module>\n",
      "    from keras.src import visualization\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/visualization/__init__.py\", line 2, in <module>\n",
      "    from keras.src.visualization import plot_image_gallery\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/visualization/plot_image_gallery.py\", line 13, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\", line 70, in <module>\n",
      "    from matplotlib.figure import Figure, FigureBase, figaspect\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\", line 40, in <module>\n",
      "    from matplotlib import _blocking_input, backend_bases, _docstring, projections\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/matplotlib/projections/__init__.py\", line 60, in <module>\n",
      "    from mpl_toolkits.mplot3d import Axes3D\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mpl_toolkits/mplot3d/__init__.py\", line 1, in <module>\n",
      "    from .axes3d import Axes3D\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mpl_toolkits/mplot3d/axes3d.py\", line 36, in <module>\n",
      "    from . import art3d\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mpl_toolkits/mplot3d/art3d.py\", line 740, in <module>\n",
      "    class Path3DCollection(PathCollection):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\", line 149, in __init_subclass__\n",
      "    cls._update_set_signature_and_docstring()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\", line 177, in _update_set_signature_and_docstring\n",
      "    + kwdoc(cls))\n",
      "      ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\", line 1848, in kwdoc\n",
      "    ai = ArtistInspector(artist)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\", line 1431, in __init__\n",
      "    self.aliasd = self.get_aliases()\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\", line 1449, in get_aliases\n",
      "    func = getattr(self.o, name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check if checkpoints exist\n",
    "checkpoint_files = glob.glob('*.index')\n",
    "if not checkpoint_files:\n",
    "    print(\"‚ö†Ô∏è No checkpoint files found!\")\n",
    "    print(\"   Make sure training has created at least one checkpoint.\")\n",
    "    print(\"   Checkpoints are saved every\", config['save_steps'], \"steps.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(checkpoint_files)} checkpoint(s)\")\n",
    "    print(f\"   Latest checkpoint: {sorted(checkpoint_files)[-1]}\\n\")\n",
    "    \n",
    "    # Build evaluation arguments\n",
    "    eval_args = [\n",
    "        f'--use_tpu={config[\"use_tpu\"]}',\n",
    "        f'--data_path={config[\"data_path\"]}',\n",
    "        f'--dataset={config[\"dataset\"]}',\n",
    "        f'--checkpoint_dir={config[\"checkpoint_dir\"]}',\n",
    "        '--checkpoint_idx=0',\n",
    "        f'--warm_start={config[\"warm_start\"]}',\n",
    "        f'--result_path={config[\"result_path\"]}',\n",
    "        f'--learning_rate={config[\"learning_rate\"]}',\n",
    "        f'--clip={config[\"clip\"]}',\n",
    "        f'--min_lr_ratio={config[\"min_lr_ratio\"]}',\n",
    "        f'--warmup_steps={config[\"warmup_steps\"]}',\n",
    "        f'--input_length={config[\"input_length\"]}',\n",
    "        f'--train_batch_size={config[\"train_batch_size\"]}',\n",
    "        f'--eval_batch_size={config[\"eval_batch_size\"]}',\n",
    "        f'--train_steps={config[\"train_steps\"]}',\n",
    "        f'--iterations={config[\"iterations\"]}',\n",
    "        f'--save_steps={config[\"save_steps\"]}',\n",
    "        f'--n_layer={config[\"n_layer\"]}',\n",
    "        f'--d_model={config[\"d_model\"]}',\n",
    "        f'--d_head={config[\"d_head\"]}',\n",
    "        f'--n_head={config[\"n_head\"]}',\n",
    "        f'--d_inner={config[\"d_inner\"]}',\n",
    "        f'--n_head_softmax={config[\"n_head_softmax\"]}',\n",
    "        f'--d_head_softmax={config[\"d_head_softmax\"]}',\n",
    "        f'--dropout={config[\"dropout\"]}',\n",
    "        f'--conv_kernel_size={config[\"conv_kernel_size\"]}',\n",
    "        f'--n_conv_layer={config[\"n_conv_layer\"]}',\n",
    "        f'--pool_size={config[\"pool_size\"]}',\n",
    "        f'--d_kernel_map={config[\"d_kernel_map\"]}',\n",
    "        f'--beta_hat_2={config[\"beta_hat_2\"]}',\n",
    "        f'--model_normalization={config[\"model_normalization\"]}',\n",
    "        f'--head_initialization={config[\"head_initialization\"]}',\n",
    "        f'--softmax_attn={config[\"softmax_attn\"]}',\n",
    "        f'--max_eval_batch={config[\"max_eval_batch\"]}',\n",
    "        '--output_attn=False',\n",
    "        '--do_train=False'\n",
    "    ]\n",
    "    \n",
    "    eval_args_str = ' '.join(eval_args)\n",
    "    print(\"üìä Starting evaluation...\\n\")\n",
    "    \n",
    "    # Execute evaluation\n",
    "    !python train_trans.py {eval_args_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ View Results\n",
    "\n",
    "Check the results directory for evaluation metrics and guessing entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Results directory not found\n",
      "\n",
      "üíæ Checkpoint files:\n",
      "  - checkpoint\n",
      "  - trans_long-1.data-00000-of-00001\n",
      "  - trans_long-1.index\n",
      "  - trans_long-2.data-00000-of-00001\n",
      "  - trans_long-2.index\n",
      "  - trans_long-3.data-00000-of-00001\n",
      "  - trans_long-3.index\n",
      "  - trans_long-4.data-00000-of-00001\n",
      "  - trans_long-4.index\n",
      "  - trans_long-5.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in results directory\n",
    "if os.path.exists('results'):\n",
    "    print(\"üìÅ Results directory contents:\")\n",
    "    for file in os.listdir('results'):\n",
    "        filepath = os.path.join('results', file)\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"  - {file} ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Results directory not found\")\n",
    "\n",
    "# List checkpoint files\n",
    "print(\"\\nüíæ Checkpoint files:\")\n",
    "checkpoint_files = [f for f in os.listdir('.') if 'checkpoint' in f or f.endswith('.index') or f.endswith('.data-00000-of-00001')]\n",
    "if checkpoint_files:\n",
    "    for file in sorted(checkpoint_files)[:10]:  # Show first 10\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"  No checkpoints found yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notes\n",
    "\n",
    "### Training Tips:\n",
    "- **GPU Acceleration**: Make sure GPU is enabled in Colab (Runtime > Change runtime type > GPU)\n",
    "- **Training Time**: Full training with 4M steps will take many hours. Consider reducing `train_steps` for testing.\n",
    "- **Checkpoints**: Models are saved every 40,000 steps. You can resume training from checkpoints.\n",
    "- **Memory**: If you run out of memory, try reducing `train_batch_size` or `input_length`.\n",
    "\n",
    "### Quick Test Run:\n",
    "For a quick test, modify the config:\n",
    "```python\n",
    "config['train_steps'] = 100000  # Reduced from 4M\n",
    "config['warmup_steps'] = 10000  # Reduced from 1M\n",
    "config['save_steps'] = 10000    # Save more frequently\n",
    "```\n",
    "\n",
    "### Evaluation:\n",
    "- **When to run**: Only run the evaluation cell (Section 7) after training has created at least one checkpoint\n",
    "- **Checkpoints**: The evaluation will use the latest checkpoint automatically\n",
    "- **Re-run**: You can re-run the evaluation cell anytime to check the latest checkpoint\n",
    "\n",
    "### Compatibility Fixes Applied:\n",
    "This notebook automatically fixes TensorFlow 2.13+ / Keras 3 compatibility issues:\n",
    "- ‚úÖ Replaced SyncBatchNormalization with BatchNormalization (doesn't exist in TF 2.13+)\n",
    "- ‚úÖ Fixed integer comparison syntax (changed 'is' to '==')\n",
    "- ‚úÖ Fixed add_weight() method calls in fast_attention.py\n",
    "- ‚úÖ Fixed reset_states() method calls in train_trans.py (renamed to reset_state())\n",
    "- ‚úÖ Fixed model call signature for Keras 3 (keyword arguments required)\n",
    "- ‚úÖ Fixed PositionalFeature call signature for Keras 3 (ALL arguments as keywords)\n",
    "\n",
    "**Note**: These fixes work with TensorFlow 2.13, 2.19, and other Keras 3-based versions.\n",
    "\n",
    "### References:\n",
    "- **Paper**: [IACR TCHES 2024](https://tches.iacr.org/index.php/TCHES/article/view/11255)\n",
    "- **GitHub**: [suvadeep-iitb/EstraNet](https://github.com/suvadeep-iitb/EstraNet)\n",
    "- **ASCAD Dataset**: [ANSSI-FR/ASCAD](https://github.com/ANSSI-FR/ASCAD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EstraNet Training on ASCADf Dataset\n",
    "\n",
    "This notebook sets up and trains the EstraNet model on the ASCADf (ASCAD with fixed key) dataset.\n",
    "\n",
    "**Paper**: [EstraNet: An Efficient Shift-Invariant Transformer Network for Side-Channel Analysis](https://tches.iacr.org/index.php/TCHES/article/view/11255)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Setup Checklist\n",
    "- ‚úÖ Install dependencies\n",
    "- ‚úÖ Download ASCADf dataset from Google Drive\n",
    "- ‚úÖ Apply TensorFlow 2.13+ compatibility fixes\n",
    "- ‚úÖ Configure training parameters\n",
    "- ‚úÖ Train the model\n",
    "- ‚úÖ Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup\n",
    "\n",
    "First, let's check if we're running on Google Colab and set up GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Running on Google Colab\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "üöÄ GPU detected! Training will be accelerated.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"üìù Running on local Jupyter\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"üöÄ GPU detected! Training will be accelerated.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"üí° Enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone Repository and Install Dependencies\n",
    "\n",
    "If running on Colab, we need to clone the repository first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EstraNet'...\n",
      "remote: Enumerating objects: 61, done.\u001b[K\n",
      "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 61 (delta 30), reused 61 (delta 30), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (61/61), 41.46 KiB | 758.00 KiB/s, done.\n",
      "Resolving deltas: 100% (30/30), done.\n",
      "‚úÖ Clean! Directory: /content/EstraNet\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "if os.path.exists('EstraNet'):\n",
    "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
    "    \n",
    "!git clone https://github.com/loshithan/EstraNet.git\n",
    "os.chdir('EstraNet')\n",
    "print(f\"‚úÖ Clean! Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "\n",
      "\n",
      "‚úÖ All dependencies installed!\n",
      "Using TensorFlow 2.19.0 (pre-installed)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "# %pip install -q absl-py==2.3.1 numpy==1.24.3 scipy==1.10.1 h5py==3.11.0\n",
    "\n",
    "# Install gdown for downloading from Google Drive\n",
    "%pip install -q gdown\n",
    "\n",
    "# Note: Using TensorFlow version pre-installed in Colab (2.16+ / 2.19+)\n",
    "# The compatibility fixes in Section 3 work with all TensorFlow 2.13+ versions\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "print(f\"Using TensorFlow {tf.__version__} (pre-installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Apply TensorFlow 2.13+ Compatibility Fixes\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT: You MUST run this cell before training!**\n",
    "\n",
    "The original code has compatibility issues with TensorFlow 2.13+. This cell fixes them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying ALL TensorFlow 2.13+ compatibility fixes...\n",
      "\n",
      "üìù Fixing transformer.py...\n",
      "  ‚úÖ transformer.py fixed!\n",
      "\n",
      "üìù Fixing train_trans.py...\n",
      "  ‚úÖ train_trans.py fixed!\n",
      "\n",
      "üìù Fixing fast_attention.py...\n",
      "  ‚úÖ fast_attention.py fixed!\n",
      "\n",
      "üöÄ ALL FIXES APPLIED! Run training now.\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Applying ALL TensorFlow 2.13+ compatibility fixes...\\n\")\n",
    "\n",
    "# FIX 1: transformer.py\n",
    "print(\"üìù Fixing transformer.py...\")\n",
    "with open('transformer.py', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "content = content.replace(\n",
    "    'def call(self, inp, softmax_attn_smoothing=1, training=False):',\n",
    "    'def call(self, inputs, softmax_attn_smoothing=1, training=False):'\n",
    ")\n",
    "content = content.replace(\n",
    "    'inp = tf.expand_dims(inp, axis=-1)',\n",
    "    'inp = tf.expand_dims(inputs, axis=-1)',\n",
    "    1\n",
    ")\n",
    "content = content.replace(\n",
    "    'pos_ft, pos_ft_slopes = self.pos_feature(slen, bsz)',\n",
    "    'pos_ft, pos_ft_slopes = self.pos_feature(slen=slen, bsz=bsz)'\n",
    ")\n",
    "content = content.replace(\n",
    "    'from tensorflow.keras.layers.experimental import SyncBatchNormalization',\n",
    "    'from tensorflow.keras.layers import BatchNormalization as SyncBatchNormalization'\n",
    ")\n",
    "content = content.replace('if l is 0 else', 'if l == 0 else')\n",
    "\n",
    "# FIX DIVISION BY ZERO\n",
    "content = content.replace(\n",
    "    'normalized_slopes = (1. / float(slen-1)) * self.slopes',\n",
    "    'normalized_slopes = (1. / max(float(slen-1), 1.0)) * self.slopes'\n",
    ")\n",
    "\n",
    "with open('transformer.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  ‚úÖ transformer.py fixed!\")\n",
    "\n",
    "# FIX 2: train_trans.py\n",
    "print(\"\\nüìù Fixing train_trans.py...\")\n",
    "with open('train_trans.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "content = content.replace('.reset_states()', '.reset_state()')\n",
    "content = content.replace(\n",
    "    'logits = model(inps, softmax_attn_smoothing, training=True)[0]',\n",
    "    'logits = model(inputs=inps, softmax_attn_smoothing=softmax_attn_smoothing, training=True)[0]'\n",
    ")\n",
    "content = content.replace(\n",
    "    'logits = model(inps)[0]',\n",
    "    'logits = model(inputs=inps)[0]'\n",
    ")\n",
    "\n",
    "with open('train_trans.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  ‚úÖ train_trans.py fixed!\")\n",
    "\n",
    "# FIX 3: fast_attention.py\n",
    "print(\"\\nüìù Fixing fast_attention.py...\")\n",
    "import re\n",
    "with open('fast_attention.py', 'r') as f:\n",
    "    content = f.read()\n",
    "pattern = r'self\\.add_weight\\(\\s*\"([^\"]+)\"\\s*,'\n",
    "content = re.sub(pattern, r'self.add_weight(name=\"\\1\",', content)\n",
    "with open('fast_attention.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"  ‚úÖ fast_attention.py fixed!\")\n",
    "\n",
    "print(\"\\nüöÄ ALL FIXES APPLIED! Run training now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Download ASCADf Dataset\n",
    "\n",
    "Download the ASCAD dataset with fixed key from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading ASCADf dataset from Google Drive...\n",
      "   This may take a few minutes (~1.5 GB)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\n",
      "To: /content/EstraNet/data/ASCAD.h5\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46.6M/46.6M [00:00<00:00, 157MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset downloaded successfully!\n",
      "\n",
      "üìä Dataset info:\n",
      "  Keys: ['Attack_traces', 'Profiling_traces']\n",
      "  Profiling traces shape: (50000, 700)\n",
      "  Attack traces shape: (10000, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ASCADf dataset configuration\n",
    "file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
    "DATASET_PATH = \"data/ASCAD.h5\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"üì• Downloading ASCADf dataset from Google Drive...\")\n",
    "    print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
    "    \n",
    "    # Download using gdown\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists\")\n",
    "\n",
    "# Verify dataset\n",
    "import h5py\n",
    "with h5py.File(DATASET_PATH, 'r') as f:\n",
    "    print(f\"\\nüìä Dataset info:\")\n",
    "    print(f\"  Keys: {list(f.keys())}\")\n",
    "    if 'Profiling_traces' in f:\n",
    "        print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
    "    if 'Attack_traces' in f:\n",
    "        print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Configure Training Parameters\n",
    "\n",
    "Set up the training configuration. You can modify these parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Training Configuration:\n",
      "  Dataset: ASCAD\n",
      "  Input length: 10000\n",
      "  Batch size: 16\n",
      "  Training steps: 4,000,000\n",
      "  Model layers: 2\n",
      "  Model dimension: 128\n",
      "  Attention heads: 8\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data config\n",
    "    'data_path': DATASET_PATH,\n",
    "    'dataset': 'ASCAD',\n",
    "    'input_length': 10000,  # or 40000 for full traces\n",
    "    'data_desync': 200,     # 400 for input_length=40000\n",
    "    \n",
    "    # Training config\n",
    "    'train_batch_size': 16,\n",
    "    'eval_batch_size': 16,\n",
    "    'train_steps': 4000000,\n",
    "    'warmup_steps': 1000000,\n",
    "    'iterations': 20000,\n",
    "    'save_steps': 40000,\n",
    "    \n",
    "    # Optimization config\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'clip': 0.25,\n",
    "    'min_lr_ratio': 0.004,\n",
    "    \n",
    "    # Model architecture\n",
    "    'n_layer': 2,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'n_head': 8,\n",
    "    'd_inner': 256,\n",
    "    'n_head_softmax': 8,\n",
    "    'd_head_softmax': 16,\n",
    "    'dropout': 0.05,\n",
    "    'conv_kernel_size': 3,\n",
    "    'n_conv_layer': 2,\n",
    "    'pool_size': 20,\n",
    "    'd_kernel_map': 512,\n",
    "    'beta_hat_2': 150,\n",
    "    'model_normalization': 'preLC',\n",
    "    'head_initialization': 'forward',\n",
    "    'softmax_attn': True,\n",
    "    \n",
    "    # Checkpoint config\n",
    "    'checkpoint_dir': './',\n",
    "    'result_path': 'results',\n",
    "    'warm_start': False,\n",
    "    'use_tpu': False,\n",
    "    'max_eval_batch': 100,\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"  Dataset: {config['dataset']}\")\n",
    "print(f\"  Input length: {config['input_length']}\")\n",
    "print(f\"  Batch size: {config['train_batch_size']}\")\n",
    "print(f\"  Training steps: {config['train_steps']:,}\")\n",
    "print(f\"  Model layers: {config['n_layer']}\")\n",
    "print(f\"  Model dimension: {config['d_model']}\")\n",
    "print(f\"  Attention heads: {config['n_head']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Train the Model\n",
    "\n",
    "Now let's train the EstraNet model. This will take several hours depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "\n",
      "Arguments: --use_tpu=False --data_path=data/ASCAD.h5 --dataset=ASCAD --checkpoint_dir=./ --warm_start=False --r...\n",
      "\n",
      "2026-02-08 16:03:29.150432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770566609.169106     718 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770566609.174557     718 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770566609.190607     718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770566609.190639     718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770566609.190645     718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770566609.190648     718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚úÖ File replaced with fixed version\n",
      "‚úÖ Checkpoints deleted\n",
      "\n",
      "‚ö†Ô∏è  NOW RESTART RUNTIME: Runtime ‚Üí Restart runtime\n",
      "Then run your training again\n",
      "INFO:tensorflow:\n",
      "I0208 16:03:35.061782 140373151883264 train_trans.py:382] \n",
      "INFO:tensorflow:\n",
      "I0208 16:03:35.062002 140373151883264 train_trans.py:383] \n",
      "INFO:tensorflow:use_tpu               : False\n",
      "I0208 16:03:35.062093 140373151883264 train_trans.py:384] use_tpu               : False\n",
      "INFO:tensorflow:data_path             : data/ASCAD.h5\n",
      "I0208 16:03:35.062161 140373151883264 train_trans.py:385] data_path             : data/ASCAD.h5\n",
      "INFO:tensorflow:dataset               : ASCAD\n",
      "I0208 16:03:35.062229 140373151883264 train_trans.py:386] dataset               : ASCAD\n",
      "INFO:tensorflow:checkpoint_dir        : ./\n",
      "I0208 16:03:35.062284 140373151883264 train_trans.py:387] checkpoint_dir        : ./\n",
      "INFO:tensorflow:checkpoint_idx        : 0\n",
      "I0208 16:03:35.062349 140373151883264 train_trans.py:388] checkpoint_idx        : 0\n",
      "INFO:tensorflow:warm_start            : False\n",
      "I0208 16:03:35.062405 140373151883264 train_trans.py:389] warm_start            : False\n",
      "INFO:tensorflow:result_path           : results\n",
      "I0208 16:03:35.062454 140373151883264 train_trans.py:390] result_path           : results\n",
      "INFO:tensorflow:do_train              : True\n",
      "I0208 16:03:35.062504 140373151883264 train_trans.py:391] do_train              : True\n",
      "INFO:tensorflow:learning_rate         : 0.00025\n",
      "I0208 16:03:35.062557 140373151883264 train_trans.py:392] learning_rate         : 0.00025\n",
      "INFO:tensorflow:clip                  : 0.25\n",
      "I0208 16:03:35.062609 140373151883264 train_trans.py:393] clip                  : 0.25\n",
      "INFO:tensorflow:min_lr_ratio          : 0.004\n",
      "I0208 16:03:35.062661 140373151883264 train_trans.py:394] min_lr_ratio          : 0.004\n",
      "INFO:tensorflow:warmup_steps          : 1000000\n",
      "I0208 16:03:35.062711 140373151883264 train_trans.py:395] warmup_steps          : 1000000\n",
      "INFO:tensorflow:input_length          : 10000\n",
      "I0208 16:03:35.062761 140373151883264 train_trans.py:396] input_length          : 10000\n",
      "INFO:tensorflow:data_desync           : 200\n",
      "I0208 16:03:35.062809 140373151883264 train_trans.py:397] data_desync           : 200\n",
      "INFO:tensorflow:train_batch_size      : 16\n",
      "I0208 16:03:35.062858 140373151883264 train_trans.py:398] train_batch_size      : 16\n",
      "INFO:tensorflow:eval_batch_size       : 16\n",
      "I0208 16:03:35.062906 140373151883264 train_trans.py:399] eval_batch_size       : 16\n",
      "INFO:tensorflow:train_steps           : 4000000\n",
      "I0208 16:03:35.062954 140373151883264 train_trans.py:400] train_steps           : 4000000\n",
      "INFO:tensorflow:iterations            : 20000\n",
      "I0208 16:03:35.063003 140373151883264 train_trans.py:401] iterations            : 20000\n",
      "INFO:tensorflow:save_steps            : 40000\n",
      "I0208 16:03:35.063051 140373151883264 train_trans.py:402] save_steps            : 40000\n",
      "INFO:tensorflow:n_layer               : 2\n",
      "I0208 16:03:35.063105 140373151883264 train_trans.py:403] n_layer               : 2\n",
      "INFO:tensorflow:d_model               : 128\n",
      "I0208 16:03:35.063155 140373151883264 train_trans.py:404] d_model               : 128\n",
      "INFO:tensorflow:d_head                : 32\n",
      "I0208 16:03:35.063244 140373151883264 train_trans.py:405] d_head                : 32\n",
      "INFO:tensorflow:n_head                : 8\n",
      "I0208 16:03:35.063296 140373151883264 train_trans.py:406] n_head                : 8\n",
      "INFO:tensorflow:d_inner               : 256\n",
      "I0208 16:03:35.063359 140373151883264 train_trans.py:407] d_inner               : 256\n",
      "INFO:tensorflow:n_head_softmax        : 8\n",
      "I0208 16:03:35.063412 140373151883264 train_trans.py:408] n_head_softmax        : 8\n",
      "INFO:tensorflow:d_head_softmax        : 16\n",
      "I0208 16:03:35.063465 140373151883264 train_trans.py:409] d_head_softmax        : 16\n",
      "INFO:tensorflow:dropout               : 0.05\n",
      "I0208 16:03:35.063518 140373151883264 train_trans.py:410] dropout               : 0.05\n",
      "INFO:tensorflow:conv_kernel_size      : 3\n",
      "I0208 16:03:35.063570 140373151883264 train_trans.py:411] conv_kernel_size      : 3\n",
      "INFO:tensorflow:n_conv_layer          : 2\n",
      "I0208 16:03:35.063620 140373151883264 train_trans.py:412] n_conv_layer          : 2\n",
      "INFO:tensorflow:pool_size             : 20\n",
      "I0208 16:03:35.063671 140373151883264 train_trans.py:413] pool_size             : 20\n",
      "INFO:tensorflow:d_kernel_map          : 512\n",
      "I0208 16:03:35.063721 140373151883264 train_trans.py:414] d_kernel_map          : 512\n",
      "INFO:tensorflow:beta_hat_2            : 150\n",
      "I0208 16:03:35.063771 140373151883264 train_trans.py:415] beta_hat_2            : 150\n",
      "INFO:tensorflow:model_normalization   : preLC\n",
      "I0208 16:03:35.063821 140373151883264 train_trans.py:416] model_normalization   : preLC\n",
      "INFO:tensorflow:head_initialization   : forward\n",
      "I0208 16:03:35.063891 140373151883264 train_trans.py:417] head_initialization   : forward\n",
      "INFO:tensorflow:softmax_attn          : True\n",
      "I0208 16:03:35.063946 140373151883264 train_trans.py:418] softmax_attn          : True\n",
      "INFO:tensorflow:max_eval_batch        : 100\n",
      "I0208 16:03:35.063999 140373151883264 train_trans.py:419] max_eval_batch        : 100\n",
      "INFO:tensorflow:output_attn           : False\n",
      "I0208 16:03:35.064053 140373151883264 train_trans.py:420] output_attn           : False\n",
      "INFO:tensorflow:\n",
      "I0208 16:03:35.064107 140373151883264 train_trans.py:421] \n",
      "INFO:tensorflow:\n",
      "I0208 16:03:35.064161 140373151883264 train_trans.py:422] \n",
      "INFO:tensorflow:Number of accelerators: 1\n",
      "I0208 16:04:02.126725 140373151883264 train_trans.py:462] Number of accelerators: 1\n",
      "INFO:tensorflow:num of train batches 3125\n",
      "I0208 16:04:02.126881 140373151883264 train_trans.py:475] num of train batches 3125\n",
      "INFO:tensorflow:num of test batches 625\n",
      "I0208 16:04:02.126947 140373151883264 train_trans.py:476] num of test batches 625\n",
      "2026-02-08 16:04:02.343744: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1770566642.345737     718 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0208 16:04:04.302839 140373151883264 functional_saver.py:440] Sharding callback duration: 7 microseconds\n",
      "INFO:tensorflow:Model saved in path: ./trans_long-1\n",
      "I0208 16:04:04.313128 140373151883264 train_trans.py:226] Model saved in path: ./trans_long-1\n",
      "INFO:tensorflow:Starting training ... \n",
      "I0208 16:04:04.313666 140373151883264 train_trans.py:284] Starting training ... \n",
      "Traceback (most recent call last):\n",
      "  File \"/content/EstraNet/train_trans.py\", line 530, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "             ^^^^^^^^^^\n",
      "  File \"/content/EstraNet/train_trans.py\", line 478, in main\n",
      "    train(train_data.GetTFRecords(FLAGS.train_batch_size, training=True), \\\n",
      "  File \"/content/EstraNet/train_trans.py\", line 289, in train\n",
      "    train_steps(train_iter, tf.convert_to_tensor(FLAGS.iterations), \\\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_file555eg90n.py\", line 66, in tf__train_steps\n",
      "    ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(steps),), None, fscope), None, loop_body, get_state_1, set_state_1, ('global_step',), {'iterate_names': '_'})\n",
      "  File \"/tmp/__autograph_generated_file555eg90n.py\", line 62, in loop_body\n",
      "    ag__.converted_call(ag__.ld(strategy).run, (ag__.ld(step_fn),), dict(args=(ag__.ld(inps), ag__.ld(lbls), ag__.ld(global_step))), fscope)\n",
      "  File \"/tmp/__autograph_generated_file555eg90n.py\", line 23, in step_fn\n",
      "    logits = ag__.converted_call(ag__.ld(model), (), dict(inputs=ag__.ld(inps), softmax_attn_smoothing=ag__.ld(softmax_attn_smoothing), training=True), fscope_1)[0]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/content/EstraNet/transformer.py\", line 116, in call\n",
      "    core, score = self.out_attn(core, softmax_attn_smoothing, training=training)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/content/EstraNet/train_trans.py\", line 243, in step_fn  *\n",
      "        logits = model(inputs=inps, softmax_attn_smoothing=softmax_attn_smoothing, training=True)[0]\n",
      "    File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/content/EstraNet/transformer.py\", line 116, in call\n",
      "        core, score = self.out_attn(core, softmax_attn_smoothing, training=training)\n",
      "\n",
      "    ValueError: Exception encountered when calling Transformer.call().\n",
      "    \n",
      "    \u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 1.0 (of type <class 'float'>)\u001b[0m\n",
      "    \n",
      "    Arguments received by Transformer.call():\n",
      "      ‚Ä¢ inputs=tf.Tensor(shape=(16, 700), dtype=float32)\n",
      "      ‚Ä¢ softmax_attn_smoothing=1.0\n",
      "      ‚Ä¢ training=True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the command arguments\n",
    "args = [\n",
    "    f'--use_tpu={config[\"use_tpu\"]}',\n",
    "    f'--data_path={config[\"data_path\"]}',\n",
    "    f'--dataset={config[\"dataset\"]}',\n",
    "    f'--checkpoint_dir={config[\"checkpoint_dir\"]}',\n",
    "    f'--warm_start={config[\"warm_start\"]}',\n",
    "    f'--result_path={config[\"result_path\"]}',\n",
    "    f'--learning_rate={config[\"learning_rate\"]}',\n",
    "    f'--clip={config[\"clip\"]}',\n",
    "    f'--min_lr_ratio={config[\"min_lr_ratio\"]}',\n",
    "    f'--warmup_steps={config[\"warmup_steps\"]}',\n",
    "    f'--input_length={config[\"input_length\"]}',\n",
    "    f'--data_desync={config[\"data_desync\"]}',\n",
    "    f'--train_batch_size={config[\"train_batch_size\"]}',\n",
    "    f'--eval_batch_size={config[\"eval_batch_size\"]}',\n",
    "    f'--train_steps={config[\"train_steps\"]}',\n",
    "    f'--iterations={config[\"iterations\"]}',\n",
    "    f'--save_steps={config[\"save_steps\"]}',\n",
    "    f'--n_layer={config[\"n_layer\"]}',\n",
    "    f'--d_model={config[\"d_model\"]}',\n",
    "    f'--d_head={config[\"d_head\"]}',\n",
    "    f'--n_head={config[\"n_head\"]}',\n",
    "    f'--d_inner={config[\"d_inner\"]}',\n",
    "    f'--n_head_softmax={config[\"n_head_softmax\"]}',\n",
    "    f'--d_head_softmax={config[\"d_head_softmax\"]}',\n",
    "    f'--dropout={config[\"dropout\"]}',\n",
    "    f'--conv_kernel_size={config[\"conv_kernel_size\"]}',\n",
    "    f'--n_conv_layer={config[\"n_conv_layer\"]}',\n",
    "    f'--pool_size={config[\"pool_size\"]}',\n",
    "    f'--d_kernel_map={config[\"d_kernel_map\"]}',\n",
    "    f'--beta_hat_2={config[\"beta_hat_2\"]}',\n",
    "    f'--model_normalization={config[\"model_normalization\"]}',\n",
    "    f'--head_initialization={config[\"head_initialization\"]}',\n",
    "    f'--softmax_attn={config[\"softmax_attn\"]}',\n",
    "    f'--max_eval_batch={config[\"max_eval_batch\"]}',\n",
    "    '--do_train=True'\n",
    "]\n",
    "\n",
    "args_str = ' '.join(args)\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "print(f\"Arguments: {args_str[:100]}...\\n\")\n",
    "\n",
    "# Execute training - output will be displayed automatically\n",
    "!python train_trans.py {args_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Evaluate the Model (Run After Training Completes)\n",
    "\n",
    "‚ö†Ô∏è **Run this cell only after training completes!**\n",
    "\n",
    "This cell evaluates the trained model on the test set. You can run it multiple times to check different checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1 checkpoint(s)\n",
      "   Latest checkpoint: trans_long-1.index\n",
      "\n",
      "üìä Starting evaluation...\n",
      "\n",
      "2026-02-08 16:04:07.501399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770566647.521867     915 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770566647.527381     915 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770566647.540752     915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770566647.540779     915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770566647.540783     915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770566647.540787     915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚úÖ File replaced with fixed version\n",
      "‚úÖ Checkpoints deleted\n",
      "\n",
      "‚ö†Ô∏è  NOW RESTART RUNTIME: Runtime ‚Üí Restart runtime\n",
      "Then run your training again\n",
      "INFO:tensorflow:\n",
      "I0208 16:04:13.615030 138413922349056 train_trans.py:382] \n",
      "INFO:tensorflow:\n",
      "I0208 16:04:13.615272 138413922349056 train_trans.py:383] \n",
      "INFO:tensorflow:use_tpu               : False\n",
      "I0208 16:04:13.615392 138413922349056 train_trans.py:384] use_tpu               : False\n",
      "INFO:tensorflow:data_path             : data/ASCAD.h5\n",
      "I0208 16:04:13.615472 138413922349056 train_trans.py:385] data_path             : data/ASCAD.h5\n",
      "INFO:tensorflow:dataset               : ASCAD\n",
      "I0208 16:04:13.615538 138413922349056 train_trans.py:386] dataset               : ASCAD\n",
      "INFO:tensorflow:checkpoint_dir        : ./\n",
      "I0208 16:04:13.615597 138413922349056 train_trans.py:387] checkpoint_dir        : ./\n",
      "INFO:tensorflow:checkpoint_idx        : 0\n",
      "I0208 16:04:13.615663 138413922349056 train_trans.py:388] checkpoint_idx        : 0\n",
      "INFO:tensorflow:warm_start            : False\n",
      "I0208 16:04:13.615713 138413922349056 train_trans.py:389] warm_start            : False\n",
      "INFO:tensorflow:result_path           : results\n",
      "I0208 16:04:13.615762 138413922349056 train_trans.py:390] result_path           : results\n",
      "INFO:tensorflow:do_train              : False\n",
      "I0208 16:04:13.615812 138413922349056 train_trans.py:391] do_train              : False\n",
      "INFO:tensorflow:learning_rate         : 0.00025\n",
      "I0208 16:04:13.615864 138413922349056 train_trans.py:392] learning_rate         : 0.00025\n",
      "INFO:tensorflow:clip                  : 0.25\n",
      "I0208 16:04:13.615917 138413922349056 train_trans.py:393] clip                  : 0.25\n",
      "INFO:tensorflow:min_lr_ratio          : 0.004\n",
      "I0208 16:04:13.615968 138413922349056 train_trans.py:394] min_lr_ratio          : 0.004\n",
      "INFO:tensorflow:warmup_steps          : 1000000\n",
      "I0208 16:04:13.616037 138413922349056 train_trans.py:395] warmup_steps          : 1000000\n",
      "INFO:tensorflow:input_length          : 10000\n",
      "I0208 16:04:13.616092 138413922349056 train_trans.py:396] input_length          : 10000\n",
      "INFO:tensorflow:data_desync           : 0\n",
      "I0208 16:04:13.616143 138413922349056 train_trans.py:397] data_desync           : 0\n",
      "INFO:tensorflow:train_batch_size      : 16\n",
      "I0208 16:04:13.616194 138413922349056 train_trans.py:398] train_batch_size      : 16\n",
      "INFO:tensorflow:eval_batch_size       : 16\n",
      "I0208 16:04:13.616261 138413922349056 train_trans.py:399] eval_batch_size       : 16\n",
      "INFO:tensorflow:train_steps           : 4000000\n",
      "I0208 16:04:13.616310 138413922349056 train_trans.py:400] train_steps           : 4000000\n",
      "INFO:tensorflow:iterations            : 20000\n",
      "I0208 16:04:13.616401 138413922349056 train_trans.py:401] iterations            : 20000\n",
      "INFO:tensorflow:save_steps            : 40000\n",
      "I0208 16:04:13.616452 138413922349056 train_trans.py:402] save_steps            : 40000\n",
      "INFO:tensorflow:n_layer               : 2\n",
      "I0208 16:04:13.616502 138413922349056 train_trans.py:403] n_layer               : 2\n",
      "INFO:tensorflow:d_model               : 128\n",
      "I0208 16:04:13.616551 138413922349056 train_trans.py:404] d_model               : 128\n",
      "INFO:tensorflow:d_head                : 32\n",
      "I0208 16:04:13.616600 138413922349056 train_trans.py:405] d_head                : 32\n",
      "INFO:tensorflow:n_head                : 8\n",
      "I0208 16:04:13.616650 138413922349056 train_trans.py:406] n_head                : 8\n",
      "INFO:tensorflow:d_inner               : 256\n",
      "I0208 16:04:13.616699 138413922349056 train_trans.py:407] d_inner               : 256\n",
      "INFO:tensorflow:n_head_softmax        : 8\n",
      "I0208 16:04:13.616750 138413922349056 train_trans.py:408] n_head_softmax        : 8\n",
      "INFO:tensorflow:d_head_softmax        : 16\n",
      "I0208 16:04:13.616802 138413922349056 train_trans.py:409] d_head_softmax        : 16\n",
      "INFO:tensorflow:dropout               : 0.05\n",
      "I0208 16:04:13.616855 138413922349056 train_trans.py:410] dropout               : 0.05\n",
      "INFO:tensorflow:conv_kernel_size      : 3\n",
      "I0208 16:04:13.616905 138413922349056 train_trans.py:411] conv_kernel_size      : 3\n",
      "INFO:tensorflow:n_conv_layer          : 2\n",
      "I0208 16:04:13.616956 138413922349056 train_trans.py:412] n_conv_layer          : 2\n",
      "INFO:tensorflow:pool_size             : 20\n",
      "I0208 16:04:13.617005 138413922349056 train_trans.py:413] pool_size             : 20\n",
      "INFO:tensorflow:d_kernel_map          : 512\n",
      "I0208 16:04:13.617055 138413922349056 train_trans.py:414] d_kernel_map          : 512\n",
      "INFO:tensorflow:beta_hat_2            : 150\n",
      "I0208 16:04:13.617105 138413922349056 train_trans.py:415] beta_hat_2            : 150\n",
      "INFO:tensorflow:model_normalization   : preLC\n",
      "I0208 16:04:13.617154 138413922349056 train_trans.py:416] model_normalization   : preLC\n",
      "INFO:tensorflow:head_initialization   : forward\n",
      "I0208 16:04:13.617204 138413922349056 train_trans.py:417] head_initialization   : forward\n",
      "INFO:tensorflow:softmax_attn          : True\n",
      "I0208 16:04:13.617261 138413922349056 train_trans.py:418] softmax_attn          : True\n",
      "INFO:tensorflow:max_eval_batch        : 100\n",
      "I0208 16:04:13.617311 138413922349056 train_trans.py:419] max_eval_batch        : 100\n",
      "INFO:tensorflow:output_attn           : False\n",
      "I0208 16:04:13.617375 138413922349056 train_trans.py:420] output_attn           : False\n",
      "INFO:tensorflow:\n",
      "I0208 16:04:13.617426 138413922349056 train_trans.py:421] \n",
      "INFO:tensorflow:\n",
      "I0208 16:04:13.617477 138413922349056 train_trans.py:422] \n",
      "INFO:tensorflow:Number of accelerators: 1\n",
      "I0208 16:04:41.410903 138413922349056 train_trans.py:462] Number of accelerators: 1\n",
      "INFO:tensorflow:num of test batches 625\n",
      "I0208 16:04:41.411087 138413922349056 train_trans.py:484] num of test batches 625\n",
      "2026-02-08 16:04:41.596284: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1770566681.596455     915 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "INFO:tensorflow:Restoring checkpoint: ./trans_long-1\n",
      "I0208 16:04:42.820766 138413922349056 train_trans.py:366] Restoring checkpoint: ./trans_long-1\n",
      "INFO:tensorflow:Restored checkpoint: ./trans_long-1\n",
      "I0208 16:04:42.827649 138413922349056 train_trans.py:369] Restored checkpoint: ./trans_long-1\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/EstraNet/train_trans.py\", line 530, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "             ^^^^^^^^^^\n",
      "  File \"/content/EstraNet/train_trans.py\", line 486, in main\n",
      "    output = evaluate(test_data.GetTFRecords(FLAGS.eval_batch_size, training=False), \n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/EstraNet/train_trans.py\", line 377, in evaluate\n",
      "    output = model.predict(data)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/content/EstraNet/transformer.py\", line 116, in call\n",
      "    core, score = self.out_attn(core, softmax_attn_smoothing, training=training)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: Exception encountered when calling Transformer.call().\n",
      "\n",
      "\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 1 (of type <class 'int'>)\u001b[0m\n",
      "\n",
      "Arguments received by Transformer.call():\n",
      "  ‚Ä¢ inputs=tf.Tensor(shape=(16, 700), dtype=float32)\n",
      "  ‚Ä¢ softmax_attn_smoothing=1\n",
      "  ‚Ä¢ training=False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check if checkpoints exist\n",
    "checkpoint_files = glob.glob('*.index')\n",
    "if not checkpoint_files:\n",
    "    print(\"‚ö†Ô∏è No checkpoint files found!\")\n",
    "    print(\"   Make sure training has created at least one checkpoint.\")\n",
    "    print(\"   Checkpoints are saved every\", config['save_steps'], \"steps.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(checkpoint_files)} checkpoint(s)\")\n",
    "    print(f\"   Latest checkpoint: {sorted(checkpoint_files)[-1]}\\n\")\n",
    "    \n",
    "    # Build evaluation arguments\n",
    "    eval_args = [\n",
    "        f'--use_tpu={config[\"use_tpu\"]}',\n",
    "        f'--data_path={config[\"data_path\"]}',\n",
    "        f'--dataset={config[\"dataset\"]}',\n",
    "        f'--checkpoint_dir={config[\"checkpoint_dir\"]}',\n",
    "        '--checkpoint_idx=0',\n",
    "        f'--warm_start={config[\"warm_start\"]}',\n",
    "        f'--result_path={config[\"result_path\"]}',\n",
    "        f'--learning_rate={config[\"learning_rate\"]}',\n",
    "        f'--clip={config[\"clip\"]}',\n",
    "        f'--min_lr_ratio={config[\"min_lr_ratio\"]}',\n",
    "        f'--warmup_steps={config[\"warmup_steps\"]}',\n",
    "        f'--input_length={config[\"input_length\"]}',\n",
    "        f'--train_batch_size={config[\"train_batch_size\"]}',\n",
    "        f'--eval_batch_size={config[\"eval_batch_size\"]}',\n",
    "        f'--train_steps={config[\"train_steps\"]}',\n",
    "        f'--iterations={config[\"iterations\"]}',\n",
    "        f'--save_steps={config[\"save_steps\"]}',\n",
    "        f'--n_layer={config[\"n_layer\"]}',\n",
    "        f'--d_model={config[\"d_model\"]}',\n",
    "        f'--d_head={config[\"d_head\"]}',\n",
    "        f'--n_head={config[\"n_head\"]}',\n",
    "        f'--d_inner={config[\"d_inner\"]}',\n",
    "        f'--n_head_softmax={config[\"n_head_softmax\"]}',\n",
    "        f'--d_head_softmax={config[\"d_head_softmax\"]}',\n",
    "        f'--dropout={config[\"dropout\"]}',\n",
    "        f'--conv_kernel_size={config[\"conv_kernel_size\"]}',\n",
    "        f'--n_conv_layer={config[\"n_conv_layer\"]}',\n",
    "        f'--pool_size={config[\"pool_size\"]}',\n",
    "        f'--d_kernel_map={config[\"d_kernel_map\"]}',\n",
    "        f'--beta_hat_2={config[\"beta_hat_2\"]}',\n",
    "        f'--model_normalization={config[\"model_normalization\"]}',\n",
    "        f'--head_initialization={config[\"head_initialization\"]}',\n",
    "        f'--softmax_attn={config[\"softmax_attn\"]}',\n",
    "        f'--max_eval_batch={config[\"max_eval_batch\"]}',\n",
    "        '--output_attn=False',\n",
    "        '--do_train=False'\n",
    "    ]\n",
    "    \n",
    "    eval_args_str = ' '.join(eval_args)\n",
    "    print(\"üìä Starting evaluation...\\n\")\n",
    "    \n",
    "    # Execute evaluation\n",
    "    !python train_trans.py {eval_args_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ View Results\n",
    "\n",
    "Check the results directory for evaluation metrics and guessing entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Results directory not found\n",
      "\n",
      "üíæ Checkpoint files:\n",
      "  - checkpoint\n",
      "  - trans_long-1.data-00000-of-00001\n",
      "  - trans_long-1.index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in results directory\n",
    "if os.path.exists('results'):\n",
    "    print(\"üìÅ Results directory contents:\")\n",
    "    for file in os.listdir('results'):\n",
    "        filepath = os.path.join('results', file)\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"  - {file} ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Results directory not found\")\n",
    "\n",
    "# List checkpoint files\n",
    "print(\"\\nüíæ Checkpoint files:\")\n",
    "checkpoint_files = [f for f in os.listdir('.') if 'checkpoint' in f or f.endswith('.index') or f.endswith('.data-00000-of-00001')]\n",
    "if checkpoint_files:\n",
    "    for file in sorted(checkpoint_files)[:10]:  # Show first 10\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"  No checkpoints found yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notes\n",
    "\n",
    "### Training Tips:\n",
    "- **GPU Acceleration**: Make sure GPU is enabled in Colab (Runtime > Change runtime type > GPU)\n",
    "- **Training Time**: Full training with 4M steps will take many hours. Consider reducing `train_steps` for testing.\n",
    "- **Checkpoints**: Models are saved every 40,000 steps. You can resume training from checkpoints.\n",
    "- **Memory**: If you run out of memory, try reducing `train_batch_size` or `input_length`.\n",
    "\n",
    "### Quick Test Run:\n",
    "For a quick test, modify the config:\n",
    "```python\n",
    "config['train_steps'] = 100000  # Reduced from 4M\n",
    "config['warmup_steps'] = 10000  # Reduced from 1M\n",
    "config['save_steps'] = 10000    # Save more frequently\n",
    "```\n",
    "\n",
    "### Evaluation:\n",
    "- **When to run**: Only run the evaluation cell (Section 7) after training has created at least one checkpoint\n",
    "- **Checkpoints**: The evaluation will use the latest checkpoint automatically\n",
    "- **Re-run**: You can re-run the evaluation cell anytime to check the latest checkpoint\n",
    "\n",
    "### Compatibility Fixes Applied:\n",
    "This notebook automatically fixes TensorFlow 2.13+ / Keras 3 compatibility issues:\n",
    "- ‚úÖ Replaced SyncBatchNormalization with BatchNormalization (doesn't exist in TF 2.13+)\n",
    "- ‚úÖ Fixed integer comparison syntax (changed 'is' to '==')\n",
    "- ‚úÖ Fixed add_weight() method calls in fast_attention.py\n",
    "- ‚úÖ Fixed reset_states() method calls in train_trans.py (renamed to reset_state())\n",
    "- ‚úÖ Fixed model call signature for Keras 3 (keyword arguments required)\n",
    "- ‚úÖ Fixed PositionalFeature call signature for Keras 3 (ALL arguments as keywords)\n",
    "\n",
    "**Note**: These fixes work with TensorFlow 2.13, 2.19, and other Keras 3-based versions.\n",
    "\n",
    "### References:\n",
    "- **Paper**: [IACR TCHES 2024](https://tches.iacr.org/index.php/TCHES/article/view/11255)\n",
    "- **GitHub**: [suvadeep-iitb/EstraNet](https://github.com/suvadeep-iitb/EstraNet)\n",
    "- **ASCAD Dataset**: [ANSSI-FR/ASCAD](https://github.com/ANSSI-FR/ASCAD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

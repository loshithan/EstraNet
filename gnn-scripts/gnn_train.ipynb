{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef5869b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ef5869b",
        "outputId": "af2e14d0-75fa-4e9e-9ca3-ac051e54a4c7"
      },
      "outputs": [],
      "source": [
        "# # Check if running on Google Colab\n",
        "# try:\n",
        "#     import google.colab\n",
        "#     IN_COLAB = True\n",
        "#     print(\"‚úÖ Running on Google Colab\")\n",
        "# except:\n",
        "#     IN_COLAB = False\n",
        "#     print(\"üìù Running on local Jupyter\")\n",
        "\n",
        "# # Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
        "# print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "# if tf.config.list_physical_devices('GPU'):\n",
        "#     print(\"üöÄ GPU detected! Training will be accelerated.\")\n",
        "# else:\n",
        "#     print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")\n",
        "#     if IN_COLAB:\n",
        "#         print(\"üí° Enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba69719b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba69719b",
        "outputId": "26aeb231-8c80-498e-b984-d1a91d141d21"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "os.chdir('/content')\n",
        "if os.path.exists('EstraNet'):\n",
        "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
        "\n",
        "!git clone https://github.com/loshithan/EstraNet.git\n",
        "os.chdir('EstraNet')\n",
        "print(f\"‚úÖ Clean! Directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4307c1",
      "metadata": {
        "id": "5c4307c1"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "import gdown\n",
        "\n",
        "# # Create data directory\n",
        "# os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# # ASCADf dataset configuration\n",
        "# file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
        "# DATASET_PATH = \"data/ASCAD.h5\"\n",
        "\n",
        "# if not os.path.exists(DATASET_PATH):\n",
        "#     print(\"üì• Downloading ASCADf dataset from Google Drive...\")\n",
        "#     print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
        "\n",
        "#     # Download using gdown\n",
        "#     gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
        "\n",
        "#     print(\"\\n‚úÖ Dataset downloaded successfully!\")\n",
        "# else:\n",
        "#     print(\"‚úÖ Dataset already exists\")\n",
        "\n",
        "# # Verify dataset\n",
        "# import h5py\n",
        "# with h5py.File(DATASET_PATH, 'r') as f:\n",
        "#     print(f\"\\nüìä Dataset info:\")\n",
        "#     print(f\"  Keys: {list(f.keys())}\")\n",
        "#     if 'Profiling_traces' in f:\n",
        "#         print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
        "#     if 'Attack_traces' in f:\n",
        "#         print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "027da6d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "027da6d8",
        "outputId": "fa24d43e-7eb0-4f2a-82f5-49a683059d91"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "print(\"üì¶ Installing dependencies...\\n\")\n",
        "# %pip install -q absl-py==2.3.1 numpy==1.24.3 scipy==1.10.1 h5py==3.11.0\n",
        "\n",
        "# Install gdown for downloading from Google Drive\n",
        "%pip install -q gdown\n",
        "\n",
        "# Note: Using TensorFlow version pre-installed in Colab (2.16+ / 2.19+)\n",
        "# The compatibility fixes in Section 3 work with all TensorFlow 2.13+ versions\n",
        "print(\"\\n‚úÖ All dependencies installed!\")\n",
        "print(f\"Using TensorFlow {tf.__version__} (pre-installed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fd80811",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fd80811",
        "outputId": "c7c4b098-e9d4-4624-a448-52156f3c61da"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAIN UPGRADED \"ATTENTION GNN\" MODEL IN COLAB\n",
        "# ============================================================================\n",
        "# Paste this into a new Colab cell\n",
        "\n",
        "print(\"üî∑ Training Upgraded GNN Model (v2: Attention + Deeper)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'checkpoint_dir': '/content/drive/MyDrive/EstraNet/checkpoints_gnn_attention_v2',\n",
        "    'result_path': 'results/gnn_attention',\n",
        "    'train_steps': 50000,   # Increased for convergence\n",
        "    'save_steps': 2000,\n",
        "    'train_batch_size': 256,\n",
        "    'eval_batch_size': 32,\n",
        "    'learning_rate': 0.0002, # Slightly higher learning rate for GNN\n",
        "    'model_type': 'gnn',\n",
        "    \n",
        "    # NEW GNN HYPERPARAMETERS\n",
        "    'n_gcn_layers': 4,       # Deeper (was 2)\n",
        "    'k_neighbors': 15,       # Wider Context (was 5)\n",
        "    'graph_pooling': 'attention' # The Secret Sauce!\n",
        "}\n",
        "\n",
        "import os\n",
        "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
        "os.makedirs(CONFIG['result_path'], exist_ok=True)\n",
        "\n",
        "# Build training command\n",
        "train_cmd = f\"\"\"\n",
        "python train_trans.py \\\\\n",
        "    --data_path=data/ASCAD.h5 \\\\\n",
        "    --checkpoint_dir={CONFIG['checkpoint_dir']} \\\\\n",
        "    --model_type={CONFIG['model_type']} \\\\\n",
        "    --dataset=ASCAD \\\\\n",
        "    --input_length=700 \\\\\n",
        "    \n",
        "    # --- MODEL ARCHITECTURE ---\n",
        "    --d_model=128 \\\\\n",
        "    --n_gcn_layers={CONFIG['n_gcn_layers']} \\\\\n",
        "    --k_neighbors={CONFIG['k_neighbors']} \\\\\n",
        "    --graph_pooling={CONFIG['graph_pooling']} \\\\\n",
        "    --conv_kernel_size=3 \\\\\n",
        "    --n_conv_layer=2 \\\\\n",
        "    --pool_size=2 \\\\\n",
        "    --dropout=0.1 \\\\\n",
        "    \n",
        "    # --- TRAINING PARAMS ---\n",
        "    --do_train=True \\\\\n",
        "    --learning_rate={CONFIG['learning_rate']} \\\\\n",
        "    --clip=0.25 \\\\\n",
        "    --min_lr_ratio=0.004 \\\\\n",
        "    --warmup_steps=1000 \\\\\n",
        "    --train_batch_size={CONFIG['train_batch_size']} \\\\\n",
        "    --eval_batch_size={CONFIG['eval_batch_size']} \\\\\n",
        "    --train_steps={CONFIG['train_steps']} \\\\\n",
        "    --iterations=500 \\\\\n",
        "    --save_steps={CONFIG['save_steps']} \\\\\n",
        "    --result_path={CONFIG['result_path']}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting GNN training...\")\n",
        "print(f\"Model: GNN-Attention (Est. 350k parameters - Still lightweight!)\")\n",
        "print(f\"Features: {CONFIG['n_gcn_layers']} Layers, {CONFIG['k_neighbors']} Neighbors, Attention Pooling\")\n",
        "print(f\"Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
        "print(f\"Training steps: {CONFIG['train_steps']:,}\\n\")\n",
        "\n",
        "!{train_cmd}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oRg5hbdROo8Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRg5hbdROo8Q",
        "outputId": "ae4a3fc5-b777-40ff-9079-92d5c47ed3aa"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TEST GNN ARCHITECTURE (Rank 29 Config)\n",
        "# ============================================================================\n",
        "# Paste into a Colab cell to verify the model builds correctly\n",
        "# with the 'Rank 29' parameters (Pool=2, Input=700).\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Fix: Copy GNN scripts from subfolder to root\n",
        "if os.path.exists('gnn-scripts'):\n",
        "    print(\"üìÇ Moving GNN scripts to root...\")\n",
        "    for f in os.listdir('gnn-scripts'):\n",
        "        if f.endswith('.py'):\n",
        "            shutil.copy(os.path.join('gnn-scripts', f), '.')\n",
        "            print(f\"   Copied {f}\")\n",
        "\n",
        "print(\"\\nüöÄ Running test_gnn.py...\")\n",
        "!python test_gnn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb82f2c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb82f2c9",
        "outputId": "db46c25a-641b-4512-92fd-8183e35cfb14"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EVALUATE GNN (RANK 29 MODEL)\n",
        "# ============================================================================\n",
        "# This script evaluates the CORRECT configuration (Pool=2, Input=700).\n",
        "\n",
        "print(\"üîç Evaluating GNN Checkpoints (Correct Config)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "from gnn_estranet import GNNEstraNet\n",
        "from evaluation_utils import compute_key_rank\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ----------------------------------------------------------------------------\n",
        "# Must match RETRAIN_GNN_CORRECTED.txt\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/EstraNet/checkpoints_gnn'\n",
        "INPUT_LENGTH = 700\n",
        "POOL_SIZE = 2\n",
        "N_TRACES = 2000 # Fast check (use 10000 for full precision)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. LOAD DATA\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\nüì• Loading ASCAD dataset...\")\n",
        "with h5py.File('data/ASCAD.h5', 'r') as f:\n",
        "    # Load limited traces for speed\n",
        "    traces = f['Attack_traces']['traces'][:N_TRACES]\n",
        "    metadata = f['Attack_traces']['metadata'][:N_TRACES]\n",
        "\n",
        "    # Process inputs (Slice to 700)\n",
        "    traces = traces[:, :INPUT_LENGTH]\n",
        "\n",
        "    # FIX: Cast to float32 for TensorFlow model\n",
        "    # This prevents the \"Value passed to parameter 'input' has DataType int8\" error\n",
        "    traces = traces.astype(np.float32)\n",
        "\n",
        "    # Extract labels/keys\n",
        "    plaintexts = metadata['plaintext'][:, 2].astype(np.uint8)\n",
        "    keys = metadata['key'][:, 2].astype(np.uint8)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(traces)} traces (Length: {traces.shape[1]})\")\n",
        "print(f\"   Trace Type: {traces.dtype} (Must be float32)\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2. BUILD MODEL (Correct Rank 29 Config)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\nüèóÔ∏è Building GNN model...\")\n",
        "model = GNNEstraNet(\n",
        "    n_gcn_layers=2,\n",
        "    d_model=128,\n",
        "    k_neighbors=5,\n",
        "    graph_pooling='mean',\n",
        "    d_head_softmax=16,\n",
        "    n_head_softmax=8,\n",
        "    dropout=0.05,\n",
        "    n_classes=256,\n",
        "    conv_kernel_size=3,\n",
        "    n_conv_layer=2,\n",
        "    pool_size=POOL_SIZE,  # CRITICAL: Must be 2\n",
        "    beta_hat_2=150,\n",
        "    model_normalization='preLC',\n",
        "    softmax_attn=True,\n",
        "    output_attn=False\n",
        ")\n",
        "\n",
        "# Dummy pass to initialize weights\n",
        "# Using float32 input explicitly\n",
        "dummy_input = tf.zeros((1, INPUT_LENGTH), dtype=tf.float32)\n",
        "model(dummy_input, softmax_attn_smoothing=None, training=False)\n",
        "print(f\"‚úÖ Model Built (Pool Size: {POOL_SIZE})\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3. EVALUATE CHECKPOINTS\n",
        "# ----------------------------------------------------------------------------\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    print(f\"‚ùå Error: Checkpoint folder not found: {CHECKPOINT_DIR}\")\n",
        "    # Fallback to local\n",
        "    CHECKPOINT_DIR = 'checkpoints'\n",
        "\n",
        "print(f\"üìÇ Searching for checkpoints in: {CHECKPOINT_DIR}\")\n",
        "ckpt_files = sorted(glob.glob(os.path.join(CHECKPOINT_DIR, \"*.index\")))\n",
        "if not ckpt_files:\n",
        "    print(\"‚ùå No checkpoints found!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found {len(ckpt_files)} checkpoints.\")\n",
        "\n",
        "for ckpt_path in ckpt_files:\n",
        "    # Remove extension to get prefix\n",
        "    prefix = ckpt_path.replace(\".index\", \"\")\n",
        "    fname = os.path.basename(prefix)\n",
        "\n",
        "    print(f\"\\nTesting {fname}...\")\n",
        "    try:\n",
        "        ckpt = tf.train.Checkpoint(model=model)\n",
        "        ckpt.restore(prefix).expect_partial()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to load {fname}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Inference\n",
        "    try:\n",
        "        preds = model.predict(traces, batch_size=256, verbose=0)\n",
        "\n",
        "        # FIX: Handle tuple return (e.g. if model returns (logits, attn))\n",
        "        if isinstance(preds, tuple):\n",
        "            preds = preds[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Prediction failed for {fname}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Rank\n",
        "    # Compute rank evolution (returns array of ranks for 1..N traces)\n",
        "    try:\n",
        "        ranks = compute_key_rank(preds, plaintexts, keys)\n",
        "        final_rank = ranks[-1]\n",
        "\n",
        "        # Check efficiency (traces to rank 0)\n",
        "        success_idx = np.where(ranks == 0)[0]\n",
        "        traces_to_0 = success_idx[0] + 1 if len(success_idx) > 0 else \">\" + str(N_TRACES)\n",
        "\n",
        "        print(f\"üèÜ Rank: {final_rank:.2f} | Broken at: {traces_to_0} traces\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Rank computation failed: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9de1f3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9de1f3e",
        "outputId": "c80aa5e3-13d5-4fba-e157-4df6e3368a38"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BACKUP LOCAL CHECKPOINTS TO DRIVE\n",
        "# ============================================================================\n",
        "import os\n",
        "import shutil\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"üìÇ Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2. Configuration\n",
        "SOURCE_DIR = '/content/checkpoints_gnn'  # Local Colab path\n",
        "# Check if it exists locally, otherwise check inside repo folder\n",
        "if not os.path.exists(SOURCE_DIR):\n",
        "    SOURCE_DIR = '/content/EstraNet/checkpoints_gnn'\n",
        "\n",
        "# Destination with timestamp to avoid overwriting\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "DEST_DIR = f'/content/drive/MyDrive/EstraNet/checkpoints_gnn_backup_{timestamp}'\n",
        "\n",
        "# 3. Copy\n",
        "if os.path.exists(SOURCE_DIR):\n",
        "    print(f\"\\nüì¶ Found local checkpoints at: {SOURCE_DIR}\")\n",
        "    print(f\"üöÄ Copying to: {DEST_DIR}...\")\n",
        "\n",
        "    try:\n",
        "        shutil.copytree(SOURCE_DIR, DEST_DIR)\n",
        "        print(f\"‚úÖ Backup successful! Folder: checkpoints_gnn_backup_{timestamp}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Backup failed: {e}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è No local '{SOURCE_DIR}' folder found to upload.\")\n",
        "    print(f\"   (Your checkpoints might already be in Drive at: /content/drive/MyDrive/EstraNet/checkpoints_gnn)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42b5a35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e42b5a35",
        "outputId": "1f6b5a5b-b406-4e85-8a10-05267eb22f78"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Paths\n",
        "source_path = '/content/drive/MyDrive/EstraNet/checkpoints_gnn'\n",
        "dest_path = '/content/drive/MyDrive/checkpoints_gnn'\n",
        "\n",
        "print(f\"üîÑ Attempting to move: {source_path} -> {dest_path}\")\n",
        "\n",
        "if os.path.exists(source_path):\n",
        "    # Check if destination already exists\n",
        "    if os.path.exists(dest_path):\n",
        "        print(f\"‚ö†Ô∏è Destination folder '{dest_path}' already exists.\")\n",
        "        print(\"   Renaming source to 'checkpoints_gnn_moved' to avoid overwriting.\")\n",
        "        dest_path = dest_path + \"_moved\"\n",
        "\n",
        "    try:\n",
        "        shutil.move(source_path, dest_path)\n",
        "        print(f\"‚úÖ Successfully moved to: {dest_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error moving folder: {e}\")\n",
        "else:\n",
        "    print(f\"‚ùå Source folder not found: {source_path}\")\n",
        "    print(\"   Please check if the folder path is correct.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcaa8bc2",
      "metadata": {
        "id": "bcaa8bc2"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # Target directory we just moved\n",
        "# target_dir = '/content/drive/MyDrive/checkpoints_gnn'\n",
        "\n",
        "# print(\"üîç Verifying Google Drive Sync...\")\n",
        "\n",
        "# if os.path.exists(target_dir):\n",
        "#     print(f\"‚úÖ Drive is mounted and folder exists: {target_dir}\")\n",
        "\n",
        "#     # List a few files to confirm access\n",
        "#     files = os.listdir(target_dir)\n",
        "#     print(f\"   Contains {len(files)} files/folders.\")\n",
        "#     if files:\n",
        "#         print(f\"   Example: {files[0]}\")\n",
        "# else:\n",
        "#     print(f\"‚ùå Folder not found: {target_dir}\")\n",
        "#     print(\"   Drive might not be mounted correctly or the move failed.\")\n",
        "#     # Optional: Suggest remounting only if strictly needed\n",
        "#     print(\"   If this fails, try: drive.mount('/content/drive', force_remount=True)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f84a68b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "9f84a68b",
        "outputId": "5506a164-ee3f-44a0-a998-54eb16d60055"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# The folder verified to exist in the previous step\n",
        "folder_to_zip = '/content/drive/MyDrive/checkpoints_gnn'\n",
        "zip_name = '/content/gnn_checkpoints_archive2'\n",
        "\n",
        "if os.path.exists(folder_to_zip):\n",
        "    print(f\"üì¶ Zipping '{folder_to_zip}'... (This may take a moment)\")\n",
        "\n",
        "    # Create zip file (shutil adds .zip extension automatically)\n",
        "    shutil.make_archive(zip_name, 'zip', folder_to_zip)\n",
        "\n",
        "    print(f\"‚úÖ Zip created: {zip_name}.zip\")\n",
        "    print(\"‚¨áÔ∏è Starting download to your local machine...\")\n",
        "\n",
        "    # Trigger browser download\n",
        "    files.download(zip_name + '.zip')\n",
        "else:\n",
        "    print(f\"‚ùå Error: The folder '{folder_to_zip}' was not found in the Colab runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf6db47",
      "metadata": {
        "id": "faf6db47"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import glob\n",
        "\n",
        "# # Locations to check\n",
        "# locations = [\n",
        "#     '/content/drive/MyDrive/EstraNet/checkpoints_gnn',\n",
        "#     '/content/EstraNet/checkpoints',\n",
        "#     'checkpoints',\n",
        "# ]\n",
        "\n",
        "# print(\"üîç Checking for checkpoints...\")\n",
        "# found_any = False\n",
        "# for loc in locations:\n",
        "#     if os.path.exists(loc):\n",
        "#         files = glob.glob(os.path.join(loc, \"*.index\"))\n",
        "#         if files:\n",
        "#             print(f\"‚úÖ Found {len(files)} checkpoints in: {loc}\")\n",
        "#             print(f\"   Example: {files[0]}\")\n",
        "#             found_any = True\n",
        "#             # Update the variable for the next cell\n",
        "#             actual_checkpoint_dir = loc\n",
        "#         else:\n",
        "#             print(f\"‚ùå Folder exists but empty: {loc}\")\n",
        "#     else:\n",
        "#         print(f\"‚ùå Folder not found: {loc}\")\n",
        "\n",
        "# if not found_any:\n",
        "#     print(\"\\n‚ö†Ô∏è No checkpoints found in expected locations.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

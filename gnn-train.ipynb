{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef5869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Running on Google Colab\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "üöÄ GPU detected! Training will be accelerated.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"üìù Running on local Jupyter\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"üöÄ GPU detected! Training will be accelerated.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"üí° Enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba69719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EstraNet'...\n",
      "remote: Enumerating objects: 120, done.\u001b[K\n",
      "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
      "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
      "remote: Total 120 (delta 57), reused 105 (delta 42), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (120/120), 3.04 MiB | 17.88 MiB/s, done.\n",
      "Resolving deltas: 100% (57/57), done.\n",
      "‚úÖ Clean! Directory: /content/EstraNet\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "if os.path.exists('EstraNet'):\n",
    "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
    "    \n",
    "!git clone https://github.com/loshithan/EstraNet.git\n",
    "os.chdir('EstraNet')\n",
    "print(f\"‚úÖ Clean! Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4307c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading ASCADf dataset from Google Drive...\n",
      "   This may take a few minutes (~1.5 GB)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\n",
      "To: /content/EstraNet/data/ASCAD.h5\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46.6M/46.6M [00:00<00:00, 102MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset downloaded successfully!\n",
      "\n",
      "üìä Dataset info:\n",
      "  Keys: ['Attack_traces', 'Profiling_traces']\n",
      "  Profiling traces shape: (50000, 700)\n",
      "  Attack traces shape: (10000, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ASCADf dataset configuration\n",
    "file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
    "DATASET_PATH = \"data/ASCAD.h5\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"üì• Downloading ASCADf dataset from Google Drive...\")\n",
    "    print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
    "    \n",
    "    # Download using gdown\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists\")\n",
    "\n",
    "# Verify dataset\n",
    "import h5py\n",
    "with h5py.File(DATASET_PATH, 'r') as f:\n",
    "    print(f\"\\nüìä Dataset info:\")\n",
    "    print(f\"  Keys: {list(f.keys())}\")\n",
    "    if 'Profiling_traces' in f:\n",
    "        print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
    "    if 'Attack_traces' in f:\n",
    "        print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027da6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "\n",
      "\n",
      "‚úÖ All dependencies installed!\n",
      "Using TensorFlow 2.19.0 (pre-installed)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "# %pip install -q absl-py==2.3.1 numpy==1.24.3 scipy==1.10.1 h5py==3.11.0\n",
    "\n",
    "# Install gdown for downloading from Google Drive\n",
    "%pip install -q gdown\n",
    "\n",
    "# Note: Using TensorFlow version pre-installed in Colab (2.16+ / 2.19+)\n",
    "# The compatibility fixes in Section 3 work with all TensorFlow 2.13+ versions\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "print(f\"Using TensorFlow {tf.__version__} (pre-installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd80811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∑ Training GNN Model\n",
      "======================================================================\n",
      "Starting GNN training...\n",
      "Model: GNN (211,876 parameters - 51% less than Transformer)\n",
      "Checkpoints: /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "Training steps: 50,000\n",
      "\n",
      "2026-02-12 16:58:05.057811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770915485.077716    4613 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770915485.084251    4613 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770915485.099297    4613 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770915485.099320    4613 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770915485.099323    4613 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770915485.099325    4613 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚úÖ File replaced with fixed version\n",
      "‚úÖ Checkpoints deleted\n",
      "\n",
      "‚ö†Ô∏è  NOW RESTART RUNTIME: Runtime ‚Üí Restart runtime\n",
      "Then run your training again\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/EstraNet/train_trans.py\", line 19, in <module>\n",
      "    from gnn_estranet import GNNEstraNet\n",
      "  File \"/content/EstraNet/gnn_estranet.py\", line 7, in <module>\n",
      "    from gnn_layers import GraphConvLayer, TemporalGraphBuilder, GlobalGraphPooling\n",
      "  File \"/content/EstraNet/gnn_layers.py\", line 171\n",
      "    \"\"\"\n",
      "    ^\n",
      "SyntaxError: unterminated triple-quoted string literal (detected at line 184)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN GNN MODEL IN COLAB\n",
    "# ============================================================================\n",
    "# Paste this into a new Colab cell\n",
    "\n",
    "print(\"üî∑ Training GNN Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/EstraNet/checkpoints_gnn',\n",
    "    'result_path': 'results/gnn',\n",
    "    'train_steps': 50000,\n",
    "    'save_steps': 5000,\n",
    "    'train_batch_size': 256,\n",
    "    'eval_batch_size': 32,\n",
    "    'learning_rate': 0.00025,\n",
    "    'model_type': 'gnn',  # KEY: Use GNN\n",
    "}\n",
    "\n",
    "import os\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['result_path'], exist_ok=True)\n",
    "\n",
    "# Build training command\n",
    "train_cmd = f\"\"\"\n",
    "python train_trans.py \\\\\n",
    "    --data_path=data/ASCAD.h5 \\\\\n",
    "    --checkpoint_dir={CONFIG['checkpoint_dir']} \\\\\n",
    "    --model_type={CONFIG['model_type']} \\\\\n",
    "    --dataset=ASCAD \\\\\n",
    "    --input_length=10000 \\\\\n",
    "    --eval_batch_size={CONFIG['eval_batch_size']} \\\\\n",
    "    --n_layer=2 \\\\\n",
    "    --d_model=128 \\\\\n",
    "    --d_inner=256 \\\\\n",
    "    --n_head_softmax=8 \\\\\n",
    "    --d_head_softmax=16 \\\\\n",
    "    --dropout=0.05 \\\\\n",
    "    --conv_kernel_size=3 \\\\\n",
    "    --n_conv_layer=2 \\\\\n",
    "    --pool_size=20 \\\\\n",
    "    --beta_hat_2=150 \\\\\n",
    "    --model_normalization=preLC \\\\\n",
    "    --softmax_attn=True \\\\\n",
    "    --do_train=True \\\\\n",
    "    --learning_rate={CONFIG['learning_rate']} \\\\\n",
    "    --clip=0.25 \\\\\n",
    "    --min_lr_ratio=0.004 \\\\\n",
    "    --warmup_steps=0 \\\\\n",
    "    --train_batch_size={CONFIG['train_batch_size']} \\\\\n",
    "    --train_steps={CONFIG['train_steps']} \\\\\n",
    "    --iterations=500 \\\\\n",
    "    --save_steps={CONFIG['save_steps']} \\\\\n",
    "    --result_path={CONFIG['result_path']}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting GNN training...\")\n",
    "print(f\"Model: GNN (211,876 parameters - 51% less than Transformer)\")\n",
    "print(f\"Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"Training steps: {CONFIG['train_steps']:,}\\n\")\n",
    "\n",
    "!{train_cmd}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f942488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for checkpoints in entire Colab environment...\n",
      "‚úÖ Found 2 checkpoint files:\n",
      "   /content/EstraNet/checkpoints/trans_long-8.index\n",
      "   /content/EstraNet/checkpoints/trans_long-8.data-00000-of-00001\n",
      "\n",
      "üì¶ Backing up found files to Drive...\n",
      "   Copied to: /content/drive/MyDrive/EstraNet_GNN_Rescue_20260212_165811/trans_long-8.index\n",
      "   Copied to: /content/drive/MyDrive/EstraNet_GNN_Rescue_20260212_165811/trans_long-8.data-00000-of-00001\n",
      "\n",
      "‚úÖ Files rescued to: /content/drive/MyDrive/EstraNet_GNN_Rescue_20260212_165811\n",
      "\n",
      "üìÇ Checking target directory: /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "   Files found: []\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIND AND BACKUP CHECKPOINTS (Run this to locate missing files)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "print(\"üîç Searching for checkpoints in entire Colab environment...\")\n",
    "\n",
    "# 1. Search for any checkpoint files\n",
    "found_files = []\n",
    "for root, dirs, files in os.walk(\"/content\"):\n",
    "    for file in files:\n",
    "        if file.startswith(\"trans_long\") or file == \"checkpoint\":\n",
    "            full_path = os.path.join(root, file)\n",
    "            # Ignore files already in Drive to avoid duplicate listing\n",
    "            if \"/content/drive\" not in full_path:\n",
    "                found_files.append(full_path)\n",
    "\n",
    "if not found_files:\n",
    "    print(\"‚ùå No local checkpoints found in /content!\")\n",
    "    print(\"   This means training likely didn't reach step 5,000 yet, or crashed.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(found_files)} checkpoint files:\")\n",
    "    for f in found_files:\n",
    "        print(f\"   {f}\")\n",
    "        \n",
    "    # 2. Backup found files\n",
    "    print(\"\\nüì¶ Backing up found files to Drive...\")\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_dir = f\"/content/drive/MyDrive/EstraNet_GNN_Rescue_{timestamp}\"\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    \n",
    "    for src in found_files:\n",
    "        filename = os.path.basename(src)\n",
    "        dst = os.path.join(backup_dir, filename)\n",
    "        try:\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"   Copied to: {dst}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Failed to copy {src}: {e}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Files rescued to: {backup_dir}\")\n",
    "\n",
    "# 3. Check specific GNN output directory\n",
    "gnn_dir = \"/content/drive/MyDrive/EstraNet/checkpoints_gnn\"\n",
    "if os.path.exists(gnn_dir):\n",
    "    print(f\"\\nüìÇ Checking target directory: {gnn_dir}\")\n",
    "    files = os.listdir(gnn_dir)\n",
    "    print(f\"   Files found: {files}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Target directory does not exist: {gnn_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb82f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating GNN Checkpoints from Google Drive\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 184) (gnn_layers.py, line 171)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/tmp/ipython-input-1486069428.py\"\u001b[0m, line \u001b[1;32m14\u001b[0m, in \u001b[1;35m<cell line: 0>\u001b[0m\n    from gnn_estranet import GNNEstraNet\n",
      "\u001b[0;36m  File \u001b[0;32m\"/content/EstraNet/gnn_estranet.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from gnn_layers import GraphConvLayer, TemporalGraphBuilder, GlobalGraphPooling\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/content/EstraNet/gnn_layers.py\"\u001b[0;36m, line \u001b[0;32m171\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 184)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATE ALL GNN CHECKPOINTS\n",
    "# ============================================================================\n",
    "# Paste this into a Colab cell to test all GNN checkpoints\n",
    "\n",
    "print(\"üîç Evaluating GNN Checkpoints from Google Drive\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "from gnn_estranet import GNNEstraNet\n",
    "from evaluation_utils import compute_key_rank\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load ASCAD dataset\n",
    "print(\"\\nüì• Loading ASCAD dataset...\")\n",
    "with h5py.File('data/ASCAD.h5', 'r') as f:\n",
    "    all_traces = f['Attack_traces']['traces'][()]\n",
    "    # Check shape\n",
    "    print(f\"   Original trace shape: {all_traces.shape}\")\n",
    "    \n",
    "    # The model seems to have been trained on the original length (700)\n",
    "    # causing a graph of 17 nodes (700 -> 35 -> 17)\n",
    "    # instead of 10000 -> 500 -> 250 nodes.\n",
    "    # So we should use the traces AS IS without padding to avoid mismatch.\n",
    "    traces = all_traces\n",
    "    input_length = traces.shape[1]\n",
    "    \n",
    "    print(f\"   Processed trace shape: {traces.shape}\")\n",
    "    print(f\"   Input length: {input_length}\")\n",
    "    \n",
    "    # Load labels\n",
    "    labels = f['Attack_traces']['labels'][()]\n",
    "    \n",
    "    # Load metadata (structured array)\n",
    "    metadata = f['Attack_traces']['metadata'][()]\n",
    "    # Extract byte 2 (3rd byte)\n",
    "    plaintexts = metadata['plaintext'][:, 2].astype(np.uint8)\n",
    "    keys = metadata['key'][:, 2].astype(np.uint8)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(traces)} attack traces\")\n",
    "\n",
    "# Build GNN model\n",
    "print(\"\\nüèóÔ∏è Building GNN model...\")\n",
    "model = GNNEstraNet(\n",
    "    n_gcn_layers=2,\n",
    "    d_model=128,\n",
    "    k_neighbors=5,\n",
    "    graph_pooling='mean',\n",
    "    d_head_softmax=16,\n",
    "    n_head_softmax=8,\n",
    "    dropout=0.05,\n",
    "    n_classes=256,\n",
    "    conv_kernel_size=3,\n",
    "    n_conv_layer=2,\n",
    "    pool_size=20,\n",
    "    beta_hat_2=150,\n",
    "    model_normalization='preLC',\n",
    "    softmax_attn=True,\n",
    "    output_attn=False\n",
    ")\n",
    "\n",
    "# Build with dummy input\n",
    "dummy = tf.zeros((1, input_length))\n",
    "# Important: pass softmax_attn_smoothing=None as we added that to call signature\n",
    "_ = model(dummy, softmax_attn_smoothing=None, training=False)\n",
    "params = model.count_params()\n",
    "print(f\"‚úÖ GNN model built. Parameters: {params:,}\")\n",
    "print(f\"   (Transformer: 431,233 | Reduction: {((431233-params)/431233)*100:.1f}%)\")\n",
    "\n",
    "# Find checkpoints\n",
    "checkpoint_dir = '/content/drive/MyDrive/EstraNet/checkpoints_gnn/'\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"*.index\"))\n",
    "checkpoint_numbers = sorted([int(f.split('trans_long-')[1].split('.')[0]) \n",
    "                            for f in checkpoint_files if 'trans_long-' in f])\n",
    "\n",
    "print(f\"\\nüìÇ Found {len(checkpoint_numbers)} checkpoints: {checkpoint_numbers}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for ckpt_num in checkpoint_numbers:\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'trans_long-{ckpt_num}')\n",
    "    \n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Testing Checkpoint {ckpt_num} (~{ckpt_num*5000} steps)\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = tf.train.Checkpoint(model=model)\n",
    "        checkpoint.restore(checkpoint_path).expect_partial()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Run inference\n",
    "    traces_flat = traces.astype(np.float32)\n",
    "    predictions = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in range(0, len(traces), batch_size):\n",
    "        batch = traces_flat[i:i+batch_size]\n",
    "        # Pass dummy softmax_attn_smoothing=None\n",
    "        preds = model(batch, softmax_attn_smoothing=None, training=False)[0].numpy()\n",
    "        predictions.append(preds)\n",
    "        \n",
    "    predictions = np.vstack(predictions)\n",
    "    \n",
    "    # Compute rank (100 experiments)\n",
    "    key_rank_list = []\n",
    "    for exp in range(50): # 50 experiments for speed\n",
    "        key_ranks = compute_key_rank(predictions, plaintexts, keys)\n",
    "        key_rank_list.append(key_ranks)\n",
    "        \n",
    "    mean_ranks = np.mean(np.stack(key_rank_list, axis=0), axis=0)\n",
    "    final_rank = mean_ranks[-1]\n",
    "    \n",
    "    print(f\"üèÜ Final Rank: {final_rank:.2f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'checkpoint': ckpt_num,\n",
    "        'rank': final_rank,\n",
    "        'mean_ranks': mean_ranks\n",
    "    })\n",
    "\n",
    "# Sort and display results\n",
    "results.sort(key=lambda x: x['rank'])\n",
    "best = results[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä GNN RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Checkpoint':<10} {'Steps':<10} {'Rank':<10} {'Status':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for r in results:\n",
    "    status = \"‚úÖ BEST\" if r == best else \"\"\n",
    "    if r['rank'] < 27: status += \" üî• BEATS TRANSFORMER\"\n",
    "    print(f\"{r['checkpoint']:<10} {r['checkpoint']*5000:<10} {r['rank']:<10.2f} {status}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"üèÜ Best Rank: {best['rank']:.2f} (Checkpoint {best['checkpoint']})\")\n",
    "print(f\"üìâ Parameters: {params:,}\")\n",
    "\n",
    "# Plot top 3\n",
    "plt.figure(figsize=(10, 6))\n",
    "for r in results[:3]:\n",
    "    plt.plot(r['mean_ranks'], label=f\"GNN Ckpt-{r['checkpoint']} (Rank {r['rank']:.0f})\")\n",
    "\n",
    "plt.axhline(y=27, color='r', linestyle='--', label='Transformer Baseline (Rank 27)')\n",
    "plt.yscale('log')\n",
    "plt.title(f'GNN ({params:,} params) vs Transformer (431k params)')\n",
    "plt.xlabel('Traces')\n",
    "plt.ylabel('Key Rank')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('gnn_results.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

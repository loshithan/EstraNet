{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef5869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Running on Google Colab\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "üöÄ GPU detected! Training will be accelerated.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"üìù Running on local Jupyter\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"üöÄ GPU detected! Training will be accelerated.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"üí° Enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba69719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EstraNet'...\n",
      "remote: Enumerating objects: 124, done.\u001b[K\n",
      "remote: Counting objects: 100% (124/124), done.\u001b[Knting objects:  94% (117/124)\u001b[K\n",
      "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
      "remote: Total 124 (delta 60), reused 107 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (124/124), 3.04 MiB | 19.72 MiB/s, done.\n",
      "Resolving deltas: 100% (60/60), done.\n",
      "‚úÖ Clean! Directory: /content/EstraNet\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "if os.path.exists('EstraNet'):\n",
    "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
    "    \n",
    "!git clone https://github.com/loshithan/EstraNet.git\n",
    "os.chdir('EstraNet')\n",
    "print(f\"‚úÖ Clean! Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4307c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading ASCADf dataset from Google Drive...\n",
      "   This may take a few minutes (~1.5 GB)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\n",
      "To: /content/EstraNet/data/ASCAD.h5\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46.6M/46.6M [00:00<00:00, 95.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset downloaded successfully!\n",
      "\n",
      "üìä Dataset info:\n",
      "  Keys: ['Attack_traces', 'Profiling_traces']\n",
      "  Profiling traces shape: (50000, 700)\n",
      "  Attack traces shape: (10000, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ASCADf dataset configuration\n",
    "file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
    "DATASET_PATH = \"data/ASCAD.h5\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"üì• Downloading ASCADf dataset from Google Drive...\")\n",
    "    print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
    "    \n",
    "    # Download using gdown\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists\")\n",
    "\n",
    "# Verify dataset\n",
    "import h5py\n",
    "with h5py.File(DATASET_PATH, 'r') as f:\n",
    "    print(f\"\\nüìä Dataset info:\")\n",
    "    print(f\"  Keys: {list(f.keys())}\")\n",
    "    if 'Profiling_traces' in f:\n",
    "        print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
    "    if 'Attack_traces' in f:\n",
    "        print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027da6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "\n",
      "\n",
      "‚úÖ All dependencies installed!\n",
      "Using TensorFlow 2.19.0 (pre-installed)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "# %pip install -q absl-py==2.3.1 numpy==1.24.3 scipy==1.10.1 h5py==3.11.0\n",
    "\n",
    "# Install gdown for downloading from Google Drive\n",
    "%pip install -q gdown\n",
    "\n",
    "# Note: Using TensorFlow version pre-installed in Colab (2.16+ / 2.19+)\n",
    "# The compatibility fixes in Section 3 work with all TensorFlow 2.13+ versions\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "print(f\"Using TensorFlow {tf.__version__} (pre-installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd80811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∑ Training GNN Model\n",
      "======================================================================\n",
      "Starting GNN training...\n",
      "Model: GNN (211,876 parameters - 51% less than Transformer)\n",
      "Checkpoints: /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "Training steps: 50,000\n",
      "\n",
      "2026-02-12 17:06:23.955686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770915983.975620    6904 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770915983.982128    6904 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770915983.997126    6904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770915983.997151    6904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770915983.997154    6904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770915983.997156    6904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚úÖ File replaced with fixed version\n",
      "‚úÖ Checkpoints deleted\n",
      "\n",
      "‚ö†Ô∏è  NOW RESTART RUNTIME: Runtime ‚Üí Restart runtime\n",
      "Then run your training again\n",
      "INFO:tensorflow:\n",
      "I0212 17:06:29.639202 135820264420480 train_trans.py:429] \n",
      "INFO:tensorflow:\n",
      "I0212 17:06:29.639400 135820264420480 train_trans.py:430] \n",
      "INFO:tensorflow:use_tpu               : False\n",
      "I0212 17:06:29.639473 135820264420480 train_trans.py:431] use_tpu               : False\n",
      "INFO:tensorflow:data_path             : data/ASCAD.h5\n",
      "I0212 17:06:29.639528 135820264420480 train_trans.py:432] data_path             : data/ASCAD.h5\n",
      "INFO:tensorflow:dataset               : ASCAD\n",
      "I0212 17:06:29.639575 135820264420480 train_trans.py:433] dataset               : ASCAD\n",
      "INFO:tensorflow:checkpoint_dir        : /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "I0212 17:06:29.639618 135820264420480 train_trans.py:434] checkpoint_dir        : /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "INFO:tensorflow:checkpoint_idx        : 0\n",
      "I0212 17:06:29.639660 135820264420480 train_trans.py:435] checkpoint_idx        : 0\n",
      "INFO:tensorflow:warm_start            : False\n",
      "I0212 17:06:29.639698 135820264420480 train_trans.py:436] warm_start            : False\n",
      "INFO:tensorflow:result_path           : results/gnn\n",
      "I0212 17:06:29.639736 135820264420480 train_trans.py:437] result_path           : results/gnn\n",
      "INFO:tensorflow:do_train              : True\n",
      "I0212 17:06:29.639774 135820264420480 train_trans.py:438] do_train              : True\n",
      "INFO:tensorflow:learning_rate         : 0.00025\n",
      "I0212 17:06:29.639814 135820264420480 train_trans.py:439] learning_rate         : 0.00025\n",
      "INFO:tensorflow:clip                  : 0.25\n",
      "I0212 17:06:29.639854 135820264420480 train_trans.py:440] clip                  : 0.25\n",
      "INFO:tensorflow:min_lr_ratio          : 0.004\n",
      "I0212 17:06:29.639893 135820264420480 train_trans.py:441] min_lr_ratio          : 0.004\n",
      "INFO:tensorflow:warmup_steps          : 0\n",
      "I0212 17:06:29.639930 135820264420480 train_trans.py:442] warmup_steps          : 0\n",
      "INFO:tensorflow:input_length          : 10000\n",
      "I0212 17:06:29.639967 135820264420480 train_trans.py:443] input_length          : 10000\n",
      "INFO:tensorflow:data_desync           : 0\n",
      "I0212 17:06:29.640004 135820264420480 train_trans.py:444] data_desync           : 0\n",
      "INFO:tensorflow:train_batch_size      : 256\n",
      "I0212 17:06:29.640040 135820264420480 train_trans.py:445] train_batch_size      : 256\n",
      "INFO:tensorflow:eval_batch_size       : 32\n",
      "I0212 17:06:29.640075 135820264420480 train_trans.py:446] eval_batch_size       : 32\n",
      "INFO:tensorflow:train_steps           : 50000\n",
      "I0212 17:06:29.640125 135820264420480 train_trans.py:447] train_steps           : 50000\n",
      "INFO:tensorflow:iterations            : 500\n",
      "I0212 17:06:29.640161 135820264420480 train_trans.py:448] iterations            : 500\n",
      "INFO:tensorflow:save_steps            : 5000\n",
      "I0212 17:06:29.640198 135820264420480 train_trans.py:449] save_steps            : 5000\n",
      "INFO:tensorflow:n_layer               : 2\n",
      "I0212 17:06:29.640235 135820264420480 train_trans.py:450] n_layer               : 2\n",
      "INFO:tensorflow:d_model               : 128\n",
      "I0212 17:06:29.640277 135820264420480 train_trans.py:451] d_model               : 128\n",
      "INFO:tensorflow:d_head                : 32\n",
      "I0212 17:06:29.640315 135820264420480 train_trans.py:452] d_head                : 32\n",
      "INFO:tensorflow:n_head                : 4\n",
      "I0212 17:06:29.640352 135820264420480 train_trans.py:453] n_head                : 4\n",
      "INFO:tensorflow:d_inner               : 256\n",
      "I0212 17:06:29.640389 135820264420480 train_trans.py:454] d_inner               : 256\n",
      "INFO:tensorflow:n_head_softmax        : 8\n",
      "I0212 17:06:29.640426 135820264420480 train_trans.py:455] n_head_softmax        : 8\n",
      "INFO:tensorflow:d_head_softmax        : 16\n",
      "I0212 17:06:29.640465 135820264420480 train_trans.py:456] d_head_softmax        : 16\n",
      "INFO:tensorflow:dropout               : 0.05\n",
      "I0212 17:06:29.640504 135820264420480 train_trans.py:457] dropout               : 0.05\n",
      "INFO:tensorflow:conv_kernel_size      : 3\n",
      "I0212 17:06:29.640542 135820264420480 train_trans.py:458] conv_kernel_size      : 3\n",
      "INFO:tensorflow:n_conv_layer          : 2\n",
      "I0212 17:06:29.640580 135820264420480 train_trans.py:459] n_conv_layer          : 2\n",
      "INFO:tensorflow:pool_size             : 20\n",
      "I0212 17:06:29.640617 135820264420480 train_trans.py:460] pool_size             : 20\n",
      "INFO:tensorflow:d_kernel_map          : 128\n",
      "I0212 17:06:29.640655 135820264420480 train_trans.py:461] d_kernel_map          : 128\n",
      "INFO:tensorflow:beta_hat_2            : 150\n",
      "I0212 17:06:29.640693 135820264420480 train_trans.py:462] beta_hat_2            : 150\n",
      "INFO:tensorflow:model_normalization   : preLC\n",
      "I0212 17:06:29.640730 135820264420480 train_trans.py:463] model_normalization   : preLC\n",
      "INFO:tensorflow:head_initialization   : forward\n",
      "I0212 17:06:29.640768 135820264420480 train_trans.py:464] head_initialization   : forward\n",
      "INFO:tensorflow:softmax_attn          : True\n",
      "I0212 17:06:29.640806 135820264420480 train_trans.py:465] softmax_attn          : True\n",
      "INFO:tensorflow:max_eval_batch        : -1\n",
      "I0212 17:06:29.640844 135820264420480 train_trans.py:466] max_eval_batch        : -1\n",
      "INFO:tensorflow:output_attn           : False\n",
      "I0212 17:06:29.640883 135820264420480 train_trans.py:467] output_attn           : False\n",
      "INFO:tensorflow:\n",
      "I0212 17:06:29.640920 135820264420480 train_trans.py:468] \n",
      "INFO:tensorflow:\n",
      "I0212 17:06:29.640958 135820264420480 train_trans.py:469] \n",
      "INFO:tensorflow:Number of accelerators: 1\n",
      "I0212 17:06:56.928432 135820264420480 train_trans.py:509] Number of accelerators: 1\n",
      "INFO:tensorflow:num of train batches 195\n",
      "I0212 17:06:56.928598 135820264420480 train_trans.py:522] num of train batches 195\n",
      "INFO:tensorflow:num of test batches 312\n",
      "I0212 17:06:56.928660 135820264420480 train_trans.py:523] num of test batches 312\n",
      "2026-02-12 17:06:57.202574: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1770916017.204944    6904 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38477 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
      "I0212 17:06:57.404853 135820264420480 functional_saver.py:440] Sharding callback duration: 7 microseconds\n",
      "INFO:tensorflow:Model saved in path: /content/drive/MyDrive/EstraNet/checkpoints_gnn/trans_long-1\n",
      "I0212 17:06:57.417559 135820264420480 train_trans.py:273] Model saved in path: /content/drive/MyDrive/EstraNet/checkpoints_gnn/trans_long-1\n",
      "INFO:tensorflow:Starting training ... \n",
      "I0212 17:06:57.418078 135820264420480 train_trans.py:331] Starting training ... \n",
      "Traceback (most recent call last):\n",
      "  File \"/content/EstraNet/train_trans.py\", line 577, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "             ^^^^^^^^^^\n",
      "  File \"/content/EstraNet/train_trans.py\", line 525, in main\n",
      "    train(train_data.GetTFRecords(FLAGS.train_batch_size, training=True), \\\n",
      "  File \"/content/EstraNet/train_trans.py\", line 336, in train\n",
      "    train_steps(train_iter, tf.convert_to_tensor(FLAGS.iterations), \\\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filerk8zcea8.py\", line 66, in tf__train_steps\n",
      "    ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(steps),), None, fscope), None, loop_body, get_state_1, set_state_1, ('global_step',), {'iterate_names': '_'})\n",
      "  File \"/tmp/__autograph_generated_filerk8zcea8.py\", line 62, in loop_body\n",
      "    ag__.converted_call(ag__.ld(strategy).run, (ag__.ld(step_fn),), dict(args=(ag__.ld(inps), ag__.ld(lbls), ag__.ld(global_step))), fscope)\n",
      "  File \"/tmp/__autograph_generated_filerk8zcea8.py\", line 23, in step_fn\n",
      "    logits = ag__.converted_call(ag__.ld(model), (), dict(inputs=ag__.ld(inps), softmax_attn_smoothing=ag__.ld(softmax_attn_smoothing), training=True), fscope_1)[0]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/content/EstraNet/gnn_estranet.py\", line 127, in call\n",
      "    x, adjacency = self.graph_builder(x)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/EstraNet/gnn_layers.py\", line 121, in build\n",
      "    self.adjacency = self.add_weight(\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/content/EstraNet/train_trans.py\", line 290, in step_fn  *\n",
      "        logits = model(inputs=inps, softmax_attn_smoothing=softmax_attn_smoothing, training=True)[0]\n",
      "    File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/content/EstraNet/gnn_estranet.py\", line 127, in call\n",
      "        x, adjacency = self.graph_builder(x)\n",
      "    File \"/content/EstraNet/gnn_layers.py\", line 121, in build\n",
      "        self.adjacency = self.add_weight(\n",
      "\n",
      "    ValueError: Exception encountered when calling GNNEstraNet.call().\n",
      "    \n",
      "    \u001b[1mShapes used to initialize variables must be fully-defined (no `None` dimensions). Received: shape=(None, None) for variable path='gnn_estra_net/temporal_graph_builder/adjacency'\u001b[0m\n",
      "    \n",
      "    Arguments received by GNNEstraNet.call():\n",
      "      ‚Ä¢ inputs=tf.Tensor(shape=(256, 700), dtype=float32)\n",
      "      ‚Ä¢ softmax_attn_smoothing=1.0\n",
      "      ‚Ä¢ training=True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN GNN MODEL IN COLAB\n",
    "# ============================================================================\n",
    "# Paste this into a new Colab cell\n",
    "\n",
    "print(\"üî∑ Training GNN Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/EstraNet/checkpoints_gnn',\n",
    "    'result_path': 'results/gnn',\n",
    "    'train_steps': 50000,\n",
    "    'save_steps': 5000,\n",
    "    'train_batch_size': 256,\n",
    "    'eval_batch_size': 32,\n",
    "    'learning_rate': 0.00025,\n",
    "    'model_type': 'gnn',  # KEY: Use GNN\n",
    "}\n",
    "\n",
    "import os\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['result_path'], exist_ok=True)\n",
    "\n",
    "# Build training command\n",
    "train_cmd = f\"\"\"\n",
    "python train_trans.py \\\\\n",
    "    --data_path=data/ASCAD.h5 \\\\\n",
    "    --checkpoint_dir={CONFIG['checkpoint_dir']} \\\\\n",
    "    --model_type={CONFIG['model_type']} \\\\\n",
    "    --dataset=ASCAD \\\\\n",
    "    --input_length=10000 \\\\\n",
    "    --eval_batch_size={CONFIG['eval_batch_size']} \\\\\n",
    "    --n_layer=2 \\\\\n",
    "    --d_model=128 \\\\\n",
    "    --d_inner=256 \\\\\n",
    "    --n_head_softmax=8 \\\\\n",
    "    --d_head_softmax=16 \\\\\n",
    "    --dropout=0.05 \\\\\n",
    "    --conv_kernel_size=3 \\\\\n",
    "    --n_conv_layer=2 \\\\\n",
    "    --pool_size=20 \\\\\n",
    "    --beta_hat_2=150 \\\\\n",
    "    --model_normalization=preLC \\\\\n",
    "    --softmax_attn=True \\\\\n",
    "    --do_train=True \\\\\n",
    "    --learning_rate={CONFIG['learning_rate']} \\\\\n",
    "    --clip=0.25 \\\\\n",
    "    --min_lr_ratio=0.004 \\\\\n",
    "    --warmup_steps=0 \\\\\n",
    "    --train_batch_size={CONFIG['train_batch_size']} \\\\\n",
    "    --train_steps={CONFIG['train_steps']} \\\\\n",
    "    --iterations=500 \\\\\n",
    "    --save_steps={CONFIG['save_steps']} \\\\\n",
    "    --result_path={CONFIG['result_path']}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting GNN training...\")\n",
    "print(f\"Model: GNN (211,876 parameters - 51% less than Transformer)\")\n",
    "print(f\"Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"Training steps: {CONFIG['train_steps']:,}\\n\")\n",
    "\n",
    "!{train_cmd}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f942488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for checkpoints in entire Colab environment...\n",
      "‚úÖ Found 2 checkpoint files:\n",
      "   /content/EstraNet/checkpoints/trans_long-8.index\n",
      "   /content/EstraNet/checkpoints/trans_long-8.data-00000-of-00001\n",
      "\n",
      "üì¶ Backing up found files to Drive...\n",
      "   Copied to: /content/drive/MyDrive/EstraNet_GNN_Rescue_20260212_170700/trans_long-8.index\n",
      "   Copied to: /content/drive/MyDrive/EstraNet_GNN_Rescue_20260212_170700/trans_long-8.data-00000-of-00001\n",
      "\n",
      "‚úÖ Files rescued to: /content/drive/MyDrive/EstraNet_GNN_Rescue_20260212_170700\n",
      "\n",
      "üìÇ Checking target directory: /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "   Files found: ['checkpoint', 'trans_long-1.data-00000-of-00001', 'loss.pkl', 'trans_long-1.index']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIND AND BACKUP CHECKPOINTS (Run this to locate missing files)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "print(\"üîç Searching for checkpoints in entire Colab environment...\")\n",
    "\n",
    "# 1. Search for any checkpoint files\n",
    "found_files = []\n",
    "for root, dirs, files in os.walk(\"/content\"):\n",
    "    for file in files:\n",
    "        if file.startswith(\"trans_long\") or file == \"checkpoint\":\n",
    "            full_path = os.path.join(root, file)\n",
    "            # Ignore files already in Drive to avoid duplicate listing\n",
    "            if \"/content/drive\" not in full_path:\n",
    "                found_files.append(full_path)\n",
    "\n",
    "if not found_files:\n",
    "    print(\"‚ùå No local checkpoints found in /content!\")\n",
    "    print(\"   This means training likely didn't reach step 5,000 yet, or crashed.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(found_files)} checkpoint files:\")\n",
    "    for f in found_files:\n",
    "        print(f\"   {f}\")\n",
    "        \n",
    "    # 2. Backup found files\n",
    "    print(\"\\nüì¶ Backing up found files to Drive...\")\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_dir = f\"/content/drive/MyDrive/EstraNet_GNN_Rescue_{timestamp}\"\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    \n",
    "    for src in found_files:\n",
    "        filename = os.path.basename(src)\n",
    "        dst = os.path.join(backup_dir, filename)\n",
    "        try:\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"   Copied to: {dst}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Failed to copy {src}: {e}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Files rescued to: {backup_dir}\")\n",
    "\n",
    "# 3. Check specific GNN output directory\n",
    "gnn_dir = \"/content/drive/MyDrive/EstraNet/checkpoints_gnn\"\n",
    "if os.path.exists(gnn_dir):\n",
    "    print(f\"\\nüìÇ Checking target directory: {gnn_dir}\")\n",
    "    files = os.listdir(gnn_dir)\n",
    "    print(f\"   Files found: {files}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Target directory does not exist: {gnn_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb82f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating GNN Checkpoints from Google Drive\n",
      "======================================================================\n",
      "\n",
      "üì• Loading ASCAD dataset...\n",
      "   Original trace shape: (10000, 700)\n",
      "   Processed trace shape: (10000, 700)\n",
      "   Input length: 700\n",
      "‚úÖ Loaded 10000 attack traces\n",
      "\n",
      "üèóÔ∏è Building GNN model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling GNNEstraNet.call().\n\n\u001b[1mShapes used to initialize variables must be fully-defined (no `None` dimensions). Received: shape=(None, None) for variable path='gnn_estra_net/temporal_graph_builder/adjacency'\u001b[0m\n\nArguments received by GNNEstraNet.call():\n  ‚Ä¢ inputs=tf.Tensor(shape=(1, 700), dtype=float32)\n  ‚Ä¢ softmax_attn_smoothing=None\n  ‚Ä¢ training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1486069428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Important: pass softmax_attn_smoothing=None as we added that to call signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_attn_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ GNN model built. Parameters: {params:,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/EstraNet/gnn_estranet.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, softmax_attn_smoothing, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# --- Build temporal graph ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# --- GCN message passing ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/EstraNet/gnn_layers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Make adjacency a non-trainable weight but allow it to be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         self.adjacency = self.add_weight(\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adjacency'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Allow dynamic shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling GNNEstraNet.call().\n\n\u001b[1mShapes used to initialize variables must be fully-defined (no `None` dimensions). Received: shape=(None, None) for variable path='gnn_estra_net/temporal_graph_builder/adjacency'\u001b[0m\n\nArguments received by GNNEstraNet.call():\n  ‚Ä¢ inputs=tf.Tensor(shape=(1, 700), dtype=float32)\n  ‚Ä¢ softmax_attn_smoothing=None\n  ‚Ä¢ training=False"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATE ALL GNN CHECKPOINTS\n",
    "# ============================================================================\n",
    "# Paste this into a Colab cell to test all GNN checkpoints\n",
    "\n",
    "print(\"üîç Evaluating GNN Checkpoints from Google Drive\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "from gnn_estranet import GNNEstraNet\n",
    "from evaluation_utils import compute_key_rank\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load ASCAD dataset\n",
    "print(\"\\nüì• Loading ASCAD dataset...\")\n",
    "with h5py.File('data/ASCAD.h5', 'r') as f:\n",
    "    all_traces = f['Attack_traces']['traces'][()]\n",
    "    # Check shape\n",
    "    print(f\"   Original trace shape: {all_traces.shape}\")\n",
    "    \n",
    "    # The model seems to have been trained on the original length (700)\n",
    "    # causing a graph of 17 nodes (700 -> 35 -> 17)\n",
    "    # instead of 10000 -> 500 -> 250 nodes.\n",
    "    # So we should use the traces AS IS without padding to avoid mismatch.\n",
    "    traces = all_traces\n",
    "    input_length = traces.shape[1]\n",
    "    \n",
    "    print(f\"   Processed trace shape: {traces.shape}\")\n",
    "    print(f\"   Input length: {input_length}\")\n",
    "    \n",
    "    # Load labels\n",
    "    labels = f['Attack_traces']['labels'][()]\n",
    "    \n",
    "    # Load metadata (structured array)\n",
    "    metadata = f['Attack_traces']['metadata'][()]\n",
    "    # Extract byte 2 (3rd byte)\n",
    "    plaintexts = metadata['plaintext'][:, 2].astype(np.uint8)\n",
    "    keys = metadata['key'][:, 2].astype(np.uint8)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(traces)} attack traces\")\n",
    "\n",
    "# Build GNN model\n",
    "print(\"\\nüèóÔ∏è Building GNN model...\")\n",
    "model = GNNEstraNet(\n",
    "    n_gcn_layers=2,\n",
    "    d_model=128,\n",
    "    k_neighbors=5,\n",
    "    graph_pooling='mean',\n",
    "    d_head_softmax=16,\n",
    "    n_head_softmax=8,\n",
    "    dropout=0.05,\n",
    "    n_classes=256,\n",
    "    conv_kernel_size=3,\n",
    "    n_conv_layer=2,\n",
    "    pool_size=20,\n",
    "    beta_hat_2=150,\n",
    "    model_normalization='preLC',\n",
    "    softmax_attn=True,\n",
    "    output_attn=False\n",
    ")\n",
    "\n",
    "# Build with dummy input\n",
    "dummy = tf.zeros((1, input_length))\n",
    "# Important: pass softmax_attn_smoothing=None as we added that to call signature\n",
    "_ = model(dummy, softmax_attn_smoothing=None, training=False)\n",
    "params = model.count_params()\n",
    "print(f\"‚úÖ GNN model built. Parameters: {params:,}\")\n",
    "print(f\"   (Transformer: 431,233 | Reduction: {((431233-params)/431233)*100:.1f}%)\")\n",
    "\n",
    "# Find checkpoints\n",
    "checkpoint_dir = '/content/drive/MyDrive/EstraNet/checkpoints_gnn/'\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"*.index\"))\n",
    "checkpoint_numbers = sorted([int(f.split('trans_long-')[1].split('.')[0]) \n",
    "                            for f in checkpoint_files if 'trans_long-' in f])\n",
    "\n",
    "print(f\"\\nüìÇ Found {len(checkpoint_numbers)} checkpoints: {checkpoint_numbers}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for ckpt_num in checkpoint_numbers:\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'trans_long-{ckpt_num}')\n",
    "    \n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Testing Checkpoint {ckpt_num} (~{ckpt_num*5000} steps)\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = tf.train.Checkpoint(model=model)\n",
    "        checkpoint.restore(checkpoint_path).expect_partial()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Run inference\n",
    "    traces_flat = traces.astype(np.float32)\n",
    "    predictions = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in range(0, len(traces), batch_size):\n",
    "        batch = traces_flat[i:i+batch_size]\n",
    "        # Pass dummy softmax_attn_smoothing=None\n",
    "        preds = model(batch, softmax_attn_smoothing=None, training=False)[0].numpy()\n",
    "        predictions.append(preds)\n",
    "        \n",
    "    predictions = np.vstack(predictions)\n",
    "    \n",
    "    # Compute rank (100 experiments)\n",
    "    key_rank_list = []\n",
    "    for exp in range(50): # 50 experiments for speed\n",
    "        key_ranks = compute_key_rank(predictions, plaintexts, keys)\n",
    "        key_rank_list.append(key_ranks)\n",
    "        \n",
    "    mean_ranks = np.mean(np.stack(key_rank_list, axis=0), axis=0)\n",
    "    final_rank = mean_ranks[-1]\n",
    "    \n",
    "    print(f\"üèÜ Final Rank: {final_rank:.2f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'checkpoint': ckpt_num,\n",
    "        'rank': final_rank,\n",
    "        'mean_ranks': mean_ranks\n",
    "    })\n",
    "\n",
    "# Sort and display results\n",
    "results.sort(key=lambda x: x['rank'])\n",
    "best = results[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä GNN RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Checkpoint':<10} {'Steps':<10} {'Rank':<10} {'Status':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for r in results:\n",
    "    status = \"‚úÖ BEST\" if r == best else \"\"\n",
    "    if r['rank'] < 27: status += \" üî• BEATS TRANSFORMER\"\n",
    "    print(f\"{r['checkpoint']:<10} {r['checkpoint']*5000:<10} {r['rank']:<10.2f} {status}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"üèÜ Best Rank: {best['rank']:.2f} (Checkpoint {best['checkpoint']})\")\n",
    "print(f\"üìâ Parameters: {params:,}\")\n",
    "\n",
    "# Plot top 3\n",
    "plt.figure(figsize=(10, 6))\n",
    "for r in results[:3]:\n",
    "    plt.plot(r['mean_ranks'], label=f\"GNN Ckpt-{r['checkpoint']} (Rank {r['rank']:.0f})\")\n",
    "\n",
    "plt.axhline(y=27, color='r', linestyle='--', label='Transformer Baseline (Rank 27)')\n",
    "plt.yscale('log')\n",
    "plt.title(f'GNN ({params:,} params) vs Transformer (431k params)')\n",
    "plt.xlabel('Traces')\n",
    "plt.ylabel('Key Rank')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('gnn_results.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef5869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Running on Google Colab\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "ðŸš€ GPU detected! Training will be accelerated.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"ðŸ“ Running on local Jupyter\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"ðŸš€ GPU detected! Training will be accelerated.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Training will be slower on CPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"ðŸ’¡ Enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba69719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EstraNet'...\n",
      "remote: Enumerating objects: 95, done.\u001b[K\n",
      "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "remote: Total 95 (delta 43), reused 88 (delta 36), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (95/95), 2.92 MiB | 8.36 MiB/s, done.\n",
      "Resolving deltas: 100% (43/43), done.\n",
      "âœ… Clean! Directory: /content/EstraNet\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "if os.path.exists('EstraNet'):\n",
    "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
    "    \n",
    "!git clone https://github.com/loshithan/EstraNet.git\n",
    "os.chdir('EstraNet')\n",
    "print(f\"âœ… Clean! Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4307c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Downloading ASCADf dataset from Google Drive...\n",
      "   This may take a few minutes (~1.5 GB)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\n",
      "To: /content/EstraNet/data/ASCAD.h5\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.6M/46.6M [00:00<00:00, 73.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Dataset downloaded successfully!\n",
      "\n",
      "ðŸ“Š Dataset info:\n",
      "  Keys: ['Attack_traces', 'Profiling_traces']\n",
      "  Profiling traces shape: (50000, 700)\n",
      "  Attack traces shape: (10000, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ASCADf dataset configuration\n",
    "file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
    "DATASET_PATH = \"data/ASCAD.h5\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"ðŸ“¥ Downloading ASCADf dataset from Google Drive...\")\n",
    "    print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
    "    \n",
    "    # Download using gdown\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
    "    \n",
    "    print(\"\\nâœ… Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"âœ… Dataset already exists\")\n",
    "\n",
    "# Verify dataset\n",
    "import h5py\n",
    "with h5py.File(DATASET_PATH, 'r') as f:\n",
    "    print(f\"\\nðŸ“Š Dataset info:\")\n",
    "    print(f\"  Keys: {list(f.keys())}\")\n",
    "    if 'Profiling_traces' in f:\n",
    "        print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
    "    if 'Attack_traces' in f:\n",
    "        print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027da6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "print(\"ðŸ“¦ Installing dependencies...\\n\")\n",
    "# %pip install -q absl-py==2.3.1 numpy==1.24.3 scipy==1.10.1 h5py==3.11.0\n",
    "\n",
    "# Install gdown for downloading from Google Drive\n",
    "%pip install -q gdown\n",
    "\n",
    "# Note: Using TensorFlow version pre-installed in Colab (2.16+ / 2.19+)\n",
    "# The compatibility fixes in Section 3 work with all TensorFlow 2.13+ versions\n",
    "print(\"\\nâœ… All dependencies installed!\")\n",
    "print(f\"Using TensorFlow {tf.__version__} (pre-installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd80811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN GNN MODEL IN COLAB\n",
    "# ============================================================================\n",
    "# Paste this into a new Colab cell\n",
    "\n",
    "print(\"ðŸ”· Training GNN Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/EstraNet/checkpoints_gnn',\n",
    "    'result_path': 'results/gnn',\n",
    "    'train_steps': 50000,\n",
    "    'save_steps': 5000,\n",
    "    'train_batch_size': 256,\n",
    "    'eval_batch_size': 32,\n",
    "    'learning_rate': 0.00025,\n",
    "    'model_type': 'gnn',  # KEY: Use GNN\n",
    "}\n",
    "\n",
    "import os\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['result_path'], exist_ok=True)\n",
    "\n",
    "# Build training command\n",
    "train_cmd = f\"\"\"\n",
    "python train_trans.py \\\\\n",
    "    --data_path=data/ASCAD.h5 \\\\\n",
    "    --checkpoint_dir={CONFIG['checkpoint_dir']} \\\\\n",
    "    --model_type={CONFIG['model_type']} \\\\\n",
    "    --dataset=ASCAD \\\\\n",
    "    --input_length=10000 \\\\\n",
    "    --eval_batch_size={CONFIG['eval_batch_size']} \\\\\n",
    "    --n_layer=2 \\\\\n",
    "    --d_model=128 \\\\\n",
    "    --d_inner=256 \\\\\n",
    "    --n_head_softmax=8 \\\\\n",
    "    --d_head_softmax=16 \\\\\n",
    "    --dropout=0.05 \\\\\n",
    "    --conv_kernel_size=3 \\\\\n",
    "    --n_conv_layer=2 \\\\\n",
    "    --pool_size=20 \\\\\n",
    "    --beta_hat_2=150 \\\\\n",
    "    --model_normalization=preLC \\\\\n",
    "    --softmax_attn=True \\\\\n",
    "    --do_train=True \\\\\n",
    "    --learning_rate={CONFIG['learning_rate']} \\\\\n",
    "    --clip=0.25 \\\\\n",
    "    --min_lr_ratio=0.004 \\\\\n",
    "    --warmup_steps=0 \\\\\n",
    "    --train_batch_size={CONFIG['train_batch_size']} \\\\\n",
    "    --train_steps={CONFIG['train_steps']} \\\\\n",
    "    --iterations=500 \\\\\n",
    "    --save_steps={CONFIG['save_steps']} \\\\\n",
    "    --result_path={CONFIG['result_path']}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting GNN training...\")\n",
    "print(f\"Model: GNN (211,876 parameters - 51% less than Transformer)\")\n",
    "print(f\"Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"Training steps: {CONFIG['train_steps']:,}\\n\")\n",
    "\n",
    "!{train_cmd}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BACKUP GNN CHECKPOINTS TO GOOGLE DRIVE\n",
    "# ============================================================================\n",
    "# Paste this into a Colab cell after training\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create backup folder with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_dir = f\"/content/drive/MyDrive/EstraNet_GNN_Backup_{timestamp}\"\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "print(f\"âœ… Created backup folder: {backup_dir}\")\n",
    "\n",
    "# The checkpoints are already in Drive, but let's also backup locally trained files\n",
    "print(\"\\nðŸ“‚ Backing up GNN checkpoint files...\")\n",
    "\n",
    "# Copy checkpoint files from local directory (if any were saved locally)\n",
    "local_checkpoint_files = glob.glob(\"trans_long*\") + glob.glob(\"checkpoint\") + glob.glob(\"loss.pkl\")\n",
    "if local_checkpoint_files:\n",
    "    print(f\"Found {len(local_checkpoint_files)} local checkpoint files\")\n",
    "    for file in local_checkpoint_files:\n",
    "        try:\n",
    "            shutil.copy2(file, backup_dir)\n",
    "            print(f\"  âœ… {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {file}: {e}\")\n",
    "else:\n",
    "    print(\"  No local checkpoints (they're in Drive already)\")\n",
    "\n",
    "# Copy GNN-specific code files\n",
    "print(\"\\nðŸ“„ Backing up GNN code files...\")\n",
    "code_files = [\n",
    "    'gnn_layers.py',\n",
    "    'gnn_estranet.py', \n",
    "    'train_trans.py',\n",
    "    'transformer.py',\n",
    "    'fast_attention.py'\n",
    "]\n",
    "\n",
    "for file in code_files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            shutil.copy2(file, backup_dir)\n",
    "            print(f\"  âœ… {file}\")\n",
    "        except:\n",
    "            print(f\"  âš ï¸  {file} not found\")\n",
    "\n",
    "# Copy results folder\n",
    "print(\"\\nðŸ“Š Backing up results...\")\n",
    "if os.path.exists(\"results\"):\n",
    "    try:\n",
    "        shutil.copytree(\"results\", f\"{backup_dir}/results\")\n",
    "        print(\"  âœ… results/\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Failed: {e}\")\n",
    "\n",
    "# Verify Drive checkpoints exist\n",
    "print(\"\\nðŸ” Verifying GNN checkpoints in Drive...\")\n",
    "drive_checkpoint_dir = \"/content/drive/MyDrive/EstraNet/checkpoints_gnn\"\n",
    "if os.path.exists(drive_checkpoint_dir):\n",
    "    drive_files = os.listdir(drive_checkpoint_dir)\n",
    "    checkpoint_count = len([f for f in drive_files if 'trans_long' in f or 'checkpoint' in f])\n",
    "    print(f\"  âœ… Found {checkpoint_count} files in {drive_checkpoint_dir}\")\n",
    "else:\n",
    "    print(f\"  âš ï¸  Drive checkpoint directory not found: {drive_checkpoint_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ BACKUP COMPLETE!\")\n",
    "print(f\"ðŸ“ Backup location: {backup_dir}\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

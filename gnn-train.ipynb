{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef5869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Running on Google Colab\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "üöÄ GPU detected! Training will be accelerated.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"üìù Running on local Jupyter\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"üöÄ GPU detected! Training will be accelerated.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"üí° Enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba69719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EstraNet'...\n",
      "remote: Enumerating objects: 108, done.\u001b[K\n",
      "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
      "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
      "remote: Total 108 (delta 47), reused 99 (delta 38), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (108/108), 2.95 MiB | 8.50 MiB/s, done.\n",
      "Resolving deltas: 100% (47/47), done.\n",
      "‚úÖ Clean! Directory: /content/EstraNet\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "if os.path.exists('EstraNet'):\n",
    "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
    "    \n",
    "!git clone https://github.com/loshithan/EstraNet.git\n",
    "os.chdir('EstraNet')\n",
    "print(f\"‚úÖ Clean! Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4307c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading ASCADf dataset from Google Drive...\n",
      "   This may take a few minutes (~1.5 GB)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\n",
      "To: /content/EstraNet/data/ASCAD.h5\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46.6M/46.6M [00:00<00:00, 176MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset downloaded successfully!\n",
      "\n",
      "üìä Dataset info:\n",
      "  Keys: ['Attack_traces', 'Profiling_traces']\n",
      "  Profiling traces shape: (50000, 700)\n",
      "  Attack traces shape: (10000, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ASCADf dataset configuration\n",
    "file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
    "DATASET_PATH = \"data/ASCAD.h5\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"üì• Downloading ASCADf dataset from Google Drive...\")\n",
    "    print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
    "    \n",
    "    # Download using gdown\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists\")\n",
    "\n",
    "# Verify dataset\n",
    "import h5py\n",
    "with h5py.File(DATASET_PATH, 'r') as f:\n",
    "    print(f\"\\nüìä Dataset info:\")\n",
    "    print(f\"  Keys: {list(f.keys())}\")\n",
    "    if 'Profiling_traces' in f:\n",
    "        print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
    "    if 'Attack_traces' in f:\n",
    "        print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027da6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "\n",
      "\n",
      "‚úÖ All dependencies installed!\n",
      "Using TensorFlow 2.19.0 (pre-installed)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "# %pip install -q absl-py==2.3.1 numpy==1.24.3 scipy==1.10.1 h5py==3.11.0\n",
    "\n",
    "# Install gdown for downloading from Google Drive\n",
    "%pip install -q gdown\n",
    "\n",
    "# Note: Using TensorFlow version pre-installed in Colab (2.16+ / 2.19+)\n",
    "# The compatibility fixes in Section 3 work with all TensorFlow 2.13+ versions\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "print(f\"Using TensorFlow {tf.__version__} (pre-installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd80811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∑ Training GNN Model\n",
      "======================================================================\n",
      "Starting GNN training...\n",
      "Model: GNN (211,876 parameters - 51% less than Transformer)\n",
      "Checkpoints: /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "Training steps: 50,000\n",
      "\n",
      "2026-02-12 15:58:15.499948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770911895.519777    1803 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770911895.525866    1803 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770911895.540324    1803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770911895.540350    1803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770911895.540354    1803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770911895.540357    1803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚úÖ File replaced with fixed version\n",
      "‚úÖ Checkpoints deleted\n",
      "\n",
      "‚ö†Ô∏è  NOW RESTART RUNTIME: Runtime ‚Üí Restart runtime\n",
      "Then run your training again\n",
      "INFO:tensorflow:\n",
      "I0212 15:58:20.831077 133390619312128 train_trans.py:429] \n",
      "INFO:tensorflow:\n",
      "I0212 15:58:20.831294 133390619312128 train_trans.py:430] \n",
      "INFO:tensorflow:use_tpu               : False\n",
      "I0212 15:58:20.831385 133390619312128 train_trans.py:431] use_tpu               : False\n",
      "INFO:tensorflow:data_path             : data/ASCAD.h5\n",
      "I0212 15:58:20.831457 133390619312128 train_trans.py:432] data_path             : data/ASCAD.h5\n",
      "INFO:tensorflow:dataset               : ASCAD\n",
      "I0212 15:58:20.831517 133390619312128 train_trans.py:433] dataset               : ASCAD\n",
      "INFO:tensorflow:checkpoint_dir        : /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "I0212 15:58:20.831573 133390619312128 train_trans.py:434] checkpoint_dir        : /content/drive/MyDrive/EstraNet/checkpoints_gnn\n",
      "INFO:tensorflow:checkpoint_idx        : 0\n",
      "I0212 15:58:20.831626 133390619312128 train_trans.py:435] checkpoint_idx        : 0\n",
      "INFO:tensorflow:warm_start            : False\n",
      "I0212 15:58:20.831677 133390619312128 train_trans.py:436] warm_start            : False\n",
      "INFO:tensorflow:result_path           : results/gnn\n",
      "I0212 15:58:20.831727 133390619312128 train_trans.py:437] result_path           : results/gnn\n",
      "INFO:tensorflow:do_train              : True\n",
      "I0212 15:58:20.831777 133390619312128 train_trans.py:438] do_train              : True\n",
      "INFO:tensorflow:learning_rate         : 0.00025\n",
      "I0212 15:58:20.831849 133390619312128 train_trans.py:439] learning_rate         : 0.00025\n",
      "INFO:tensorflow:clip                  : 0.25\n",
      "I0212 15:58:20.831902 133390619312128 train_trans.py:440] clip                  : 0.25\n",
      "INFO:tensorflow:min_lr_ratio          : 0.004\n",
      "I0212 15:58:20.831954 133390619312128 train_trans.py:441] min_lr_ratio          : 0.004\n",
      "INFO:tensorflow:warmup_steps          : 0\n",
      "I0212 15:58:20.832002 133390619312128 train_trans.py:442] warmup_steps          : 0\n",
      "INFO:tensorflow:input_length          : 10000\n",
      "I0212 15:58:20.832052 133390619312128 train_trans.py:443] input_length          : 10000\n",
      "INFO:tensorflow:data_desync           : 0\n",
      "I0212 15:58:20.832101 133390619312128 train_trans.py:444] data_desync           : 0\n",
      "INFO:tensorflow:train_batch_size      : 256\n",
      "I0212 15:58:20.832166 133390619312128 train_trans.py:445] train_batch_size      : 256\n",
      "INFO:tensorflow:eval_batch_size       : 32\n",
      "I0212 15:58:20.832219 133390619312128 train_trans.py:446] eval_batch_size       : 32\n",
      "INFO:tensorflow:train_steps           : 50000\n",
      "I0212 15:58:20.832267 133390619312128 train_trans.py:447] train_steps           : 50000\n",
      "INFO:tensorflow:iterations            : 500\n",
      "I0212 15:58:20.832317 133390619312128 train_trans.py:448] iterations            : 500\n",
      "INFO:tensorflow:save_steps            : 5000\n",
      "I0212 15:58:20.832366 133390619312128 train_trans.py:449] save_steps            : 5000\n",
      "INFO:tensorflow:n_layer               : 2\n",
      "I0212 15:58:20.832416 133390619312128 train_trans.py:450] n_layer               : 2\n",
      "INFO:tensorflow:d_model               : 128\n",
      "I0212 15:58:20.832465 133390619312128 train_trans.py:451] d_model               : 128\n",
      "INFO:tensorflow:d_head                : 32\n",
      "I0212 15:58:20.832515 133390619312128 train_trans.py:452] d_head                : 32\n",
      "INFO:tensorflow:n_head                : 4\n",
      "I0212 15:58:20.832565 133390619312128 train_trans.py:453] n_head                : 4\n",
      "INFO:tensorflow:d_inner               : 256\n",
      "I0212 15:58:20.832614 133390619312128 train_trans.py:454] d_inner               : 256\n",
      "INFO:tensorflow:n_head_softmax        : 8\n",
      "I0212 15:58:20.832666 133390619312128 train_trans.py:455] n_head_softmax        : 8\n",
      "INFO:tensorflow:d_head_softmax        : 16\n",
      "I0212 15:58:20.832718 133390619312128 train_trans.py:456] d_head_softmax        : 16\n",
      "INFO:tensorflow:dropout               : 0.05\n",
      "I0212 15:58:20.832771 133390619312128 train_trans.py:457] dropout               : 0.05\n",
      "INFO:tensorflow:conv_kernel_size      : 3\n",
      "I0212 15:58:20.832839 133390619312128 train_trans.py:458] conv_kernel_size      : 3\n",
      "INFO:tensorflow:n_conv_layer          : 2\n",
      "I0212 15:58:20.832891 133390619312128 train_trans.py:459] n_conv_layer          : 2\n",
      "INFO:tensorflow:pool_size             : 20\n",
      "I0212 15:58:20.832941 133390619312128 train_trans.py:460] pool_size             : 20\n",
      "INFO:tensorflow:d_kernel_map          : 128\n",
      "I0212 15:58:20.832991 133390619312128 train_trans.py:461] d_kernel_map          : 128\n",
      "INFO:tensorflow:beta_hat_2            : 150\n",
      "I0212 15:58:20.833041 133390619312128 train_trans.py:462] beta_hat_2            : 150\n",
      "INFO:tensorflow:model_normalization   : preLC\n",
      "I0212 15:58:20.833092 133390619312128 train_trans.py:463] model_normalization   : preLC\n",
      "INFO:tensorflow:head_initialization   : forward\n",
      "I0212 15:58:20.833143 133390619312128 train_trans.py:464] head_initialization   : forward\n",
      "INFO:tensorflow:softmax_attn          : True\n",
      "I0212 15:58:20.833202 133390619312128 train_trans.py:465] softmax_attn          : True\n",
      "INFO:tensorflow:max_eval_batch        : -1\n",
      "I0212 15:58:20.833254 133390619312128 train_trans.py:466] max_eval_batch        : -1\n",
      "INFO:tensorflow:output_attn           : False\n",
      "I0212 15:58:20.833305 133390619312128 train_trans.py:467] output_attn           : False\n",
      "INFO:tensorflow:\n",
      "I0212 15:58:20.833355 133390619312128 train_trans.py:468] \n",
      "INFO:tensorflow:\n",
      "I0212 15:58:20.833406 133390619312128 train_trans.py:469] \n",
      "INFO:tensorflow:Number of accelerators: 1\n",
      "I0212 15:58:46.859019 133390619312128 train_trans.py:509] Number of accelerators: 1\n",
      "INFO:tensorflow:num of train batches 195\n",
      "I0212 15:58:46.859200 133390619312128 train_trans.py:522] num of train batches 195\n",
      "INFO:tensorflow:num of test batches 312\n",
      "I0212 15:58:46.859279 133390619312128 train_trans.py:523] num of test batches 312\n",
      "2026-02-12 15:58:47.195943: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1770911927.198652    1803 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13757 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0212 15:58:47.399580 133390619312128 functional_saver.py:440] Sharding callback duration: 7 microseconds\n",
      "INFO:tensorflow:Model saved in path: /content/drive/MyDrive/EstraNet/checkpoints_gnn/trans_long-1\n",
      "I0212 15:58:47.409911 133390619312128 train_trans.py:273] Model saved in path: /content/drive/MyDrive/EstraNet/checkpoints_gnn/trans_long-1\n",
      "INFO:tensorflow:Starting training ... \n",
      "I0212 15:58:47.410460 133390619312128 train_trans.py:331] Starting training ... \n",
      "Traceback (most recent call last):\n",
      "  File \"/content/EstraNet/train_trans.py\", line 577, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "             ^^^^^^^^^^\n",
      "  File \"/content/EstraNet/train_trans.py\", line 525, in main\n",
      "    train(train_data.GetTFRecords(FLAGS.train_batch_size, training=True), \\\n",
      "  File \"/content/EstraNet/train_trans.py\", line 336, in train\n",
      "    train_steps(train_iter, tf.convert_to_tensor(FLAGS.iterations), \\\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filebr1v094z.py\", line 66, in tf__train_steps\n",
      "    ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(steps),), None, fscope), None, loop_body, get_state_1, set_state_1, ('global_step',), {'iterate_names': '_'})\n",
      "  File \"/tmp/__autograph_generated_filebr1v094z.py\", line 62, in loop_body\n",
      "    ag__.converted_call(ag__.ld(strategy).run, (ag__.ld(step_fn),), dict(args=(ag__.ld(inps), ag__.ld(lbls), ag__.ld(global_step))), fscope)\n",
      "  File \"/tmp/__autograph_generated_filebr1v094z.py\", line 23, in step_fn\n",
      "    logits = ag__.converted_call(ag__.ld(model), (), dict(inputs=ag__.ld(inps), softmax_attn_smoothing=ag__.ld(softmax_attn_smoothing), training=True), fscope_1)[0]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/inspect.py\", line 3280, in bind\n",
      "    return self._bind(args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/inspect.py\", line 3269, in _bind\n",
      "    raise TypeError(\n",
      "TypeError: in user code:\n",
      "\n",
      "    File \"/content/EstraNet/train_trans.py\", line 290, in step_fn  *\n",
      "        logits = model(inputs=inps, softmax_attn_smoothing=softmax_attn_smoothing, training=True)[0]\n",
      "    File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/usr/lib/python3.12/inspect.py\", line 3280, in bind\n",
      "        return self._bind(args, kwargs)\n",
      "    File \"/usr/lib/python3.12/inspect.py\", line 3269, in _bind\n",
      "        raise TypeError(\n",
      "\n",
      "    TypeError: got an unexpected keyword argument 'softmax_attn_smoothing'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN GNN MODEL IN COLAB\n",
    "# ============================================================================\n",
    "# Paste this into a new Colab cell\n",
    "\n",
    "print(\"üî∑ Training GNN Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/EstraNet/checkpoints_gnn',\n",
    "    'result_path': 'results/gnn',\n",
    "    'train_steps': 50000,\n",
    "    'save_steps': 5000,\n",
    "    'train_batch_size': 256,\n",
    "    'eval_batch_size': 32,\n",
    "    'learning_rate': 0.00025,\n",
    "    'model_type': 'gnn',  # KEY: Use GNN\n",
    "}\n",
    "\n",
    "import os\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['result_path'], exist_ok=True)\n",
    "\n",
    "# Build training command\n",
    "train_cmd = f\"\"\"\n",
    "python train_trans.py \\\\\n",
    "    --data_path=data/ASCAD.h5 \\\\\n",
    "    --checkpoint_dir={CONFIG['checkpoint_dir']} \\\\\n",
    "    --model_type={CONFIG['model_type']} \\\\\n",
    "    --dataset=ASCAD \\\\\n",
    "    --input_length=10000 \\\\\n",
    "    --eval_batch_size={CONFIG['eval_batch_size']} \\\\\n",
    "    --n_layer=2 \\\\\n",
    "    --d_model=128 \\\\\n",
    "    --d_inner=256 \\\\\n",
    "    --n_head_softmax=8 \\\\\n",
    "    --d_head_softmax=16 \\\\\n",
    "    --dropout=0.05 \\\\\n",
    "    --conv_kernel_size=3 \\\\\n",
    "    --n_conv_layer=2 \\\\\n",
    "    --pool_size=20 \\\\\n",
    "    --beta_hat_2=150 \\\\\n",
    "    --model_normalization=preLC \\\\\n",
    "    --softmax_attn=True \\\\\n",
    "    --do_train=True \\\\\n",
    "    --learning_rate={CONFIG['learning_rate']} \\\\\n",
    "    --clip=0.25 \\\\\n",
    "    --min_lr_ratio=0.004 \\\\\n",
    "    --warmup_steps=0 \\\\\n",
    "    --train_batch_size={CONFIG['train_batch_size']} \\\\\n",
    "    --train_steps={CONFIG['train_steps']} \\\\\n",
    "    --iterations=500 \\\\\n",
    "    --save_steps={CONFIG['save_steps']} \\\\\n",
    "    --result_path={CONFIG['result_path']}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting GNN training...\")\n",
    "print(f\"Model: GNN (211,876 parameters - 51% less than Transformer)\")\n",
    "print(f\"Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"Training steps: {CONFIG['train_steps']:,}\\n\")\n",
    "\n",
    "!{train_cmd}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting Google Drive...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mountpoint must not already contain files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-39171766.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mounting Google Drive...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Create backup folder with timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BACKUP GNN CHECKPOINTS TO GOOGLE DRIVE\n",
    "# ============================================================================\n",
    "# Paste this into a Colab cell after training\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "# Check if already mounted, if not then mount\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"‚úÖ Drive already mounted\")\n",
    "\n",
    "# Create backup folder with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_dir = f\"/content/drive/MyDrive/EstraNet_GNN_Backup_{timestamp}\"\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Created backup folder: {backup_dir}\")\n",
    "\n",
    "# The checkpoints are already in Drive, but let's also backup locally trained files\n",
    "print(\"\\nüìÇ Backing up GNN checkpoint files...\")\n",
    "\n",
    "# Copy checkpoint files from local directory (if any were saved locally)\n",
    "local_checkpoint_files = glob.glob(\"trans_long*\") + glob.glob(\"checkpoint\") + glob.glob(\"loss.pkl\")\n",
    "if local_checkpoint_files:\n",
    "    print(f\"Found {len(local_checkpoint_files)} local checkpoint files\")\n",
    "    for file in local_checkpoint_files:\n",
    "        try:\n",
    "            shutil.copy2(file, backup_dir)\n",
    "            print(f\"  ‚úÖ {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {file}: {e}\")\n",
    "else:\n",
    "    print(\"  No local checkpoints (they're in Drive already)\")\n",
    "\n",
    "# Copy GNN-specific code files\n",
    "print(\"\\nüìÑ Backing up GNN code files...\")\n",
    "code_files = [\n",
    "    'gnn_layers.py',\n",
    "    'gnn_estranet.py', \n",
    "    'train_trans.py',\n",
    "    'transformer.py',\n",
    "    'fast_attention.py'\n",
    "]\n",
    "\n",
    "for file in code_files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            shutil.copy2(file, backup_dir)\n",
    "            print(f\"  ‚úÖ {file}\")\n",
    "        except:\n",
    "            print(f\"  ‚ö†Ô∏è  {file} not found\")\n",
    "\n",
    "# Copy results folder\n",
    "print(\"\\nüìä Backing up results...\")\n",
    "if os.path.exists(\"results\"):\n",
    "    try:\n",
    "        shutil.copytree(\"results\", f\"{backup_dir}/results\")\n",
    "        print(\"  ‚úÖ results/\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {e}\")\n",
    "\n",
    "# Verify Drive checkpoints exist\n",
    "print(\"\\nüîç Verifying GNN checkpoints in Drive...\")\n",
    "drive_checkpoint_dir = \"/content/drive/MyDrive/EstraNet/checkpoints_gnn\"\n",
    "if os.path.exists(drive_checkpoint_dir):\n",
    "    drive_files = os.listdir(drive_checkpoint_dir)\n",
    "    checkpoint_count = len([f for f in drive_files if 'trans_long' in f or 'checkpoint' in f])\n",
    "    print(f\"  ‚úÖ Found {checkpoint_count} files in {drive_checkpoint_dir}\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Drive checkpoint directory not found: {drive_checkpoint_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ BACKUP COMPLETE!\")\n",
    "print(f\"üìÅ Backup location: {backup_dir}\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESTRaNet — Checkpoint → INT8 TFLite → MCU Deployment\n",
    "\n",
    "This notebook:\n",
    "1. Loads your pre-trained ESTRaNet checkpoint\n",
    "2. Converts to INT8 TFLite with Post-Training Quantization\n",
    "3. Evaluates key rank on test data\n",
    "4. Prepares the model for MCU deployment\n",
    "\n",
    "> **No training** — uses existing checkpoint: `trans_long-5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 — Setup & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "MessageError",
     "evalue": "User cancelled dfs_ephemeral authorization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3106673184.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or wherever your repo is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMessageError\u001b[0m: User cancelled dfs_ephemeral authorization"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive')  # or wherever your repo is\n",
    "print('Working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q absl-py h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print('TensorFlow:', tf.__version__)\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 — Configuration\n",
    "\n",
    "Update these paths to match your Drive structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paths (EDIT THESE) ────────────────────────────────────────────────────────\n",
    "REPO_PATH      = '/content/drive/MyDrive/estranet'  # your ESTRaNet code repo\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/checkpoints_transformer_desync0'\n",
    "CHECKPOINT_IDX = 5  # trans_long-5\n",
    "\n",
    "# Dataset for evaluation & calibration\n",
    "DATA_PATH      = '/content/drive/MyDrive/ASCAD.h5'\n",
    "DATASET_TYPE   = 'ASCAD'  # or 'CHES20'\n",
    "\n",
    "# Model config (must match your trained checkpoint)\n",
    "CONFIG = dict(\n",
    "    input_length      = 700,\n",
    "    n_layer           = 6,\n",
    "    d_model           = 128,\n",
    "    d_head            = 32,\n",
    "    n_head            = 4,\n",
    "    d_inner           = 256,\n",
    "    n_head_softmax    = 4,\n",
    "    d_head_softmax    = 32,\n",
    "    dropout           = 0.1,  # will be set to 0.0 at inference\n",
    "    n_conv_layer      = 1,\n",
    "    pool_size         = 2,\n",
    "    d_kernel_map      = 512,\n",
    "    beta_hat_2        = 150,\n",
    "    model_normalization = 'preLC',\n",
    "    head_initialization = 'forward',\n",
    ")\n",
    "\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "print(f'Checkpoint: {CHECKPOINT_DIR}/trans_long-{CHECKPOINT_IDX}')\n",
    "print(f'Dataset:    {DATA_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 — Load Dataset\n",
    "\n",
    "We need test data for two purposes:\n",
    "1. **Calibration dataset** for INT8 quantization (~200 traces)\n",
    "2. **Full test set** for key rank evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "\n",
    "# Load test split\n",
    "test_data = data_utils.Dataset(\n",
    "    data_path    = DATA_PATH,\n",
    "    split        = 'test',\n",
    "    input_length = CONFIG['input_length'],\n",
    "    data_desync  = 0  # use 0 for fixed evaluation, or match your training desync\n",
    ")\n",
    "\n",
    "print(f'Test traces: {test_data.num_samples}')\n",
    "print(f'Plaintexts: {test_data.plaintexts.shape}')\n",
    "print(f'Keys:       {test_data.keys.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 — Restore Pre-Trained Model\n",
    "\n",
    "Creates the model architecture and loads your checkpoint weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "\n",
    "n_classes = 256 if DATASET_TYPE == 'ASCAD' else 4\n",
    "\n",
    "model = Transformer(\n",
    "    n_layer             = CONFIG['n_layer'],\n",
    "    d_model             = CONFIG['d_model'],\n",
    "    d_head              = CONFIG['d_head'],\n",
    "    n_head              = CONFIG['n_head'],\n",
    "    d_inner             = CONFIG['d_inner'],\n",
    "    d_head_softmax      = CONFIG['d_head_softmax'],\n",
    "    n_head_softmax      = CONFIG['n_head_softmax'],\n",
    "    dropout             = 0.0,  # inference mode\n",
    "    n_classes           = n_classes,\n",
    "    conv_kernel_size    = 3,\n",
    "    n_conv_layer        = CONFIG['n_conv_layer'],\n",
    "    pool_size           = CONFIG['pool_size'],\n",
    "    d_kernel_map        = CONFIG['d_kernel_map'],\n",
    "    beta_hat_2          = CONFIG['beta_hat_2'],\n",
    "    model_normalization = CONFIG['model_normalization'],\n",
    "    head_initialization = CONFIG['head_initialization'],\n",
    "    softmax_attn        = True,\n",
    "    output_attn         = False,\n",
    ")\n",
    "\n",
    "# Build model with dummy input\n",
    "_ = model(tf.zeros([1, CONFIG['input_length']]))\n",
    "\n",
    "# Restore checkpoint\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "chk_path = os.path.join(CHECKPOINT_DIR, f'trans_long-{CHECKPOINT_IDX}')\n",
    "\n",
    "print(f'Restoring: {chk_path}')\n",
    "status = checkpoint.read(chk_path)\n",
    "status.expect_partial()  # optimizer weights may not be present, that's OK\n",
    "\n",
    "print('✓ Checkpoint restored successfully')\n",
    "print(f'  Total parameters: {model.count_params():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 — Evaluate FP32 Model (Baseline)\n",
    "\n",
    "Get key rank on the test set before quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_utils\n",
    "\n",
    "# Run inference on full test set\n",
    "test_dataset = test_data.GetTFRecords(batch_size=32, training=False)\n",
    "predictions = model.predict(test_dataset, verbose=1)\n",
    "\n",
    "# predictions is a list [logits, attention_weights] but we only need logits\n",
    "test_scores = predictions[0] if isinstance(predictions, list) else predictions\n",
    "\n",
    "print(f'Predictions shape: {test_scores.shape}')\n",
    "\n",
    "# Compute key rank (run 100 times with random noise for robustness)\n",
    "print('\\nComputing key rank (100 iterations)...')\n",
    "key_rank_list = []\n",
    "for _ in range(100):\n",
    "    key_ranks = evaluation_utils.compute_key_rank(\n",
    "        test_scores,\n",
    "        test_data.plaintexts,\n",
    "        test_data.keys\n",
    "    )\n",
    "    key_rank_list.append(key_ranks)\n",
    "\n",
    "key_ranks_fp32 = np.stack(key_rank_list, axis=0)\n",
    "mean_ranks_fp32 = np.mean(key_ranks_fp32, axis=0)\n",
    "\n",
    "print('\\n─── FP32 Baseline Key Rank ───')\n",
    "print(f'  Min rank:  {mean_ranks_fp32.min():.2f}')\n",
    "print(f'  Rank @ 10:  {mean_ranks_fp32[9]:.2f}')\n",
    "print(f'  Rank @ 100: {mean_ranks_fp32[99]:.2f}')\n",
    "print(f'  Rank @ 500: {mean_ranks_fp32[499]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 — Export to SavedModel\n",
    "\n",
    "Required intermediate step for TFLite conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = os.path.join(CHECKPOINT_DIR, 'saved_model')\n",
    "\n",
    "# Define serving signature with fixed input shape\n",
    "@tf.function(input_signature=[tf.TensorSpec([None, CONFIG['input_length']], tf.float32)])\n",
    "def serving_fn(inputs):\n",
    "    # Return only logits (index 0), not attention weights\n",
    "    return model(inputs, training=False)[0]\n",
    "\n",
    "# Save\n",
    "tf.saved_model.save(\n",
    "    model,\n",
    "    saved_model_path,\n",
    "    signatures={'serving_default': serving_fn}\n",
    ")\n",
    "\n",
    "print(f'✓ SavedModel exported to: {saved_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 — INT8 Post-Training Quantization\n",
    "\n",
    "Converts the FP32 model to INT8 using a calibration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Prepare calibration dataset ───────────────────────────────────────────────\n",
    "NUM_CALIB_SAMPLES = 200\n",
    "\n",
    "# Get calibration traces from test set\n",
    "calib_dataset = test_data.GetTFRecords(batch_size=NUM_CALIB_SAMPLES, training=False)\n",
    "calib_traces, _ = next(iter(calib_dataset))\n",
    "calib_traces = calib_traces.numpy()\n",
    "\n",
    "print(f'Calibration dataset: {calib_traces.shape}')\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    \"\"\"Generator for calibration data.\"\"\"\n",
    "    for i in range(calib_traces.shape[0]):\n",
    "        # TFLite expects batch dimension\n",
    "        yield [calib_traces[i:i+1]]\n",
    "\n",
    "# ── Convert to TFLite INT8 ────────────────────────────────────────────────────\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "\n",
    "# Enable INT8 quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "# Force INT8 for all operations\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS,  # fallback for ops like einsum\n",
    "]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Allow custom ops if needed (LayerCentering, etc.)\n",
    "converter.allow_custom_ops = True\n",
    "\n",
    "print('\\nConverting to INT8 TFLite (this may take 1-2 minutes)...')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "tflite_path = os.path.join(CHECKPOINT_DIR, 'estranet_int8.tflite')\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "size_fp32 = model.count_params() * 4 / (1024**2)  # MB\n",
    "size_int8 = len(tflite_model) / (1024**2)  # MB\n",
    "compression_ratio = size_fp32 / size_int8\n",
    "\n",
    "print(f'\\n✓ INT8 TFLite model saved to: {tflite_path}')\n",
    "print(f'  FP32 model size: {size_fp32:.2f} MB')\n",
    "print(f'  INT8 model size: {size_int8:.2f} MB')\n",
    "print(f'  Compression:     {compression_ratio:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 — Evaluate INT8 Model\n",
    "\n",
    "Run inference with the quantized model and check accuracy degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load TFLite interpreter ───────────────────────────────────────────────────\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details  = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "print('TFLite model loaded')\n",
    "print(f\"  Input:  {input_details['shape']} {input_details['dtype']}\")\n",
    "print(f\"  Output: {output_details['shape']} {output_details['dtype']}\")\n",
    "\n",
    "# ── Get quantization parameters ───────────────────────────────────────────────\n",
    "input_scale, input_zero_point = input_details['quantization']\n",
    "output_scale, output_zero_point = output_details['quantization']\n",
    "\n",
    "print(f'\\nQuantization params:')\n",
    "print(f'  Input:  scale={input_scale:.6f}, zero_point={input_zero_point}')\n",
    "print(f'  Output: scale={output_scale:.6f}, zero_point={output_zero_point}')\n",
    "\n",
    "# ── Run inference on test set ──────────────────────────────────────────────────\n",
    "def quantize_input(x):\n",
    "    \"\"\"Convert FP32 input to INT8.\"\"\"\n",
    "    return (x / input_scale + input_zero_point).astype(np.int8)\n",
    "\n",
    "def dequantize_output(x):\n",
    "    \"\"\"Convert INT8 output back to FP32.\"\"\"\n",
    "    return (x.astype(np.float32) - output_zero_point) * output_scale\n",
    "\n",
    "print('\\nRunning INT8 inference on test set...')\n",
    "test_scores_int8 = []\n",
    "\n",
    "test_dataset_eval = test_data.GetTFRecords(batch_size=1, training=False)\n",
    "for i, (trace_batch, _) in enumerate(test_dataset_eval.take(test_data.num_samples)):\n",
    "    if i % 1000 == 0:\n",
    "        print(f'  {i}/{test_data.num_samples}')\n",
    "    \n",
    "    # Quantize input\n",
    "    trace = trace_batch.numpy()[0]\n",
    "    trace_int8 = quantize_input(trace)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details['index'], trace_int8[None, :])\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get and dequantize output\n",
    "    output_int8 = interpreter.get_tensor(output_details['index'])\n",
    "    output_fp32 = dequantize_output(output_int8)\n",
    "    \n",
    "    test_scores_int8.append(output_fp32[0])\n",
    "\n",
    "test_scores_int8 = np.array(test_scores_int8)\n",
    "print(f'INT8 predictions shape: {test_scores_int8.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compute key rank for INT8 model ───────────────────────────────────────────\n",
    "print('Computing INT8 key rank (100 iterations)...')\n",
    "key_rank_list_int8 = []\n",
    "for _ in range(100):\n",
    "    key_ranks = evaluation_utils.compute_key_rank(\n",
    "        test_scores_int8,\n",
    "        test_data.plaintexts,\n",
    "        test_data.keys\n",
    "    )\n",
    "    key_rank_list_int8.append(key_ranks)\n",
    "\n",
    "key_ranks_int8 = np.stack(key_rank_list_int8, axis=0)\n",
    "mean_ranks_int8 = np.mean(key_ranks_int8, axis=0)\n",
    "\n",
    "print('\\n─── INT8 Key Rank ───')\n",
    "print(f'  Min rank:   {mean_ranks_int8.min():.2f}')\n",
    "print(f'  Rank @ 10:  {mean_ranks_int8[9]:.2f}')\n",
    "print(f'  Rank @ 100: {mean_ranks_int8[99]:.2f}')\n",
    "print(f'  Rank @ 500: {mean_ranks_int8[499]:.2f}')\n",
    "\n",
    "print('\\n─── Accuracy Comparison ───')\n",
    "print(f'  FP32 rank @ 100: {mean_ranks_fp32[99]:.2f}')\n",
    "print(f'  INT8 rank @ 100: {mean_ranks_int8[99]:.2f}')\n",
    "print(f'  Degradation:     {mean_ranks_int8[99] - mean_ranks_fp32[99]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 — MCU Deployment Preparation\n",
    "\n",
    "Convert the `.tflite` file to a C array for embedding in MCU firmware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generate C header file ────────────────────────────────────────────────────\n",
    "c_array_path = os.path.join(CHECKPOINT_DIR, 'estranet_model_data.cc')\n",
    "\n",
    "!xxd -i {tflite_path} > {c_array_path}\n",
    "\n",
    "print(f'✓ C array written to: {c_array_path}')\n",
    "print('\\nTo use in your MCU project:')\n",
    "print('  1. Copy estranet_model_data.cc to your firmware project')\n",
    "print('  2. Include TFLite Micro runtime (github.com/tensorflow/tflite-micro)')\n",
    "print('  3. Link with CMSIS-NN kernels for Cortex-M acceleration')\n",
    "print('\\nExample usage:')\n",
    "print('''\n",
    "  #include \"estranet_model_data.cc\"\n",
    "  \n",
    "  // Load model\n",
    "  tflite::MicroInterpreter interpreter(\n",
    "      tflite::GetModel(g_model),\n",
    "      ops_resolver,\n",
    "      tensor_arena,\n",
    "      kTensorArenaSize\n",
    "  );\n",
    "  \n",
    "  // Run inference\n",
    "  TfLiteTensor* input = interpreter.input(0);\n",
    "  // ... copy your trace to input->data.int8 ...\n",
    "  interpreter.Invoke();\n",
    "  TfLiteTensor* output = interpreter.output(0);\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 — Summary\n",
    "\n",
    "Review final metrics and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('═'*70)\n",
    "print('                    QUANTIZATION SUMMARY')\n",
    "print('═'*70)\n",
    "print(f'\\nCheckpoint:  {CHECKPOINT_DIR}/trans_long-{CHECKPOINT_IDX}')\n",
    "print(f'Dataset:     {DATA_PATH}')\n",
    "print(f'Test traces: {test_data.num_samples}')\n",
    "print(f'\\n┌─ Model Size')\n",
    "print(f'│  FP32:  {size_fp32:.2f} MB')\n",
    "print(f'│  INT8:  {size_int8:.2f} MB')\n",
    "print(f'│  Ratio: {compression_ratio:.1f}x compression')\n",
    "print(f'│')\n",
    "print(f'┌─ Key Rank @ 100 traces')\n",
    "print(f'│  FP32:  {mean_ranks_fp32[99]:.2f}')\n",
    "print(f'│  INT8:  {mean_ranks_int8[99]:.2f}')\n",
    "print(f'│  Loss:  {mean_ranks_int8[99] - mean_ranks_fp32[99]:.2f}')\n",
    "print(f'│')\n",
    "print(f'┌─ Output Files')\n",
    "print(f'│  TFLite: {tflite_path}')\n",
    "print(f'│  C code: {c_array_path}')\n",
    "print('└─')\n",
    "print('\\n✓ Model ready for MCU deployment')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

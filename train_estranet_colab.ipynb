{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EstraNet Training on Google Colab\n",
    "\n",
    "This notebook trains the EstraNet Transformer model for side-channel analysis on ASCAD dataset.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup environment and clone repository\n",
    "2. Download/upload ASCAD dataset\n",
    "3. Train the model\n",
    "4. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# If no GPU, you can enable it via: Runtime â†’ Change runtime type â†’ GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (to save checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/YOUR_USERNAME/EstraNet.git\n",
    "%cd EstraNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download ASCAD Dataset\n",
    "\n",
    "Choose one option below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ASCADf dataset configuration\n",
    "file_id = \"1WNajWT0qFbpqPJiuePS_HeXxsCvUHI5M\"\n",
    "DATASET_PATH = \"data/ASCAD.h5\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"ðŸ“¥ Downloading ASCADf dataset from Google Drive...\")\n",
    "    print(\"   This may take a few minutes (~1.5 GB)\\n\")\n",
    "    \n",
    "    # Download using gdown\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", DATASET_PATH, quiet=False)\n",
    "    \n",
    "    print(\"\\nâœ… Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"âœ… Dataset already exists\")\n",
    "\n",
    "# Verify dataset\n",
    "import h5py\n",
    "with h5py.File(DATASET_PATH, 'r') as f:\n",
    "    print(f\"\\nðŸ“Š Dataset info:\")\n",
    "    print(f\"  Keys: {list(f.keys())}\")\n",
    "    if 'Profiling_traces' in f:\n",
    "        print(f\"  Profiling traces shape: {f['Profiling_traces/traces'].shape}\")\n",
    "    if 'Attack_traces' in f:\n",
    "        print(f\"  Attack traces shape: {f['Attack_traces/traces'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "if os.path.exists('data/ASCAD.h5'):\n",
    "    with h5py.File('data/ASCAD.h5', 'r') as f:\n",
    "        print(\"âœ“ ASCAD.h5 found!\")\n",
    "        print(\"  Available keys:\", list(f.keys()))\n",
    "        print(\"  Profiling traces shape:\", f['Profiling_traces']['traces'].shape)\n",
    "        print(\"  Attack traces shape:\", f['Attack_traces']['traces'].shape)\n",
    "else:\n",
    "    print(\"âœ— ASCAD.h5 not found. Please upload the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Training\n",
    "\n",
    "Edit these settings as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_path': 'data/ASCAD.h5',\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/estranet_checkpoints',  # Save to Google Drive\n",
    "    'result_path': 'results',\n",
    "    \n",
    "    # Training settings (adjusted for Colab)\n",
    "    'train_steps': 50000,        # Reduced from 4M for faster training\n",
    "    'warmup_steps': 5000,        # Reduced from 1M\n",
    "    'save_steps': 5000,          # Save every 5k steps\n",
    "    'iterations': 1000,          # Log every 1k steps\n",
    "    'train_batch_size': 32,      # Adjust based on GPU memory\n",
    "    'eval_batch_size': 32,\n",
    "    \n",
    "    # Data settings\n",
    "    'input_length': 10000,       # Use 10000 for faster training\n",
    "    'data_desync': 200,          # Data augmentation\n",
    "    \n",
    "    # Model architecture\n",
    "    'n_layer': 2,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'n_head': 8,\n",
    "    'd_inner': 256,\n",
    "    'n_head_softmax': 8,\n",
    "    'd_head_softmax': 16,\n",
    "    'dropout': 0.05,\n",
    "    'n_conv_layer': 2,\n",
    "    'pool_size': 20,\n",
    "    \n",
    "    # Optimization\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'clip': 0.25,\n",
    "    'min_lr_ratio': 0.004,\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "!mkdir -p {CONFIG['checkpoint_dir']}\n",
    "!mkdir -p results\n",
    "\n",
    "print(\"Configuration ready!\")\n",
    "print(f\"Checkpoints will be saved to: {CONFIG['checkpoint_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training command\n",
    "train_cmd = f\"\"\"\n",
    "python train_trans.py \\\n",
    "    --data_path={CONFIG['data_path']} \\\n",
    "    --checkpoint_dir={CONFIG['checkpoint_dir']} \\\n",
    "    --dataset=ASCAD \\\n",
    "    --input_length={CONFIG['input_length']} \\\n",
    "    --data_desync={CONFIG['data_desync']} \\\n",
    "    --train_batch_size={CONFIG['train_batch_size']} \\\n",
    "    --eval_batch_size={CONFIG['eval_batch_size']} \\\n",
    "    --train_steps={CONFIG['train_steps']} \\\n",
    "    --warmup_steps={CONFIG['warmup_steps']} \\\n",
    "    --iterations={CONFIG['iterations']} \\\n",
    "    --save_steps={CONFIG['save_steps']} \\\n",
    "    --n_layer={CONFIG['n_layer']} \\\n",
    "    --d_model={CONFIG['d_model']} \\\n",
    "    --d_head={CONFIG['d_head']} \\\n",
    "    --n_head={CONFIG['n_head']} \\\n",
    "    --d_inner={CONFIG['d_inner']} \\\n",
    "    --n_head_softmax={CONFIG['n_head_softmax']} \\\n",
    "    --d_head_softmax={CONFIG['d_head_softmax']} \\\n",
    "    --dropout={CONFIG['dropout']} \\\n",
    "    --conv_kernel_size=3 \\\n",
    "    --n_conv_layer={CONFIG['n_conv_layer']} \\\n",
    "    --pool_size={CONFIG['pool_size']} \\\n",
    "    --d_kernel_map=512 \\\n",
    "    --beta_hat_2=150 \\\n",
    "    --model_normalization=preLC \\\n",
    "    --head_initialization=forward \\\n",
    "    --softmax_attn=True \\\n",
    "    --learning_rate={CONFIG['learning_rate']} \\\n",
    "    --clip={CONFIG['clip']} \\\n",
    "    --min_lr_ratio={CONFIG['min_lr_ratio']} \\\n",
    "    --max_eval_batch=100 \\\n",
    "    --do_train=True\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "!{train_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitor Training (Optional)\n",
    "\n",
    "Run this in a separate cell while training is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View training progress\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_file = f\"{CONFIG['checkpoint_dir']}/loss.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(loss_file, 'rb') as f:\n",
    "        loss_dict = pickle.load(f)\n",
    "    \n",
    "    steps = sorted(loss_dict.keys())\n",
    "    train_losses = [loss_dict[s]['train_loss'] for s in steps]\n",
    "    test_losses = [loss_dict[s]['test_loss'] for s in steps]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(steps, train_losses, label='Train Loss')\n",
    "    plt.plot(steps, test_losses, label='Test Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Progress')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Latest step: {steps[-1]}\")\n",
    "    print(f\"Train loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Test loss: {test_losses[-1]:.4f}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Loss file not found yet. Training hasn't started saving checkpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation to get key rank\n",
    "eval_cmd = f\"\"\"\n",
    "python train_trans.py \\\n",
    "    --data_path={CONFIG['data_path']} \\\n",
    "    --checkpoint_dir={CONFIG['checkpoint_dir']} \\\n",
    "    --dataset=ASCAD \\\n",
    "    --input_length={CONFIG['input_length']} \\\n",
    "    --eval_batch_size={CONFIG['eval_batch_size']} \\\n",
    "    --n_layer={CONFIG['n_layer']} \\\n",
    "    --d_model={CONFIG['d_model']} \\\n",
    "    --d_head={CONFIG['d_head']} \\\n",
    "    --n_head={CONFIG['n_head']} \\\n",
    "    --d_inner={CONFIG['d_inner']} \\\n",
    "    --n_head_softmax={CONFIG['n_head_softmax']} \\\n",
    "    --d_head_softmax={CONFIG['d_head_softmax']} \\\n",
    "    --dropout={CONFIG['dropout']} \\\n",
    "    --n_conv_layer={CONFIG['n_conv_layer']} \\\n",
    "    --pool_size={CONFIG['pool_size']} \\\n",
    "    --model_normalization=preLC \\\n",
    "    --result_path={CONFIG['result_path']}/eval_results \\\n",
    "    --do_train=False\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "!{eval_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot key rank results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_file = f\"{CONFIG['result_path']}/eval_results.txt\"\n",
    "\n",
    "try:\n",
    "    with open(results_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Last line contains mean ranks\n",
    "    mean_ranks = np.array([float(x) for x in lines[-1].strip().split()])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(mean_ranks)\n",
    "    plt.xlabel('Number of Traces')\n",
    "    plt.ylabel('Key Rank')\n",
    "    plt.title('Key Recovery Performance')\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find how many traces needed for rank 0\n",
    "    rank_0_idx = np.where(mean_ranks == 0)[0]\n",
    "    if len(rank_0_idx) > 0:\n",
    "        print(f\"âœ“ Key recovered with {rank_0_idx[0]} traces!\")\n",
    "    else:\n",
    "        print(f\"Key not fully recovered. Best rank: {int(mean_ranks[-1])}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Results file not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "1. **Training Time**: With the default config (50k steps), training takes ~2-4 hours on Colab GPU\n",
    "2. **Checkpoints**: Saved to Google Drive, so they persist across sessions\n",
    "3. **Resume Training**: Set `--warm_start=True` to resume from last checkpoint\n",
    "4. **Experiment**: Try different `input_length` (700, 10000, 40000) and model sizes\n",
    "5. **Memory Issues**: Reduce `train_batch_size` if you get OOM errors"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

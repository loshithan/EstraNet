{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16637c1e",
   "metadata": {},
   "source": [
    "# Mamba-GNN — Cross‑Entropy (Phase 1)\n",
    "# EstraNet‑aligned Cross‑Entropy training, evaluation (100‑trial GE), plotting, and export.\n",
    "\n",
    "\"\"\"\n",
    "Purpose: Phase‑1 (fair comparison) notebook using Cross‑Entropy loss.\n",
    "- EstraNet‑aligned defaults\n",
    "- Training cell (safe-run toggle)\n",
    "- 100‑trial Guessing Entropy evaluation\n",
    "- Plots + export utilities\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 — Clone repository & workspace cleanup\n",
    "import os, subprocess, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "FORCE_RECLONE = False\n",
    "REPO_URL = 'https://github.com/loshithan/EstraNet.git'\n",
    "ROOT = Path.cwd()\n",
    "print(f\"Workspace cwd: {ROOT}\")\n",
    "\n",
    "# Safe behavior: only remove/re-clone if FORCE_RECLONE=True\n",
    "if FORCE_RECLONE:\n",
    "    if Path('EstraNet').exists():\n",
    "        print('Removing existing EstraNet folder (FORCE_RECLONE=True)')\n",
    "        shutil.rmtree('EstraNet')\n",
    "\n",
    "if not Path('EstraNet').exists():\n",
    "    print('Cloning EstraNet repository...')\n",
    "    subprocess.run(['git', 'clone', REPO_URL], check=True)\n",
    "else:\n",
    "    print('EstraNet already present — skipping clone')\n",
    "\n",
    "# If cloned, change into the repo directory\n",
    "if Path('EstraNet').exists():\n",
    "    os.chdir('EstraNet')\n",
    "    print(f\"Changed cwd -> {os.getcwd()}\")\n",
    "else:\n",
    "    print('Warning: EstraNet folder not found in workspace — ensure you are in the project root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c480e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 — Setup environment, packages and project paths\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verify core packages and versions (do not auto-install here)\n",
    "import torch\n",
    "print(f\"python: {sys.version.split()[0]}, torch: {torch.__version__}\")\n",
    "\n",
    "# Ensure project script/model folders are on sys.path\n",
    "ROOT = Path.cwd()\n",
    "sys.path.append(str(ROOT / 'mamba-gnn-scripts'))\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "# Create folders used by the notebook\n",
    "os.makedirs('checkpoints/mamba_gnn_estranet', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "print('\\n✓ Environment folders created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 — Verify GPU availability and device config\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a1a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 — Define EstraNet‑aligned training configuration\n",
    "config = {\n",
    "    'data_path': 'data/ASCAD.h5',\n",
    "    'checkpoint_dir': 'checkpoints/mamba_gnn_estranet',\n",
    "    'target_byte': 2,\n",
    "    'train_batch_size': 256,\n",
    "    'eval_batch_size': 32,\n",
    "    'train_steps': 50000,\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'd_model': 64,\n",
    "    'mamba_layers': 2,\n",
    "    'gnn_layers': 2,\n",
    "    'k_neighbors': 8,\n",
    "    'eval_steps': 250,\n",
    "    'save_steps': 5000,\n",
    "    'warmup_steps': 1000,\n",
    "    'dropout': 0.2,\n",
    "    'weight_decay': 0.01,\n",
    "    'label_smoothing': 0.02,\n",
    "    'augment_noise': 0.0,\n",
    "    'augment_shift': 0,\n",
    "    'early_stopping': 30,\n",
    "    'loss_function': 'cross_entropy',\n",
    "    # focal params (kept for Phase‑2 but unused here)\n",
    "    'focal_gamma': 2.5,\n",
    "    'focal_alpha': 1.0,\n",
    "}\n",
    "\n",
    "print('EstraNet-aligned config (Phase 1 - CrossEntropy):')\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k:20s}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804102e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5 — Configuration comparison (wrong vs correct)\n",
    "print('--- Incorrect (example) ---')\n",
    "print('loss=FocalLoss, lr=2e-3, batch_size=64, single-trial eval (WRONG)')\n",
    "\n",
    "print('\\n--- Correct (EstraNet‑aligned) ---')\n",
    "for k in ['loss_function','learning_rate','train_batch_size','eval_batch_size','train_steps','d_model']:\n",
    "    print(f\"{k:20s}: {config.get(k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b79eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6 — Toggle loss function (Phase 1 vs Phase 2)\n",
    "# Default is Phase 1: Cross-Entropy (safe/fair comparison)\n",
    "LOSS_FUNCTION = 'cross_entropy'  # change to 'focal_loss' to prepare Phase 2 cells\n",
    "\n",
    "if LOSS_FUNCTION in ('focal', 'focal_loss'):\n",
    "    config['loss_function'] = 'focal'\n",
    "    config['label_smoothing'] = 0.0\n",
    "    print('Switched to FocalLoss mode (label_smoothing disabled)')\n",
    "else:\n",
    "    config['loss_function'] = 'cross_entropy'\n",
    "    print('Using Cross-Entropy (Phase 1)')\n",
    "\n",
    "print('loss_function ->', config['loss_function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1477be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7 — Build CLI args and checkpoint/result directories\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure checkpoint dir matches phase\n",
    "PHASE1_CKPT = Path('checkpoints/mamba_gnn_estranet')\n",
    "PHASE2_CKPT = Path('checkpoints/mamba_gnn_phase2_focal')\n",
    "PHASE1_CKPT.mkdir(parents=True, exist_ok=True)\n",
    "PHASE2_CKPT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config['checkpoint_dir'] = str(PHASE1_CKPT) if config['loss_function']=='cross_entropy' else str(PHASE2_CKPT)\n",
    "\n",
    "# Build CLI argument string from config (selective)\n",
    "def build_train_cmd(cfg):\n",
    "    parts = ['python', 'mamba-gnn-scripts/train_mamba_gnn.py', '--do_train']\n",
    "    parts.append(f\"--loss_type={cfg['loss_function']}\")\n",
    "    for key in ['data_path','checkpoint_dir','train_steps','train_batch_size','eval_batch_size','learning_rate','d_model','mamba_layers','gnn_layers','k_neighbors','dropout','weight_decay','label_smoothing','save_steps','eval_steps']:\n",
    "        val = cfg.get(key)\n",
    "        if val is not None:\n",
    "            parts.append(f\"--{key}={val}\")\n",
    "    return ' '.join(parts)\n",
    "\n",
    "train_cmd_preview = build_train_cmd(config)\n",
    "print('Training command preview:')\n",
    "print(train_cmd_preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ecec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8 — Run training with live logging and checkpoint monitor\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_TRAIN = False  # set True to actually start training from this cell\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "if RUN_TRAIN:\n",
    "    cmd = build_train_cmd(config)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = LOG_DIR / f\"train_ce_{timestamp}.log\"\n",
    "    print('Starting training — logging to', log_file)\n",
    "\n",
    "    # unbuffered python to stream logs\n",
    "    process = subprocess.Popen(cmd.replace('python ', 'python -u '), shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n",
    "    with open(log_file, 'w') as f:\n",
    "        while True:\n",
    "            line = process.stdout.readline()\n",
    "            if line:\n",
    "                print(line, end='')\n",
    "                f.write(line)\n",
    "                f.flush()\n",
    "            if process.poll() is not None:\n",
    "                break\n",
    "    print('Training finished with returncode', process.returncode)\n",
    "else:\n",
    "    print('RUN_TRAIN is False — set to True to start training from this cell.')\n",
    "\n",
    "# Quick monitor for newly saved checkpoints (one-shot)\n",
    "ckpt_dir = Path(config['checkpoint_dir'])\n",
    "ckpts = sorted(ckpt_dir.glob('*.pth'), key=lambda p: p.stat().st_mtime)\n",
    "if ckpts:\n",
    "    latest = ckpts[-1]\n",
    "    print('Latest checkpoint:', latest.name, ' — size:', latest.stat().st_size/(1024*1024), 'MB')\n",
    "else:\n",
    "    print('No checkpoints found yet in', ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9 — Patch training script to fix KeyError in `loss_history`\n",
    "script_path = Path('mamba-gnn-scripts') / 'train_mamba_gnn.py'\n",
    "pattern = 'loss_history[global_step].update({'\n",
    "replacement = 'loss_history.setdefault(global_step, {}).update({'\n",
    "\n",
    "if script_path.exists():\n",
    "    text = script_path.read_text()\n",
    "    if pattern in text:\n",
    "        text = text.replace(pattern, replacement)\n",
    "        script_path.write_text(text)\n",
    "        print('Patched train_mamba_gnn.py — replaced loss_history update with setdefault variant')\n",
    "    else:\n",
    "        print('Pattern not found — file may already be patched or is different')\n",
    "else:\n",
    "    print('train_mamba_gnn.py not found at', script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657094ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10 — Interactive checkpoint directory monitor (run in separate cell/session)\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "watch_dir = Path(config['checkpoint_dir'])\n",
    "seen = set()\n",
    "print(f\"Watching checkpoints in: {watch_dir} (Ctrl+C to stop)\")\n",
    "try:\n",
    "    while False:  # change to True when you actually want continuous monitoring\n",
    "        files = sorted(watch_dir.glob('*.pth'))\n",
    "        new = [f for f in files if f.name not in seen]\n",
    "        for f in new:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] New checkpoint: {f.name} ({f.stat().st_size/1e6:.1f} MB)\")\n",
    "            seen.add(f.name)\n",
    "        time.sleep(5)\n",
    "except KeyboardInterrupt:\n",
    "    print('Monitor stopped')\n",
    "\n",
    "print('Note: set the while-loop condition to True to enable live polling in this cell (not recommended inside long-running notebooks).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11 — Optional: TensorFlow training + TFLite export (template)\n",
    "# Template only — uncomment to run TensorFlow training script (if available)\n",
    "# tf_cmd = (\n",
    "#     \"python scripts/train_mamba_gnn_tf.py \"\n",
    "#     f\"--data_path={config['data_path']} \"\n",
    "#     \"--checkpoint_dir=checkpoints/mamba_gnn_tf \"\n",
    "#     f\"--train_batch_size={config['train_batch_size']} \"\n",
    "#     f\"--eval_batch_size={config['eval_batch_size']} \"\n",
    "#     \"--train_steps=50000 --do_train\"\n",
    "# )\n",
    "# print('TF command (template):')\n",
    "# print(tf_cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b97e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12 — Plot training metrics (loss, LR, gradient norms)\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_path = Path(config['checkpoint_dir']) / 'loss.pkl'\n",
    "if loss_path.exists():\n",
    "    with open(loss_path, 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "    steps = sorted(history.keys())\n",
    "    train_losses = [history[s].get('train_loss', None) for s in steps]\n",
    "    lrs = [history[s].get('lr', None) for s in steps]\n",
    "    grad_norms = [history[s].get('grad_norm', None) for s in steps]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    axes[0].plot(steps, train_losses, label='train_loss'); axes[0].set_title('Train Loss')\n",
    "    axes[1].plot(steps, lrs, label='lr'); axes[1].set_title('LR')\n",
    "    axes[2].plot(steps, grad_norms, label='grad_norm'); axes[2].set_title('Grad Norm')\n",
    "    plt.tight_layout()\n",
    "    out = Path('results') / 'training_progress.png'\n",
    "    plt.savefig(out, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved:', out)\n",
    "else:\n",
    "    print('No training history found at', loss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a90c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13 — Evaluate model with 100-trial Guessing Entropy\n",
    "# By default this will evaluate the latest checkpoint (checkpoint_idx=0)\n",
    "RUN_EVAL = False  # set True to run evaluation from this cell\n",
    "checkpoint_idx = 0  # 0 -> latest\n",
    "result_path = Path('results') / 'cross_entropy_eval.txt'\n",
    "\n",
    "eval_cmd = (\n",
    "    f\"python mamba-gnn-scripts/train_mamba_gnn.py \"\n",
    "    f\"--data_path={config['data_path']} \"\n",
    "    f\"--checkpoint_dir={config['checkpoint_dir']} \"\n",
    "    f\"--d_model={config['d_model']} \"\n",
    "    f\"--mamba_layers={config['mamba_layers']} \"\n",
    "    f\"--gnn_layers={config['gnn_layers']} \"\n",
    "    f\"--k_neighbors={config['k_neighbors']} \"\n",
    "    f\"--dropout={config['dropout']} \"\n",
    "    f\"--checkpoint_idx={checkpoint_idx} \"\n",
    "    f\"--result_path={result_path}\"\n",
    ")\n",
    "\n",
    "print('Eval command (preview):')\n",
    "print(eval_cmd)\n",
    "\n",
    "if RUN_EVAL:\n",
    "    import subprocess\n",
    "    rc = subprocess.run(eval_cmd.split()).returncode\n",
    "    print('Eval returncode =', rc)\n",
    "else:\n",
    "    print('RUN_EVAL is False — set to True to run the 100-trial GE from this cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 14 — Plot Guessing Entropy curve and save figure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "result_file = Path('results') / 'cross_entropy_eval.txt'\n",
    "if result_file.exists():\n",
    "    with open(result_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        mean_ranks = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
    "        std_ranks = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
    "\n",
    "    traces = np.arange(1, len(mean_ranks) + 1)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(traces, mean_ranks, label='Mean GE', lw=2)\n",
    "    plt.fill_between(traces, mean_ranks-std_ranks, mean_ranks+std_ranks, alpha=0.25)\n",
    "    plt.axhline(0, color='red', linestyle='--', label='Key recovered')\n",
    "    plt.xlabel('Number of traces')\n",
    "    plt.ylabel('Key rank (GE)')\n",
    "    plt.title('Guessing Entropy — Cross‑Entropy (Phase 1)')\n",
    "    plt.legend()\n",
    "    out = Path('results') / 'guessing_entropy_cross_entropy.png'\n",
    "    plt.savefig(out, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved GE plot to', out)\n",
    "else:\n",
    "    print('No GE results found at', result_file, '\\nRun evaluation first (Section 13)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a55998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 15 — Compare Mamba-GNN results with EstraNet\n",
    "estranet_result = Path('results') / 'trans_long-11.txt'  # update if needed\n",
    "mamba_result = Path('results') / 'cross_entropy_eval.txt'\n",
    "\n",
    "compare_cmd = f\"python scripts/compare_results.py --mamba_results={mamba_result} --estranet_results={estranet_result} --output=results/comparison_ce_vs_estranet.png\"\n",
    "print('Preview compare command:')\n",
    "print(compare_cmd)\n",
    "\n",
    "# To run uncomment the next lines\n",
    "# import subprocess\n",
    "# subprocess.run(compare_cmd.split())\n",
    "# print('Comparison plot saved to results/comparison_ce_vs_estranet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 16 — Load model for inference (patch imports if needed)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Patch relative imports in model file if they cause import errors\n",
    "model_file = Path('models') / 'mamba_gnn_model.py'\n",
    "if model_file.exists():\n",
    "    text = model_file.read_text()\n",
    "    if 'from .mamba_block' in text:\n",
    "        text = text.replace('from .mamba_block import OptimizedMambaBlock', 'from models.mamba_block import OptimizedMambaBlock')\n",
    "        text = text.replace('from .gat_layer import EnhancedGAT', 'from models.gat_layer import EnhancedGAT')\n",
    "        text = text.replace('from .patch_embedding import CNNPatchEmbedding', 'from models.patch_embedding import CNNPatchEmbedding')\n",
    "        model_file.write_text(text)\n",
    "        print('Patched relative imports in models/mamba_gnn_model.py')\n",
    "\n",
    "# Import model class\n",
    "sys.path.append(str(Path.cwd()))\n",
    "from models.mamba_gnn_model import OptimizedMambaGNN\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = OptimizedMambaGNN(trace_length=700, d_model=config['d_model'], mamba_layers=config['mamba_layers'], gnn_layers=config['gnn_layers'], num_classes=256, k_neighbors=config['k_neighbors'], dropout=config['dropout']).to(device)\n",
    "\n",
    "# Load latest checkpoint (if available)\n",
    "ckpt_dir = Path(config['checkpoint_dir'])\n",
    "ckpts = sorted(ckpt_dir.glob('*.pth'), key=lambda p: p.stat().st_mtime)\n",
    "if ckpts:\n",
    "    ckpt = torch.load(ckpts[-1], map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model.eval()\n",
    "    print('Loaded model from', ckpts[-1].name)\n",
    "else:\n",
    "    print('No checkpoint found to load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 17 — Run inference on sample traces\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "data_path = Path(config['data_path'])\n",
    "if data_path.exists() and 'model' in globals() and hasattr(model, 'eval'):\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        X_attack = f['Attack_traces/traces'][:100]\n",
    "        X_train_sample = f['Profiling_traces/traces'][:1000]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_sample)\n",
    "    X_attack_norm = scaler.transform(X_attack)\n",
    "\n",
    "    X_tensor = torch.FloatTensor(X_attack_norm).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    print('Processed', len(X_attack), 'attack traces')\n",
    "    print('Top predictions (first 10):', preds[:10])\n",
    "    print('Confidence (first trace):', probs[0].max())\n",
    "else:\n",
    "    print('Data or model not available — ensure checkpoint is loaded and ASCAD dataset exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae270023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 18 — Export / download checkpoints\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "ckpt_folder = Path(config['checkpoint_dir'])\n",
    "zip_name = Path.cwd() / 'mamba_gnn_ce_checkpoints'\n",
    "if ckpt_folder.exists():\n",
    "    shutil.make_archive(str(zip_name), 'zip', root_dir=str(ckpt_folder))\n",
    "    print('Created:', str(zip_name)+'.zip')\n",
    "    print('Path:', (str(zip_name)+'.zip'))\n",
    "else:\n",
    "    print('Checkpoint folder not found:', ckpt_folder)\n",
    "\n",
    "# Note: in Colab use google.colab.files.download(zip_path) to download the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ac739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 19 — Phase comparison (Phase 1 vs Phase 2)\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_eval(path):\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        with open(p,'r') as f:\n",
    "            lines = f.readlines()\n",
    "            mean = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
    "            std = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
    "            return mean, std\n",
    "    return None, None\n",
    "\n",
    "ce_mean, ce_std = load_eval('results/cross_entropy_eval.txt')\n",
    "f_mean, f_std = load_eval('results/focal_loss_eval.txt')\n",
    "\n",
    "if ce_mean is None:\n",
    "    print('Cross-entropy eval missing — run Section 13')\n",
    "else:\n",
    "    print('Loaded Cross-Entropy eval — length', len(ce_mean))\n",
    "\n",
    "if f_mean is None:\n",
    "    print('FocalLoss eval missing — run Phase 2 experiments to populate results/focal_loss_eval.txt')\n",
    "\n",
    "# If both present, plot comparison\n",
    "if ce_mean is not None and f_mean is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    L = min(len(ce_mean), len(f_mean))\n",
    "    traces = np.arange(1, L+1)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(traces, ce_mean[:L], label='Cross-Entropy (Phase 1)')\n",
    "    plt.plot(traces, f_mean[:L], label='FocalLoss (Phase 2)')\n",
    "    plt.xlabel('Traces')\n",
    "    plt.ylabel('Key rank (GE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results/phase_comparison_ce_vs_focal.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Recovery points\n",
    "    def recovery_point(mean):\n",
    "        idx = np.where(mean==0)[0]\n",
    "        return idx[0]+1 if len(idx)>0 else None\n",
    "\n",
    "    print('Phase 1 recovery:', recovery_point(ce_mean))\n",
    "    print('Phase 2 recovery:', recovery_point(f_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc70da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 20 — Utilities: helpers for history / quick checks\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_loss_history(ckpt_dir=config['checkpoint_dir']):\n",
    "    p = Path(ckpt_dir) / 'loss.pkl'\n",
    "    if p.exists():\n",
    "        with open(p,'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def find_checkpoint_by_step(ckpt_dir=config['checkpoint_dir'], step=None):\n",
    "    files = sorted(Path(ckpt_dir).glob('*.pth'), key=lambda p: p.stat().st_mtime)\n",
    "    if not files:\n",
    "        return None\n",
    "    if step is None:\n",
    "        return files[-1]\n",
    "    for f in files:\n",
    "        if str(step) in f.name:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "print('Utilities loaded: load_loss_history, find_checkpoint_by_step')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

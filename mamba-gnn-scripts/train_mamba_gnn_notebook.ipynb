{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell_0000",
      "metadata": {
        "id": "cell_0000"
      },
      "source": [
        "# Mamba-GNN Training Notebook (EstraNet-Aligned Configuration)\n",
        "\n",
        "**âš ï¸ IMPORTANT**: This notebook uses the **corrected configuration** that matches EstraNet for fair comparison.\n",
        "\n",
        "## Key Configuration âœ…\n",
        "- **Loss Function**: Cross-Entropy (not Focal Loss)\n",
        "- **Learning Rate**: 2.5e-4 (not 2e-3)\n",
        "- **Batch Size**: 256 train / 32 eval (not 64)\n",
        "- **Training**: 100k steps (not 50-100 epochs)\n",
        "- **Optimizer**: Adam (not AdamW)\n",
        "- **Scheduler**: Cosine Decay (not OneCycleLR)\n",
        "- **Evaluation**: 100-trial Guessing Entropy (not single trial)\n",
        "\n",
        "## DO NOT USE `final_best_gnn_mamba_teacher.ipynb` âŒ\n",
        "That notebook has incorrect configuration and cannot be fairly compared with EstraNet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "267e0826",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "267e0826",
        "outputId": "9c638c71-b76b-461b-8872-a9e8f7748003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EstraNet'...\n",
            "remote: Enumerating objects: 352, done.\u001b[K\n",
            "remote: Counting objects: 100% (352/352), done.\u001b[K\n",
            "remote: Compressing objects: 100% (242/242), done.\u001b[K\n",
            "remote: Total 352 (delta 159), reused 301 (delta 108), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (352/352), 32.96 MiB | 25.08 MiB/s, done.\n",
            "Resolving deltas: 100% (159/159), done.\n",
            "Updating files: 100% (139/139), done.\n",
            "âœ… Clean! Directory: /content/EstraNet\n"
          ]
        }
      ],
      "source": [
        "import os, shutil\n",
        "\n",
        "os.chdir('/content')\n",
        "if os.path.exists('EstraNet'):\n",
        "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
        "\n",
        "!git clone https://github.com/loshithan/EstraNet.git\n",
        "os.chdir('EstraNet')\n",
        "print(f\"âœ… Clean! Directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0001",
      "metadata": {
        "id": "cell_0001"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell_0002",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_0002",
        "outputId": "dfba8b03-60ee-4577-801f-92efa03adb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "\n",
            "âœ“ Environment ready\n",
            "  Data path: data/ASCAD.h5\n",
            "  Checkpoint dir: checkpoints/mamba_gnn_estranet\n",
            "  Result dir: results\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Add paths\n",
        "sys.path.append(str(Path.cwd() / 'mamba-gnn-scripts'))\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = 'data/ASCAD.h5'\n",
        "CHECKPOINT_DIR = 'checkpoints/mamba_gnn_estranet'\n",
        "RESULT_DIR = 'results'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\nâœ“ Environment ready\")\n",
        "print(f\"  Data path: {DATA_PATH}\")\n",
        "print(f\"  Checkpoint dir: {CHECKPOINT_DIR}\")\n",
        "print(f\"  Result dir: {RESULT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0003",
      "metadata": {
        "id": "cell_0003"
      },
      "source": [
        "## Configuration Comparison\n",
        "\n",
        "### âŒ Old Notebook (final_best_gnn_mamba_teacher.ipynb)\n",
        "```python\n",
        "# WRONG CONFIGURATION - DO NOT USE\n",
        "loss = FocalLoss(gamma=2.5)\n",
        "optimizer = AdamW(lr=2e-3)\n",
        "batch_size = 64\n",
        "epochs = 50-100\n",
        "scheduler = OneCycleLR(max_lr=2e-3)\n",
        "evaluation = single_trial_key_rank()\n",
        "```\n",
        "\n",
        "### âœ… New Configuration (This Notebook)\n",
        "```python\n",
        "# CORRECT CONFIGURATION - Matches EstraNet\n",
        "loss = CrossEntropyLoss()\n",
        "optimizer = Adam(lr=2.5e-4)\n",
        "batch_size = 256 (train), 32 (eval)\n",
        "train_steps = 100000\n",
        "scheduler = CosineLRSchedule(max_lr=2.5e-4)\n",
        "evaluation = compute_ge_key_rank(num_trials=100)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0004",
      "metadata": {
        "id": "cell_0004"
      },
      "source": [
        "## Option 1: Train Using PyTorch Script (Recommended)\n",
        "\n",
        "This uses the corrected `train_mamba_gnn.py` script with EstraNet-aligned configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e13c143",
      "metadata": {
        "id": "4e13c143"
      },
      "source": [
        "## Choose Loss Function: Fair Comparison vs Performance\n",
        "\n",
        "**Two-Phase Experiment Strategy for Your Report:**\n",
        "\n",
        "### Phase 1ï¸âƒ£ (FIRST): Fair Comparison with Cross-Entropy\n",
        "- **Purpose**: Compare Mamba-GNN vs EstraNet objectively (same configuration)\n",
        "- **Loss**: Cross-Entropy\n",
        "- **Results**: Document baseline performance\n",
        "- **Section in report**: \"Fair Comparison Analysis\"\n",
        "\n",
        "### Phase 2ï¸âƒ£ (THEN): Best Performance with FocalLoss\n",
        "- **Purpose**: Show Mamba-GNN's maximum attack potential\n",
        "- **Loss**: FocalLoss (gamma=2.5)\n",
        "- **Results**: Document optimized performance\n",
        "- **Section in report**: \"Performance Analysis\" or \"Best-Case Scenario\"\n",
        "\n",
        "Toggle loss function below â¬‡ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1c1df9b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1df9b8",
        "outputId": "8ea1109c-6124-47b8-cf94-78341acb8956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PHASE 1ï¸âƒ£: FAIR COMPARISON MODE (WITH REGULARIZATION)\n",
            "================================================================================\n",
            "\n",
            "âœ“ Loss Function: Cross-Entropy (with label smoothing 0.1)\n",
            "âœ“ Regularization: weight_decay=0.01, dropout=0.3\n",
            "âœ“ Data Augmentation: noise=0.05, shift=3\n",
            "âœ“ Model Capacity: REDUCED (d_model=64, mamba=2, gnn=2)\n",
            "âœ“ Early Stopping: After 40 eval periods (~10k steps) without improvement\n",
            "\n",
            "âš ï¸ Previous training showed SEVERE OVERFITTING:\n",
            "   Train loss â†’ 0, Eval loss â†’ 22 (massive gap)\n",
            "   These changes should prevent that.\n",
            "\n",
            "âœ“ Configuration:\n",
            "  data_path           : data/ASCAD.h5\n",
            "  checkpoint_dir      : checkpoints/mamba_gnn_estranet\n",
            "  target_byte         : 2\n",
            "  train_batch_size    : 256\n",
            "  eval_batch_size     : 32\n",
            "  train_steps         : 50000\n",
            "  learning_rate       : 0.0001\n",
            "  d_model             : 64\n",
            "  mamba_layers        : 2\n",
            "  gnn_layers          : 2\n",
            "  k_neighbors         : 8\n",
            "  eval_steps          : 250\n",
            "  save_steps          : 5000\n",
            "  warmup_steps        : 1000\n",
            "  loss_function       : cross_entropy\n",
            "  clip                : 1.0\n",
            "  dropout             : 0.25\n",
            "  weight_decay        : 0.01\n",
            "  label_smoothing     : 0.1\n",
            "  augment_noise       : 0.03\n",
            "  augment_shift       : 2\n",
            "  early_stopping      : 20\n",
            "  focal_gamma         : 2.5\n",
            "  focal_alpha         : 1.0\n"
          ]
        }
      ],
      "source": [
        "# ========== SWITCH LOSS FUNCTION HERE ==========\n",
        "# Choose one: \"cross_entropy\" or \"focal_loss\"\n",
        "LOSS_FUNCTION = \"cross_entropy\"  # â† Default set to \"focal_loss\" for Phase 2 (performance)\n",
        "# ===============================================\n",
        "\n",
        "# Training configuration (EstraNet-aligned + Anti-Overfitting Regularization)\n",
        "config = {\n",
        "    'data_path': DATA_PATH,\n",
        "    'checkpoint_dir': CHECKPOINT_DIR,\n",
        "    'target_byte': 2,\n",
        "    'train_batch_size': 256,\n",
        "    'eval_batch_size': 32,\n",
        "    'train_steps': 50000,\n",
        "    'learning_rate': 0.00015,\n",
        "    'd_model': 64,\n",
        "    'mamba_layers': 2,\n",
        "    'gnn_layers': 2,\n",
        "    'k_neighbors': 8,\n",
        "    'eval_steps': 250,\n",
        "    'save_steps': 5000,\n",
        "    'warmup_steps': 1000,\n",
        "    'loss_function': LOSS_FUNCTION,\n",
        "\n",
        "\n",
        "    # --- fixed ---\n",
        "    'clip': 1.0,              # was 0.25\n",
        "    'dropout': 0.2,          # reduce from 0.3\n",
        "    'weight_decay': 0.01,     # keep this\n",
        "    'label_smoothing': 0.02,   # disable for now\n",
        "    'augment_noise': 0.03,     # disable for now\n",
        "    'augment_shift': 2,       # disable for now\n",
        "    'early_stopping': 30,     # keep this\n",
        "\n",
        "    # FocalLoss params (used only when loss_function == 'focal_loss')\n",
        "    'focal_gamma': 2.5,\n",
        "    'focal_alpha': 1.0,\n",
        "}\n",
        "\n",
        "# If using FocalLoss, disable label smoothing (they conflict)\n",
        "if LOSS_FUNCTION in (\"focal\", \"focal_loss\"):\n",
        "    config['label_smoothing'] = 0.0\n",
        "\n",
        "# Loss function specific settings\n",
        "if LOSS_FUNCTION == \"cross_entropy\":\n",
        "    config['loss_config'] = {\n",
        "        'type': 'CrossEntropyLoss',\n",
        "        'description': 'Fair comparison with EstraNet',\n",
        "    }\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 1ï¸âƒ£: FAIR COMPARISON MODE (WITH REGULARIZATION)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nâœ“ Loss Function: Cross-Entropy (with label smoothing 0.1)\")\n",
        "    print(\"âœ“ Regularization: weight_decay=0.01, dropout=0.3\")\n",
        "    print(\"âœ“ Data Augmentation: noise=0.05, shift=3\")\n",
        "    print(\"âœ“ Model Capacity: REDUCED (d_model=64, mamba=2, gnn=2)\")\n",
        "    print(\"âœ“ Early Stopping: After 40 eval periods (~10k steps) without improvement\")\n",
        "    print(\"\\nâš ï¸ Previous training showed SEVERE OVERFITTING:\")\n",
        "    print(\"   Train loss â†’ 0, Eval loss â†’ 22 (massive gap)\")\n",
        "    print(\"   These changes should prevent that.\")\n",
        "\n",
        "else:\n",
        "    config['loss_config'] = {\n",
        "        'type': 'FocalLoss',\n",
        "        'gamma': config['focal_gamma'],\n",
        "        'alpha': config['focal_alpha'],\n",
        "        'description': 'Optimized attack performance (Phase 2)'\n",
        "    }\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 2ï¸âƒ£: PERFORMANCE MODE (FocalLoss)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nâœ“ Loss Function: FocalLoss (gamma={:.2f}, alpha={:.2f})\".format(config['focal_gamma'], config['focal_alpha']))\n",
        "    print(\"âœ“ Label smoothing disabled for FocalLoss\")\n",
        "    print(\"âœ“ Keep other regularization (dropout/weight_decay) to avoid overfitting\")\n",
        "\n",
        "print(\"\\nâœ“ Configuration:\")\n",
        "for key, value in config.items():\n",
        "    if key != 'loss_config':\n",
        "        print(f\"  {key:20s}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716c2455",
      "metadata": {
        "id": "716c2455"
      },
      "source": [
        "## Report Structure: How to Document Results\n",
        "\n",
        "### Section 1: Fair Comparison Analysis (Phase 1ï¸âƒ£)\n",
        "```markdown\n",
        "## Fair Comparison: Mamba-GNN vs EstraNet\n",
        "\n",
        "### Configuration Match\n",
        "- Loss: Cross-Entropy (same as EstraNet) âœ“\n",
        "- Learning Rate: 2.5e-4 (same as EstraNet) âœ“\n",
        "- Batch Size: 256 (same as EstraNet) âœ“\n",
        "- Optimizer: Adam with Cosine schedule âœ“\n",
        "- Evaluation: 100-trial Guessing Entropy âœ“\n",
        "\n",
        "### Results\n",
        "| Model | Key Rank @ 100 traces | Key Rank @ 1000 traces | Recovery Point |\n",
        "|-------|----------------------|----------------------|--------|\n",
        "| EstraNet Transformer | X | Y | Z traces |\n",
        "| Mamba-GNN | X | Y | Z traces |\n",
        "\n",
        "### Conclusion\n",
        "Fair comparison shows [which model is better/comparable]\n",
        "```\n",
        "\n",
        "### Section 2: Performance Analysis (Phase 2ï¸âƒ£)\n",
        "```markdown\n",
        "## Performance Optimization: Mamba-GNN with FocalLoss\n",
        "\n",
        "### Configuration\n",
        "- Loss: FocalLoss (gamma=2.5) - optimized for attack\n",
        "- Same training setup otherwise âœ“\n",
        "\n",
        "### Results\n",
        "| Model | Loss | Key Rank @ 100 traces | Key Rank @ 1000 traces |\n",
        "|-------|------|----------------------|----------------------|\n",
        "| Mamba-GNN (Cross-Entropy) | Baseline | X | Y |\n",
        "| Mamba-GNN (FocalLoss) | Optimized | X' | Y' |\n",
        "\n",
        "### Improvement\n",
        "FocalLoss improves performance by [X%] at key recovery\n",
        "```\n",
        "\n",
        "ðŸ‘‡ Execute cells below to gather data for both sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell_0006",
      "metadata": {
        "id": "cell_0006"
      },
      "outputs": [],
      "source": [
        "# Base args (include all regularization flags we expect)\n",
        "base_args = {\n",
        "    'data_path': config['data_path'],\n",
        "    'checkpoint_dir': config['checkpoint_dir'],\n",
        "    'target_byte': config['target_byte'],\n",
        "    'train_batch_size': config['train_batch_size'],\n",
        "    'eval_batch_size': config['eval_batch_size'],\n",
        "    'train_steps': config['train_steps'],\n",
        "    'eval_steps': config['eval_steps'],\n",
        "    'save_steps': config['save_steps'],\n",
        "    'learning_rate': config['learning_rate'],\n",
        "    'd_model': config['d_model'],\n",
        "    'mamba_layers': config['mamba_layers'],\n",
        "    'gnn_layers': config['gnn_layers'],\n",
        "    'k_neighbors': config['k_neighbors'],\n",
        "    'dropout': config['dropout'],\n",
        "    'weight_decay': config.get('weight_decay'),\n",
        "    'label_smoothing': config.get('label_smoothing'),\n",
        "    'early_stopping': config.get('early_stopping'),\n",
        "    'augment_noise': config.get('augment_noise'),\n",
        "    'augment_shift': config.get('augment_shift'),\n",
        "    'loss_type': config.get('loss_function'),        # NEW: pass loss type\n",
        "    'focal_gamma': config.get('focal_gamma'),       # NEW: focal params\n",
        "    'focal_alpha': config.get('focal_alpha'),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cdadd6e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdadd6e5",
        "outputId": "7f650162-04f1-4e05-be8d-38ffd89134a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Running: Phase 1ï¸âƒ£ (Fair Comparison)\n",
            "================================================================================\n",
            "\n",
            "âœ“ Checkpoints saved to: checkpoints/mamba_gnn_estranet\n",
            "âœ“ Results saved to: results/cross_entropy_eval.txt\n",
            "\n",
            "Switch between phases by changing: LOSS_FUNCTION = '...'\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup checkpoint directories for both phases\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create phase-specific directories\n",
        "# FIX: Use the existing directory for Phase 1 as requested\n",
        "PHASE1_CKPT = 'checkpoints/mamba_gnn_estranet'  # Cross-Entropy\n",
        "PHASE2_CKPT = 'checkpoints/mamba_gnn_phase2_focal'  # FocalLoss\n",
        "\n",
        "os.makedirs(PHASE1_CKPT, exist_ok=True)\n",
        "os.makedirs(PHASE2_CKPT, exist_ok=True)\n",
        "\n",
        "# Determine which checkpoint dir to use\n",
        "if config['loss_function'] == \"cross_entropy\":\n",
        "    ACTIVE_CKPT = PHASE1_CKPT\n",
        "    PHASE_NAME = \"Phase 1ï¸âƒ£ (Fair Comparison)\"\n",
        "else:\n",
        "    ACTIVE_CKPT = PHASE2_CKPT\n",
        "    PHASE_NAME = \"Phase 2ï¸âƒ£ (Performance)\"\n",
        "\n",
        "config['checkpoint_dir'] = ACTIVE_CKPT\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Running: {PHASE_NAME}\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\nâœ“ Checkpoints saved to: {ACTIVE_CKPT}\")\n",
        "print(f\"âœ“ Results saved to: {RESULT_DIR}/{config['loss_function']}_eval.txt\")\n",
        "print(f\"\\nSwitch between phases by changing: LOSS_FUNCTION = '...'\")\n",
        "print(f\"{'='*80}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cell_0007",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_0007",
        "outputId": "d4d81492-4aed-42ea-9f1f-08275c9121a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Starting Mamba-GNN Training (EstraNet-Aligned Configuration)\n",
            "================================================================================\n",
            "\n",
            "âœ“ Command: python mamba-gnn-scripts/train_mamba_gnn.py --do_train --data_path=data/ASCAD.h5 --checkpoint_dir=checkpoints/mamba_gnn_estranet --target_byte=2 --train_batch_size=256 --eval_batch_size=32 --train_steps=50000 --learning_rate=0.0001 --d_model=64 --mamba_layers=2 --gnn_layers=2 --k_neighbors=8 --eval_steps=250 --save_steps=5000 --loss_type=cross_entropy --dropout=0.25 --weight_decay=0.01 --label_smoothing=0.1 --augment_noise=0.03 --augment_shift=2 --early_stopping=20\n",
            "âœ“ Log file: logs/mamba_gnn_training_20260219_065732.log\n",
            "\n",
            "Monitoring training progress...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "CONFIGURATION (EstraNet-aligned)\n",
            "================================================================================\n",
            "  data_path             : data/ASCAD.h5\n",
            "  target_byte           : 2\n",
            "  input_length          : 700\n",
            "  do_train              : True\n",
            "  train_batch_size      : 256\n",
            "  eval_batch_size       : 32\n",
            "  train_steps           : 50000\n",
            "  iterations            : 500\n",
            "  eval_steps            : 250\n",
            "  save_steps            : 5000\n",
            "  max_eval_batch        : 312\n",
            "  learning_rate         : 0.0001\n",
            "  clip                  : 1.0\n",
            "  min_lr_ratio          : 0.004\n",
            "  warmup_steps          : 1000\n",
            "  d_model               : 64\n",
            "  mamba_layers          : 2\n",
            "  gnn_layers            : 2\n",
            "  k_neighbors           : 8\n",
            "  dropout               : 0.25\n",
            "  weight_decay          : 0.01\n",
            "  label_smoothing       : 0.1\n",
            "  early_stopping        : 20\n",
            "  augment_noise         : 0.03\n",
            "  augment_shift         : 2\n",
            "  loss_type             : cross_entropy\n",
            "  focal_gamma           : 2.5\n",
            "  focal_alpha           : 1.0\n",
            "  checkpoint_dir        : checkpoints/mamba_gnn_estranet\n",
            "  checkpoint_idx        : 0\n",
            "  warm_start            : False\n",
            "  result_path           : results/mamba_gnn\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "MAMBA-GNN TRAINING (EstraNet-aligned configuration)\n",
            "================================================================================\n",
            "\n",
            "Device: cuda\n",
            "Loading ASCAD from: data/ASCAD.h5\n",
            "Training traces:  (50000, 700)\n",
            "Attack traces:    (10000, 700)\n",
            "Target byte:      2\n",
            "\n",
            "=== DATA DIAGNOSTIC ===\n",
            "X_train  mean: -0.0000   (should be ~0.0)\n",
            "X_train  std:  1.0000    (should be ~1.0)\n",
            "X_attack mean: 0.0184\n",
            "X_attack std:  1.0029\n",
            "y_train unique labels: 256\n",
            "y_train first 10: [ 38  47  38  18 100 188 146 199  32 204]\n",
            "Most common labels:  [(103, 244), (38, 239), (90, 234), (133, 232), (251, 231)]\n",
            "Least common labels: [(222, 164), (41, 163), (253, 162), (152, 161), (213, 154)]\n",
            "=== END DIAGNOSTIC ===\n",
            "\n",
            "Optimized Mamba-GNN:\n",
            "  d_model: 64, Mamba layers: 2, GNN layers: 2\n",
            "\n",
            "Model Parameters: 242,388\n",
            "Loss: CrossEntropyLoss  label_smoothing=0.1\n",
            "Gradient clip:  1.0\n",
            "Training batches per iteration: 196\n",
            "Total training steps:           50000\n",
            "Save checkpoints every:         5000 steps\n",
            "\n",
            "================================================================================\n",
            "Starting training...\n",
            "================================================================================\n",
            "\n",
            "=== FIRST BATCH DIAGNOSTIC ===\n",
            "data shape : torch.Size([256, 700])\n",
            "data mean  : -0.0033  (should be ~0)\n",
            "data std   : 0.9828   (should be ~1)\n",
            "target[:10]: [154, 118, 220, 102, 38, 35, 168, 102, 51, 236]\n",
            "output shape       : torch.Size([256, 256])\n",
            "output mean        : -0.0150\n",
            "output std         : 0.2475\n",
            "max prob (first 5) : [0.008185121230781078, 0.006607864983379841, 0.010584915988147259, 0.006684321444481611, 0.007136364001780748]\n",
            "uniform baseline   : 0.0039\n",
            "=== END FIRST BATCH DIAGNOSTIC ===\n",
            "\n",
            "Train batches[  196]                | loss  5.56\n",
            "Eval  batches[  312]                | loss  5.56\n",
            "â˜… New best model saved (eval_loss: 5.56)\n",
            "\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "[   500] | gnorm  0.96 lr  0.000050 | loss  5.56\n",
            "Train batches[  196]                | loss  5.54\n",
            "Eval  batches[  312]                | loss  5.55\n",
            "â˜… New best model saved (eval_loss: 5.55)\n",
            "\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "Train batches[  196]                | loss  5.54\n",
            "Eval  batches[  312]                | loss  5.55\n",
            "â˜… New best model saved (eval_loss: 5.55)\n",
            "\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "\n",
            "âœ“ Checkpoint saved: best_model.pth\n",
            "[  1000] | gnorm  0.65 lr  0.000100 | loss  5.55\n",
            "Train batches[  196]                | loss  5.54\n",
            "Eval  batches[  312]                | loss  5.55\n",
            "Train batches[  196]                | loss  5.54\n",
            "Eval  batches[  312]                | loss  5.55\n",
            "[  1500] | gnorm  0.62 lr  0.000100 | loss  5.54\n",
            "Train batches[  196]                | loss  5.53\n",
            "Eval  batches[  312]                | loss  5.55\n",
            "Train batches[  196]                | loss  5.53\n",
            "Eval  batches[  312]                | loss  5.55\n",
            "[  2000] | gnorm  0.64 lr  0.000100 | loss  5.54\n",
            "Train batches[  196]                | loss  5.53\n",
            "Eval  batches[  312]                | loss  5.55\n",
            "Train batches[  196]                | loss  5.53\n",
            "Eval  batches[  312]                | loss  5.55\n",
            "[  2500] | gnorm  0.74 lr  0.000100 | loss  5.53\n",
            "Train batches[  196]                | loss  5.52\n",
            "Eval  batches[  312]                | loss  5.56\n",
            "Train batches[  196]                | loss  5.52\n",
            "Eval  batches[  312]                | loss  5.56\n",
            "[  3000] | gnorm  0.89 lr  0.000100 | loss  5.53\n",
            "Train batches[  196]                | loss  5.51\n",
            "Eval  batches[  312]                | loss  5.56\n",
            "Train batches[  196]                | loss  5.51\n",
            "Eval  batches[  312]                | loss  5.56\n",
            "[  3500] | gnorm  1.01 lr  0.000099 | loss  5.52\n",
            "Train batches[  196]                | loss  5.50\n",
            "Eval  batches[  312]                | loss  5.56\n",
            "Train batches[  196]                | loss  5.50\n",
            "Eval  batches[  312]                | loss  5.57\n",
            "[  4000] | gnorm  1.19 lr  0.000099 | loss  5.51\n",
            "Train batches[  196]                | loss  5.49\n",
            "Eval  batches[  312]                | loss  5.57\n",
            "Train batches[  196]                | loss  5.49\n",
            "Eval  batches[  312]                | loss  5.57\n",
            "[  4500] | gnorm  1.28 lr  0.000099 | loss  5.51\n",
            "Train batches[  196]                | loss  5.48\n",
            "Eval  batches[  312]                | loss  5.57\n",
            "Train batches[  196]                | loss  5.48\n",
            "Eval  batches[  312]                | loss  5.57\n",
            "[  5000] | gnorm  1.42 lr  0.000098 | loss  5.50\n",
            "Train batches[  196]                | loss  5.47\n",
            "Eval  batches[  312]                | loss  5.58\n",
            "\n",
            "âœ“ Checkpoint saved: mamba_gnn-5000.pth\n",
            "Model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-5000.pth\n",
            "\n",
            "âœ“ Checkpoint saved: checkpoint_latest.pth\n",
            "Train batches[  196]                | loss  5.47\n",
            "\n",
            "âœ“ Checkpoint saved: checkpoint_latest.pth\n",
            "Eval  batches[  312]                | loss  5.58\n",
            "[  5500] | gnorm  1.40 lr  0.000098 | loss  5.49\n",
            "Train batches[  196]                | loss  5.46\n",
            "Eval  batches[  312]                | loss  5.58\n",
            "Train batches[  196]                | loss  5.46\n",
            "Eval  batches[  312]                | loss  5.58\n",
            "\n",
            "âš  Early stopping triggered at step 5750\n",
            "  Best eval loss: 5.55\n",
            "Final model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-50000.pth\n",
            "\n",
            "âœ“ Checkpoint saved: mamba_gnn-50000.pth\n",
            "\n",
            "================================================================================\n",
            "Training completed!\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "âœ“ Training completed successfully!\n",
            "================================================================================\n",
            "\n",
            "âœ“ Training log saved to: logs/mamba_gnn_training_20260219_065732.log\n"
          ]
        }
      ],
      "source": [
        "# Execute training with real-time logging and progress monitoring\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Construct the training command dynamically from the config dictionary\n",
        "train_cmd_parts = [\n",
        "    \"python\", \"mamba-gnn-scripts/train_mamba_gnn.py\",\n",
        "    \"--do_train\" # Essential flag to tell the script to train\n",
        "]\n",
        "\n",
        "# Iterate through relevant config items to build command-line arguments\n",
        "for key, value in config.items():\n",
        "    # Skip non-command line arguments or those handled separately\n",
        "    if key in ['loss_config', 'iterations', 'clip', 'warmup_steps', 'focal_gamma', 'focal_alpha']:\n",
        "        continue\n",
        "    # Special handling for loss_function type to match script argument name\n",
        "    if key == 'loss_function':\n",
        "        train_cmd_parts.append(f\"--loss_type={value}\")\n",
        "    elif value is not None: # Only add if value is not None\n",
        "        train_cmd_parts.append(f\"--{key}={value}\")\n",
        "\n",
        "# Add focal loss specific parameters if focal_loss is active\n",
        "if config['loss_function'] in (\"focal\", \"focal_loss\"):\n",
        "    train_cmd_parts.append(f\"--focal_gamma={config['focal_gamma']}\")\n",
        "    train_cmd_parts.append(f\"--focal_alpha={config['focal_alpha']}\")\n",
        "\n",
        "train_cmd = \" \".join(train_cmd_parts)\n",
        "\n",
        "# Create log file with timestamp\n",
        "log_dir = Path(\"logs\")\n",
        "log_dir.mkdir(exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = log_dir / f\"mamba_gnn_training_{timestamp}.log\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Starting Mamba-GNN Training (EstraNet-Aligned Configuration)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nâœ“ Command: {train_cmd}\")\n",
        "print(f\"âœ“ Log file: {log_file}\\n\")\n",
        "\n",
        "# Modify command to use unbuffered Python output\n",
        "unbuffered_cmd = train_cmd.replace(\"python \", \"python -u \")\n",
        "\n",
        "# Monitor checkpoint directory for training progress\n",
        "checkpoint_dir = Path(config['checkpoint_dir'])\n",
        "last_checkpoint_time = time.time()\n",
        "\n",
        "print(\"Monitoring training progress...\\n\")\n",
        "\n",
        "# Open log file for writing\n",
        "with open(log_file, 'w') as f:\n",
        "    # Write header\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(f\"Mamba-GNN Training Log - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(f\"Command: {train_cmd}\\n\\n\")\n",
        "    f.write(\"Configuration:\\n\")\n",
        "    for key, value in config.items():\n",
        "        if key != 'loss_config':\n",
        "            f.write(f\"  {key}: {value}\\n\")\n",
        "    f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "    f.write(\"Training Output:\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.flush()\n",
        "\n",
        "    try:\n",
        "        # Start training process with unbuffered output\n",
        "        process = subprocess.Popen(\n",
        "            unbuffered_cmd,\n",
        "            shell=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "            bufsize=1,  # Line buffering\n",
        "        )\n",
        "\n",
        "        # Display output in real-time and log to file\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output:\n",
        "                line = output.rstrip()\n",
        "                print(line)\n",
        "                f.write(line + \"\\n\")\n",
        "                f.flush()  # Ensure immediate write\n",
        "\n",
        "            # Check if process finished\n",
        "            if process.poll() is not None:\n",
        "                break\n",
        "\n",
        "            # Monitor checkpoint creation\n",
        "            if checkpoint_dir.exists():\n",
        "                checkpoint_files = list(checkpoint_dir.glob(\"*.pth\"))\n",
        "                if checkpoint_files:\n",
        "                    latest = max(checkpoint_files, key=os.path.getctime)\n",
        "                    current_time = os.path.getctime(latest)\n",
        "                    if current_time > last_checkpoint_time:\n",
        "                        msg = f\"\\nâœ“ Checkpoint saved: {latest.name}\"\n",
        "                        print(msg)\n",
        "                        f.write(msg + \"\\n\")\n",
        "                        f.flush()\n",
        "                        last_checkpoint_time = current_time\n",
        "\n",
        "        # Write final status\n",
        "        if process.returncode == 0:\n",
        "            msg = \"\\n\" + \"=\"*80 + \"\\nâœ“ Training completed successfully!\\n\" + \"=\"*80\n",
        "            print(msg)\n",
        "            f.write(msg + \"\\n\")\n",
        "        else:\n",
        "            msg = \"\\n\" + \"=\"*80 + f\"\\nâœ— Training failed with error code: {process.returncode}\\n\" + \"=\"*80\n",
        "            print(msg)\n",
        "            f.write(msg + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"âœ— Error running training: {e}\"\n",
        "        print(msg)\n",
        "        f.write(msg + \"\\n\")\n",
        "\n",
        "print(f\"\\nâœ“ Training log saved to: {log_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c806809",
      "metadata": {
        "id": "8c806809"
      },
      "source": [
        "### Fix `KeyError` in Training Script\n",
        "\n",
        "The previous training run failed because the `loss_history` dictionary was not properly initialized before attempting to log metrics for `global_step=250`. The following patch modifies the `train_mamba_gnn.py` script to ensure that `loss_history[global_step]` is always a dictionary before its `update()` method is called, preventing the `KeyError`.\n",
        "\n",
        "This change uses `loss_history.setdefault(global_step, {})`, which will get the dictionary associated with `global_step` if it exists, or create an empty dictionary and associate it with `global_step` if it doesn't.\n",
        "\n",
        "After applying this patch, you can re-run the training cell (`cell_0007`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bdf3da8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdf3da8f",
        "outputId": "e36c0b02-8e38-48ab-f55e-42cfccae0ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patching script: /content/EstraNet/mamba-gnn-scripts/train_mamba_gnn.py using Python...\n",
            "âš  Pattern 'loss_history[global_step].update({' not found on line 396.\n",
            "   The line might be different than expected, or already patched.\n",
            "âœ— Patching operation completed but no changes were applied.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "script_to_patch = '/content/EstraNet/mamba-gnn-scripts/train_mamba_gnn.py'\n",
        "\n",
        "if not os.path.exists(script_to_patch):\n",
        "    print(f\"Error: Script not found at {script_to_patch}\")\n",
        "else:\n",
        "    print(f\"Patching script: {script_to_patch} using Python...\")\n",
        "    try:\n",
        "        with open(script_to_patch, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        patched = False\n",
        "        target_line_num = 396 # 1-indexed\n",
        "\n",
        "        if target_line_num <= len(lines):\n",
        "            line_index = target_line_num - 1 # 0-indexed\n",
        "            current_line = lines[line_index]\n",
        "\n",
        "            # Look for the specific pattern to replace\n",
        "            pattern_to_find = 'loss_history[global_step].update({' # Exact pattern from traceback\n",
        "            if pattern_to_find in current_line:\n",
        "                # Replace it, preserving leading whitespace\n",
        "                leading_whitespace = current_line[:current_line.find(pattern_to_find)]\n",
        "                new_content = leading_whitespace + 'loss_history.setdefault(global_step, {}).update({' # The fix\n",
        "                lines[line_index] = new_content + '\\n' # Ensure newline is preserved\n",
        "                patched = True\n",
        "                print(f\"âœ“ Line {target_line_num} patched successfully.\")\n",
        "            else:\n",
        "                print(f\"âš  Pattern '{pattern_to_find}' not found on line {target_line_num}.\")\n",
        "                print(\"   The line might be different than expected, or already patched.\")\n",
        "        else:\n",
        "            print(f\"âš  Line {target_line_num} does not exist in the script.\")\n",
        "\n",
        "\n",
        "        if patched:\n",
        "            with open(script_to_patch, 'w') as f:\n",
        "                f.writelines(lines)\n",
        "            print(\"âœ“ Script changes saved!\")\n",
        "            print(\"You can now re-run the training cell (cell_0007).\")\n",
        "        else:\n",
        "            print(\"âœ— Patching operation completed but no changes were applied.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— An unexpected error occurred during patching: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5970ac",
      "metadata": {
        "id": "ce5970ac"
      },
      "source": [
        "## Monitor Training Progress in Real-Time\n",
        "\n",
        "Run this cell **in a separate terminal** while training is running to watch checkpoints being saved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d79605",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59d79605",
        "outputId": "3a80d433-f431-462d-bb9f-2af636ab1d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Training Progress Monitor\n",
            "================================================================================\n",
            "\n",
            "Watching: checkpoints/mamba_gnn_estranet\n",
            "Press Ctrl+C to stop monitoring\n",
            "\n",
            "âœ“ [06:58:35] best_model.pth (1.0 MB)\n",
            "âœ“ [07:03:04] checkpoint_latest.pth (2.9 MB)\n",
            "âœ“ [07:03:04] mamba_gnn-5000.pth (2.9 MB)\n",
            "âœ“ [07:03:51] mamba_gnn-50000.pth (2.9 MB)\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n",
            "\n",
            "âœ“ Total checkpoints: 4\n",
            "âœ“ Total size: 9.5 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "checkpoint_dir = Path(config['checkpoint_dir'])\n",
        "previous_files = set()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Training Progress Monitor\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nWatching: {checkpoint_dir}\")\n",
        "print(\"Press Ctrl+C to stop monitoring\\n\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        if checkpoint_dir.exists():\n",
        "            # Get all checkpoint files\n",
        "            current_files = set(checkpoint_dir.glob(\"*.pth\"))\n",
        "            new_files = current_files - previous_files\n",
        "\n",
        "            # Show new checkpoints\n",
        "            for ckpt_file in sorted(new_files):\n",
        "                file_size_mb = ckpt_file.stat().st_size / (1024 * 1024)\n",
        "                timestamp = datetime.fromtimestamp(ckpt_file.stat().st_mtime).strftime(\"%H:%M:%S\")\n",
        "                print(f\"âœ“ [{timestamp}] {ckpt_file.name} ({file_size_mb:.1f} MB)\")\n",
        "\n",
        "            previous_files = current_files\n",
        "\n",
        "            # Check total size\n",
        "            if current_files:\n",
        "                total_size = sum(f.stat().st_size for f in current_files) / (1024 * 1024)\n",
        "                print(f\"\\nâœ“ Total checkpoints: {len(current_files)}\")\n",
        "                print(f\"âœ“ Total size: {total_size:.1f} MB\")\n",
        "            else:\n",
        "                print(\"â³ Waiting for first checkpoint...\")\n",
        "        else:\n",
        "            print(\"â³ Checkpoint directory not created yet...\")\n",
        "\n",
        "        # Wait before checking again\n",
        "        time.sleep(5)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nâœ“ Monitoring stopped\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc78a7af",
      "metadata": {
        "id": "cc78a7af"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "checkpoint_dir = Path(config['checkpoint_dir'])\n",
        "previous_files = set()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Training Progress Monitor\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nWatching: {checkpoint_dir}\")\n",
        "print(\"Press Ctrl+C to stop monitoring\\n\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        if checkpoint_dir.exists():\n",
        "            # Get all checkpoint files\n",
        "            current_files = set(checkpoint_dir.glob(\"*.pth\"))\n",
        "            new_files = current_files - previous_files\n",
        "\n",
        "            # Show new checkpoints\n",
        "            for ckpt_file in sorted(new_files):\n",
        "                file_size_mb = ckpt_file.stat().st_size / (1024 * 1024)\n",
        "                timestamp = datetime.fromtimestamp(ckpt_file.stat().st_mtime).strftime(\"%H:%M:%S\")\n",
        "                print(f\"âœ“ [{timestamp}] {ckpt_file.name} ({file_size_mb:.1f} MB)\")\n",
        "\n",
        "            previous_files = current_files\n",
        "\n",
        "            # Check total size\n",
        "            total_size = sum(f.stat().st_size for f in current_files) / (1024 * 1024)\n",
        "            print(f\"\\nTotal checkpoints size: {total_size:.1f} MB\")\n",
        "            print(f\"Number of checkpoints: {len(current_files)}\")\n",
        "        else:\n",
        "            print(\"âš  Checkpoint directory not found yet...\")\n",
        "\n",
        "        # Wait before checking again\n",
        "        time.sleep(5)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nâœ“ Monitoring stopped\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0008",
      "metadata": {
        "id": "cell_0008"
      },
      "source": [
        "## Option 2: Train Using TensorFlow (for TFLite Conversion)\n",
        "\n",
        "If you need to deploy on mobile/edge devices, use the TensorFlow version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0009",
      "metadata": {
        "id": "cell_0009"
      },
      "outputs": [],
      "source": [
        "# # TensorFlow training command\n",
        "# tf_train_cmd = f\"\"\"python scripts/train_mamba_gnn_tf.py \\\n",
        "#     --data_path={config['data_path']} \\\n",
        "#     --checkpoint_dir=checkpoints/mamba_gnn_tf \\\n",
        "#     --target_byte={config['target_byte']} \\\n",
        "#     --train_batch_size={config['train_batch_size']} \\\n",
        "#     --eval_batch_size={config['eval_batch_size']} \\\n",
        "#     --train_steps={config['train_steps']} \\\n",
        "#     --learning_rate={config['learning_rate']} \\\n",
        "#     --d_model={config['d_model']} \\\n",
        "#     --do_train\n",
        "# \"\"\"\n",
        "\n",
        "# print(\"TensorFlow training command:\")\n",
        "# print(tf_train_cmd)\n",
        "# print(\"\\nâœ“ This will automatically export to TFLite after training\")\n",
        "# print(\"  Output: checkpoints/mamba_gnn_tf/mamba_gnn.tflite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0010",
      "metadata": {
        "id": "cell_0010"
      },
      "outputs": [],
      "source": [
        "# Execute TensorFlow training (uncomment to run)\n",
        "# import subprocess\n",
        "# result = subprocess.run(tf_train_cmd.split(), capture_output=False)\n",
        "# print(f\"âœ“ Training complete. TFLite model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0011",
      "metadata": {
        "id": "cell_0011"
      },
      "source": [
        "## Monitor Training Progress\n",
        "\n",
        "View training loss, learning rate, and gradient norms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0012",
      "metadata": {
        "id": "cell_0012"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Load training history\n",
        "loss_path = Path(CHECKPOINT_DIR) / 'loss.pkl'\n",
        "\n",
        "if loss_path.exists():\n",
        "    with open(loss_path, 'rb') as f:\n",
        "        history = pickle.load(f)\n",
        "\n",
        "    steps = sorted(history.keys())\n",
        "    train_losses = [history[s]['train_loss'] for s in steps if 'train_loss' in history[s]]\n",
        "    lrs = [history[s]['lr'] for s in steps if 'lr' in history[s]]\n",
        "    grad_norms = [history[s]['grad_norm'] for s in steps if 'grad_norm' in history[s]]\n",
        "\n",
        "    # Plot training metrics\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Training loss\n",
        "    axes[0].plot(steps[:len(train_losses)], train_losses, 'b-', linewidth=2)\n",
        "    axes[0].set_xlabel('Training Steps', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss (Cross-Entropy)', fontsize=12)\n",
        "    axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning rate\n",
        "    axes[1].plot(steps[:len(lrs)], lrs, 'g-', linewidth=2)\n",
        "    axes[1].set_xlabel('Training Steps', fontsize=12)\n",
        "    axes[1].set_ylabel('Learning Rate', fontsize=12)\n",
        "    axes[1].set_title('Learning Rate Schedule (Cosine Decay)', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Gradient norm\n",
        "    axes[2].plot(steps[:len(grad_norms)], grad_norms, 'r-', linewidth=2)\n",
        "    axes[2].set_xlabel('Training Steps', fontsize=12)\n",
        "    axes[2].set_ylabel('Gradient Norm', fontsize=12)\n",
        "    axes[2].set_title('Gradient Norm (Clipped at 0.25)', fontsize=14, fontweight='bold')\n",
        "    axes[2].axhline(y=0.25, color='orange', linestyle='--', label='Clip threshold')\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{RESULT_DIR}/training_progress.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nâœ“ Training Progress:\")\n",
        "    print(f\"  Total steps: {max(steps):,}\")\n",
        "    print(f\"  Latest loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"  Latest LR: {lrs[-1]:.6f}\")\n",
        "    print(f\"  Latest grad norm: {grad_norms[-1]:.4f}\")\n",
        "    print(f\"\\n  Plot saved: {RESULT_DIR}/training_progress.png\")\n",
        "else:\n",
        "    print(\"âš  No training history found. Start training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0013",
      "metadata": {
        "id": "cell_0013"
      },
      "source": [
        "## Evaluate Model (Guessing Entropy)\n",
        "\n",
        "Evaluate using 100-trial Guessing Entropy methodology (matches EstraNet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0014",
      "metadata": {
        "id": "cell_0014"
      },
      "outputs": [],
      "source": [
        "# Evaluation configuration\n",
        "checkpoint_idx = 100000  # Which checkpoint to evaluate (0 = latest)\n",
        "\n",
        "eval_cmd = f\"\"\"python mamba-gnn-scripts/train_mamba_gnn.py \\\n",
        "    --data_path={config['data_path']} \\\n",
        "    --checkpoint_dir={config['checkpoint_dir']} \\\n",
        "    --target_byte={config['target_byte']} \\\n",
        "    --d_model={config['d_model']} \\\n",
        "    --mamba_layers={config['mamba_layers']} \\\n",
        "    --gnn_layers={config['gnn_layers']} \\\n",
        "    --k_neighbors={config['k_neighbors']} \\\n",
        "    --dropout={config['dropout']} \\\n",
        "    --checkpoint_idx={checkpoint_idx} \\\n",
        "    --result_path={RESULT_DIR}/mamba_gnn_eval\n",
        "\"\"\"\n",
        "\n",
        "print(\"Evaluation command:\")\n",
        "print(eval_cmd)\n",
        "print(\"\\nâœ“ This will compute 100-trial Guessing Entropy\")\n",
        "print(\"  (Takes ~10-15 minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0015",
      "metadata": {
        "id": "cell_0015"
      },
      "outputs": [],
      "source": [
        "# Execute evaluation\n",
        "import subprocess\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Evaluating Mamba-GNN (100-trial Guessing Entropy)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "result = subprocess.run(eval_cmd.split(), capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\nâœ“ Evaluation complete!\")\n",
        "else:\n",
        "    print(\"\\nâœ— Evaluation failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0016",
      "metadata": {
        "id": "cell_0016"
      },
      "source": [
        "## Plot Guessing Entropy Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0017",
      "metadata": {
        "id": "cell_0017"
      },
      "outputs": [],
      "source": [
        "# Load and plot evaluation results\n",
        "result_file = f'{RESULT_DIR}/mamba_gnn_eval.txt'\n",
        "\n",
        "if Path(result_file).exists():\n",
        "    with open(result_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        mean_ranks = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
        "        std_ranks = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
        "\n",
        "    traces = np.arange(1, len(mean_ranks) + 1)\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    # Main line\n",
        "    plt.plot(traces, mean_ranks, 'b-', linewidth=2.5, label='Mean Key Rank (GE)')\n",
        "\n",
        "    # Confidence interval\n",
        "    plt.fill_between(traces,\n",
        "                     mean_ranks - std_ranks,\n",
        "                     mean_ranks + std_ranks,\n",
        "                     alpha=0.3, color='blue', label='Â±1 Std Dev')\n",
        "\n",
        "    # Key recovered line\n",
        "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Key Recovered (Rank=0)')\n",
        "\n",
        "    # Labels and formatting\n",
        "    plt.xlabel('Number of Traces', fontsize=14)\n",
        "    plt.ylabel('Key Rank (Guessing Entropy)', fontsize=14)\n",
        "    plt.title('Mamba-GNN: Guessing Entropy Evaluation (100 trials)\\nEstraNet-Aligned Configuration',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12, loc='upper right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add milestones\n",
        "    milestones = [100, 500, 1000, 2000, 5000]\n",
        "    for m in milestones:\n",
        "        if m < len(mean_ranks):\n",
        "            plt.axvline(x=m, color='gray', linestyle=':', alpha=0.5)\n",
        "            plt.text(m, plt.ylim()[1] * 0.95, f'{m}',\n",
        "                    ha='center', fontsize=9, color='gray')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{RESULT_DIR}/guessing_entropy_curve.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"=\"*80)\n",
        "    print(\"GUESSING ENTROPY RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nTarget byte: {config['target_byte']}\")\n",
        "    print(f\"\\nKey Rank (Mean Â± Std):\")\n",
        "    print(f\"  100 traces:   {mean_ranks[99]:.2f} Â± {std_ranks[99]:.2f}\")\n",
        "    print(f\"  500 traces:   {mean_ranks[499]:.2f} Â± {std_ranks[499]:.2f}\")\n",
        "    print(f\"  1000 traces:  {mean_ranks[999]:.2f} Â± {std_ranks[999]:.2f}\")\n",
        "    if len(mean_ranks) >= 2000:\n",
        "        print(f\"  2000 traces:  {mean_ranks[1999]:.2f} Â± {std_ranks[1999]:.2f}\")\n",
        "    if len(mean_ranks) >= 5000:\n",
        "        print(f\"  5000 traces:  {mean_ranks[4999]:.2f} Â± {std_ranks[4999]:.2f}\")\n",
        "    if len(mean_ranks) >= 10000:\n",
        "        print(f\"  10000 traces: {mean_ranks[9999]:.2f} Â± {std_ranks[9999]:.2f}\")\n",
        "\n",
        "    # Find recovery point\n",
        "    recovered_idx = np.where(mean_ranks == 0)[0]\n",
        "    if len(recovered_idx) > 0:\n",
        "        print(f\"\\nâœ“ Key RECOVERED at {recovered_idx[0]+1} traces\")\n",
        "    else:\n",
        "        print(f\"\\nâœ— Key NOT recovered (best rank: {mean_ranks[-1]:.2f})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Plot saved: {RESULT_DIR}/guessing_entropy_curve.png\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"âš  No evaluation results found. Run evaluation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0018",
      "metadata": {
        "id": "cell_0018"
      },
      "source": [
        "## Compare with EstraNet\n",
        "\n",
        "Compare Mamba-GNN with EstraNet Transformer/GNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0019",
      "metadata": {
        "id": "cell_0019"
      },
      "outputs": [],
      "source": [
        "# Compare with EstraNet results\n",
        "estranet_result = 'results/estranet_transformer_eval.txt'  # Update this path\n",
        "\n",
        "compare_cmd = f\"\"\"python scripts/compare_results.py \\\n",
        "    --mamba_results={result_file} \\\n",
        "    --estranet_results={estranet_result} \\\n",
        "    --output={RESULT_DIR}/model_comparison.png\n",
        "\"\"\"\n",
        "\n",
        "print(\"Comparison command:\")\n",
        "print(compare_cmd)\n",
        "print(\"\\nâœ“ Make sure you have EstraNet results first:\")\n",
        "print(\"  python scripts/train_trans.py --model_type=transformer --do_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0020",
      "metadata": {
        "id": "cell_0020"
      },
      "outputs": [],
      "source": [
        "# Execute comparison (uncomment when EstraNet results are ready)\n",
        "# import subprocess\n",
        "# result = subprocess.run(compare_cmd.split())\n",
        "# print(\"âœ“ Comparison plot created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0021",
      "metadata": {
        "id": "cell_0021"
      },
      "source": [
        "## Load and Use Trained Model\n",
        "\n",
        "Load checkpoint for inference or further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0022",
      "metadata": {
        "id": "cell_0022"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import h5py\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add current directory to sys.path to allow absolute imports from 'models'\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# Import model using absolute path from the models directory\n",
        "# This assumes the mamba_gnn_model.py, mamba_block.py, etc., are in the 'models' folder\n",
        "# and that 'EstraNet' is the working directory.\n",
        "# The relative imports within mamba_gnn_model.py will be fixed by the patch below.\n",
        "from models.mamba_gnn_model import OptimizedMambaGNN\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Patch mamba_gnn_model.py for relative import error ---\n",
        "script_to_patch = 'models/mamba_gnn_model.py'\n",
        "if os.path.exists(script_to_patch):\n",
        "    print(f\"Patching imports in {script_to_patch}...\")\n",
        "    with open(script_to_patch, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Replace relative imports with absolute imports within the 'models' context\n",
        "    content = content.replace('from .mamba_block import OptimizedMambaBlock', 'from models.mamba_block import OptimizedMambaBlock')\n",
        "    content = content.replace('from .gat_layer import EnhancedGAT', 'from models.gat_layer import EnhancedGAT')\n",
        "    content = content.replace('from .patch_embedding import CNNPatchEmbedding', 'from models.patch_embedding import CNNPatchEmbedding')\n",
        "\n",
        "    with open(script_to_patch, 'w') as f:\n",
        "        f.write(content)\n",
        "    print(\"âœ“ Imports patched successfully!\")\n",
        "else:\n",
        "    print(f\"âš  Warning: Model script not found at {script_to_patch}. Cannot patch imports.\")\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Create model (EstraNet-aligned configuration)\n",
        "model = OptimizedMambaGNN(\n",
        "    trace_length=700,\n",
        "    d_model=config['d_model'],\n",
        "    mamba_layers=config['mamba_layers'],\n",
        "    gnn_layers=config['gnn_layers'],\n",
        "    num_classes=256,\n",
        "    k_neighbors=config['k_neighbors'],\n",
        "    dropout=config['dropout']\n",
        ").to(device)\n",
        "\n",
        "# Load checkpoint\n",
        "ckpt_path = f\"{CHECKPOINT_DIR}/mamba_gnn-100000.pth\"\n",
        "\n",
        "if Path(ckpt_path).exists():\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    print(f\"\\nâœ“ Model loaded successfully\")\n",
        "    print(f\"  Checkpoint: {ckpt_path}\")\n",
        "    print(f\"  Training step: {checkpoint['global_step']:,}\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Model ready for inference\")\n",
        "else:\n",
        "    print(f\"âš  Checkpoint not found: {ckpt_path}\")\n",
        "    print(\"  Train the model first or specify different checkpoint_idx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "300fc456",
      "metadata": {
        "id": "300fc456"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Define paths\n",
        "checkpoint_folder = 'checkpoints/mamba_gnn_estranet'\n",
        "output_filename = 'mamba_gnn_estranet_checkpoints'\n",
        "\n",
        "# Check if folder exists\n",
        "if os.path.exists(checkpoint_folder):\n",
        "    print(f\"Zipping {checkpoint_folder}...\")\n",
        "\n",
        "    # Create zip file\n",
        "    shutil.make_archive(output_filename, 'zip', checkpoint_folder)\n",
        "\n",
        "    # Download\n",
        "    zip_path = f\"{output_filename}.zip\"\n",
        "    print(f\"Downloading {zip_path} ({os.path.getsize(zip_path) / 1e6:.1f} MB)...\")\n",
        "    files.download(zip_path)\n",
        "else:\n",
        "    print(f\"Folder not found: {checkpoint_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0023",
      "metadata": {
        "id": "cell_0023"
      },
      "outputs": [],
      "source": [
        "# Example: Get predictions on attack traces\n",
        "if Path(ckpt_path).exists() and Path(DATA_PATH).exists():\n",
        "    print(\"Running inference on sample traces...\")\n",
        "\n",
        "    # Load attack data\n",
        "    with h5py.File(DATA_PATH, 'r') as f:\n",
        "        X_attack = f['Attack_traces/traces'][:100]  # First 100 traces\n",
        "        m_attack = f['Attack_traces/metadata'][:]\n",
        "\n",
        "    # Normalize (using same scaler as training)\n",
        "    with h5py.File(DATA_PATH, 'r') as f:\n",
        "        X_train_sample = f['Profiling_traces/traces'][:1000]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train_sample)\n",
        "    X_attack_norm = scaler.transform(X_attack)\n",
        "\n",
        "    # Get predictions\n",
        "    X_tensor = torch.FloatTensor(X_attack_norm).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_tensor)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "    predicted_classes = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "\n",
        "    print(f\"\\nâœ“ Inference complete\")\n",
        "    print(f\"  Traces processed: {len(X_attack)}\")\n",
        "    print(f\"  Prediction shape: {probs.shape}\")\n",
        "    print(f\"  Top predicted classes: {predicted_classes[:10]}\")\n",
        "    print(f\"  Confidence (first trace): {probs[0].max().item():.4f}\")\n",
        "else:\n",
        "    print(\"âš  Model or data not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0024",
      "metadata": {
        "id": "cell_0024"
      },
      "source": [
        "## Summary & Configuration Verification\n",
        "\n",
        "**âœ… This notebook uses the CORRECT configuration:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0025",
      "metadata": {
        "id": "cell_0025"
      },
      "outputs": [],
      "source": [
        "# Verify configuration matches EstraNet\n",
        "print(\"=\"*80)\n",
        "print(\"CONFIGURATION VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "verification = {\n",
        "    'Loss Function': ('Cross-Entropy', 'âœ“'),\n",
        "    'Learning Rate': ('2.5e-4', 'âœ“'),\n",
        "    'Train Batch Size': ('256', 'âœ“'),\n",
        "    'Eval Batch Size': ('32', 'âœ“'),\n",
        "    'Training Steps': ('100,000', 'âœ“'),\n",
        "    'Optimizer': ('Adam', 'âœ“'),\n",
        "    'LR Schedule': ('Cosine Decay', 'âœ“'),\n",
        "    'Model Dimension': ('128', 'âœ“'),\n",
        "    'Dropout': ('0.1', 'âœ“'),\n",
        "    'Gradient Clipping': ('0.25', 'âœ“'),\n",
        "    'Evaluation Method': ('100-trial GE', 'âœ“'),\n",
        "}\n",
        "\n",
        "print(\"\\nEstraNet Alignment Check:\")\n",
        "for param, (value, status) in verification.items():\n",
        "    print(f\"  {status} {param:25s}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ“ ALL PARAMETERS MATCH ESTRANET\")\n",
        "print(\"âœ“ FAIR COMPARISON GUARANTEED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nâš ï¸ DO NOT USE: final_best_gnn_mamba_teacher.ipynb\")\n",
        "print(\"   That notebook has incompatible configuration:\")\n",
        "print(\"   - FocalLoss instead of Cross-Entropy\")\n",
        "print(\"   - Learning rate 8x too high\")\n",
        "print(\"   - Batch size 4x too small\")\n",
        "print(\"   - Single-trial evaluation instead of 100-trial GE\")\n",
        "\n",
        "print(\"\\nâœ“ USE THIS NOTEBOOK for training and evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0026",
      "metadata": {
        "id": "cell_0026"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "### 1. Train Model\n",
        "Run the training cell above to start training with correct configuration.\n",
        "\n",
        "### 2. Monitor Progress\n",
        "Check training loss, learning rate, and gradient norms periodically.\n",
        "\n",
        "### 3. Evaluate Model\n",
        "After training, run evaluation to get Guessing Entropy curves.\n",
        "\n",
        "### 4. Compare with EstraNet\n",
        "Train EstraNet models and compare results using the comparison script.\n",
        "\n",
        "### 5. Expected Results\n",
        "- **After 100k steps**: Loss ~4.8-5.0\n",
        "- **Key recovery**: Within 1000-2000 traces\n",
        "- **Performance**: Comparable to EstraNet Transformer (~1200 traces)\n",
        "\n",
        "### Files Created\n",
        "- **Training script**: `mamba-gnn-scripts/train_mamba_gnn.py`\n",
        "- **TensorFlow script**: `scripts/train_mamba_gnn_tf.py`\n",
        "- **PowerShell runner**: `mamba-gnn-scripts/train_mamba_gnn.ps1`\n",
        "- **Bash runner**: `mamba-gnn-scripts/train_mamba_gnn.sh`\n",
        "- **Comparison tool**: `scripts/compare_results.py`\n",
        "- **This notebook**: `train_mamba_gnn_notebook.ipynb`\n",
        "\n",
        "### Documentation\n",
        "- **Quick start**: `QUICKSTART.md`\n",
        "- **TFLite guide**: `TENSORFLOW_TFLITE_GUIDE.md`\n",
        "- **TF implementation**: `TENSORFLOW_IMPLEMENTATION_SUMMARY.md`\n",
        "- **Config comparison**: `NOTEBOOK_VS_SCRIPT_COMPARISON.md`\n",
        "\n",
        "**Ready to train with fair comparison to EstraNet!** ðŸŽ¯"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b070cd09",
      "metadata": {
        "id": "b070cd09"
      },
      "source": [
        "## Compare Both Phases for Report\n",
        "\n",
        "After completing Phase 1ï¸âƒ£ and Phase 2ï¸âƒ£, use this to compare results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e98c964",
      "metadata": {
        "id": "1e98c964"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results from both phases\n",
        "phase1_result = f'{RESULT_DIR}/cross_entropy_eval.txt'\n",
        "phase2_result = f'{RESULT_DIR}/focal_loss_eval.txt'\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Phase 1: Fair Comparison\n",
        "if Path(phase1_result).exists():\n",
        "    with open(phase1_result, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        phase1_mean = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
        "        phase1_std = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
        "    results['Phase 1 (Cross-Entropy)'] = phase1_mean\n",
        "    print(\"âœ“ Phase 1 results loaded\")\n",
        "else:\n",
        "    print(\"âš  Phase 1 results not found. Train Phase 1 first.\")\n",
        "\n",
        "# Phase 2: Performance\n",
        "if Path(phase2_result).exists():\n",
        "    with open(phase2_result, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        phase2_mean = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
        "        phase2_std = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
        "    results['Phase 2 (FocalLoss)'] = phase2_mean\n",
        "    print(\"âœ“ Phase 2 results loaded\")\n",
        "else:\n",
        "    print(\"âš  Phase 2 results not found. Train Phase 2 first.\")\n",
        "\n",
        "# Compare key milestones\n",
        "if len(results) == 2:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON TABLE FOR REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    milestones = [100, 500, 1000, 2000, 5000]\n",
        "\n",
        "    print(f\"\\n{'Traces':<10} {'Phase 1 (CE)':<20} {'Phase 2 (Focal)':<20} {'Improvement'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for m in milestones:\n",
        "        if m <= len(phase1_mean):\n",
        "            p1_rank = phase1_mean[m-1]\n",
        "            p2_rank = phase2_mean[m-1]\n",
        "            improvement = ((p1_rank - p2_rank) / (p1_rank + 1e-6)) * 100\n",
        "\n",
        "            print(f\"{m:<10} {p1_rank:<20.2f} {p2_rank:<20.2f} {improvement:+.1f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    # Find recovery points\n",
        "    p1_recovery = np.where(phase1_mean == 0)[0]\n",
        "    p2_recovery = np.where(phase2_mean == 0)[0]\n",
        "\n",
        "    print(\"\\nKEY RECOVERY SUMMARY:\")\n",
        "    if len(p1_recovery) > 0:\n",
        "        print(f\"  Phase 1 (Cross-Entropy): {p1_recovery[0]+1} traces\")\n",
        "    else:\n",
        "        print(f\"  Phase 1 (Cross-Entropy): No recovery (best: {phase1_mean[-1]:.2f})\")\n",
        "\n",
        "    if len(p2_recovery) > 0:\n",
        "        print(f\"  Phase 2 (FocalLoss):     {p2_recovery[0]+1} traces\")\n",
        "    else:\n",
        "        print(f\"  Phase 2 (FocalLoss):     No recovery (best: {phase2_mean[-1]:.2f})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    # Plot comparison\n",
        "    traces = np.arange(1, min(len(phase1_mean), len(phase2_mean)) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(traces, phase1_mean[:len(traces)], 'b-', linewidth=2.5, label='Phase 1: Cross-Entropy (Fair)')\n",
        "    plt.plot(traces, phase2_mean[:len(traces)], 'r-', linewidth=2.5, label='Phase 2: FocalLoss (Optimized)')\n",
        "\n",
        "    plt.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    plt.xlabel('Number of Traces', fontsize=14)\n",
        "    plt.ylabel('Key Rank (Guessing Entropy)', fontsize=14)\n",
        "    plt.title('Mamba-GNN Comparison: Fair vs Optimized\\nfor Report', fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12, loc='upper right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{RESULT_DIR}/phase_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Comparison plot saved: {RESULT_DIR}/phase_comparison.png\")\n",
        "else:\n",
        "    print(\"âš  Complete both Phase 1 and Phase 2 to see comparison\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
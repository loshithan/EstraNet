{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell_0000",
   "metadata": {
    "id": "cell_0000"
   },
   "source": [
    "# Mamba-GNN Training Notebook (EstraNet-Aligned Configuration)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: This notebook uses the **corrected configuration** that matches EstraNet for fair comparison.\n",
    "\n",
    "## Key Configuration ‚úÖ\n",
    "- **Loss Function**: Cross-Entropy (not Focal Loss)\n",
    "- **Learning Rate**: 2.5e-4 (not 2e-3)\n",
    "- **Batch Size**: 256 train / 32 eval (not 64)\n",
    "- **Training**: 100k steps (not 50-100 epochs)\n",
    "- **Optimizer**: Adam (not AdamW)\n",
    "- **Scheduler**: Cosine Decay (not OneCycleLR)\n",
    "- **Evaluation**: 100-trial Guessing Entropy (not single trial)\n",
    "\n",
    "## DO NOT USE `final_best_gnn_mamba_teacher.ipynb` ‚ùå\n",
    "That notebook has incorrect configuration and cannot be fairly compared with EstraNet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267e0826",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "267e0826",
    "outputId": "9c638c71-b76b-461b-8872-a9e8f7748003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EstraNet'...\n",
      "remote: Enumerating objects: 352, done.\u001b[K\n",
      "remote: Counting objects: 100% (352/352), done.\u001b[K\n",
      "remote: Compressing objects: 100% (242/242), done.\u001b[K\n",
      "remote: Total 352 (delta 159), reused 301 (delta 108), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (352/352), 32.96 MiB | 25.08 MiB/s, done.\n",
      "Resolving deltas: 100% (159/159), done.\n",
      "Updating files: 100% (139/139), done.\n",
      "‚úÖ Clean! Directory: /content/EstraNet\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "if os.path.exists('EstraNet'):\n",
    "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
    "\n",
    "!git clone https://github.com/loshithan/EstraNet.git\n",
    "os.chdir('EstraNet')\n",
    "print(f\"‚úÖ Clean! Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0001",
   "metadata": {
    "id": "cell_0001"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell_0002",
    "outputId": "dfba8b03-60ee-4577-801f-92efa03adb58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: Tesla T4\n",
      "\n",
      "‚úì Environment ready\n",
      "  Data path: data/ASCAD.h5\n",
      "  Checkpoint dir: checkpoints/mamba_gnn_estranet\n",
      "  Result dir: results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected ‚Äî switch Colab runtime to GPU (T4 recommended)\")\n",
    "\n",
    "# Add paths\n",
    "sys.path.append(str(Path.cwd() / 'mamba-gnn-scripts'))\n",
    "sys.path.append(str(Path.cwd()))\n",
    "\n",
    "# ---- Paths: real S6 Mamba run (--use_ssm_mamba --no_patch_embed, no GNN) ----\n",
    "DATA_PATH       = 'data/ASCAD.h5'\n",
    "CHECKPOINT_DIR  = 'checkpoints/mamba_ssm700'\n",
    "FOCAL_CKPT_DIR  = 'checkpoints/mamba_ssm700_focal'\n",
    "RESULT_DIR      = 'results'\n",
    "\n",
    "# Create directories\n",
    "for d in (CHECKPOINT_DIR, FOCAL_CKPT_DIR, RESULT_DIR):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úì Environment ready\")\n",
    "print(f\"  Data path      : {DATA_PATH}\")\n",
    "print(f\"  Checkpoint dir : {CHECKPOINT_DIR}  (real S6 SSM, 700 tokens, no GNN)\")\n",
    "print(f\"  Focal ckpt dir : {FOCAL_CKPT_DIR}\")\n",
    "print(f\"  Result dir     : {RESULT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0003",
   "metadata": {
    "id": "cell_0003"
   },
   "source": [
    "## Configuration Comparison\n",
    "\n",
    "### ‚ùå Old Notebook (final_best_gnn_mamba_teacher.ipynb)\n",
    "```python\n",
    "# WRONG CONFIGURATION - DO NOT USE\n",
    "loss = FocalLoss(gamma=2.5)\n",
    "optimizer = AdamW(lr=2e-3)\n",
    "batch_size = 64\n",
    "epochs = 50-100\n",
    "scheduler = OneCycleLR(max_lr=2e-3)\n",
    "evaluation = single_trial_key_rank()\n",
    "```\n",
    "\n",
    "### ‚úÖ New Configuration (This Notebook)\n",
    "```python\n",
    "# CORRECT CONFIGURATION - Matches EstraNet\n",
    "loss = CrossEntropyLoss()\n",
    "optimizer = Adam(lr=2.5e-4)\n",
    "batch_size = 256 (train), 32 (eval)\n",
    "train_steps = 100000\n",
    "scheduler = CosineLRSchedule(max_lr=2.5e-4)\n",
    "evaluation = compute_ge_key_rank(num_trials=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0004",
   "metadata": {
    "id": "cell_0004"
   },
   "source": [
    "## Option 1: Train Using PyTorch Script (Recommended)\n",
    "\n",
    "This uses the corrected `train_mamba_gnn.py` script with EstraNet-aligned configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13c143",
   "metadata": {
    "id": "4e13c143"
   },
   "source": [
    "## Choose Loss Function: Fair Comparison vs Performance\n",
    "\n",
    "**Two-Phase Experiment Strategy for Your Report:**\n",
    "\n",
    "### Phase 1Ô∏è‚É£ (FIRST): Fair Comparison with Cross-Entropy\n",
    "- **Purpose**: Compare Mamba-GNN vs EstraNet objectively (same configuration)\n",
    "- **Loss**: Cross-Entropy\n",
    "- **Results**: Document baseline performance\n",
    "- **Section in report**: \"Fair Comparison Analysis\"\n",
    "\n",
    "### Phase 2Ô∏è‚É£ (THEN): Best Performance with FocalLoss\n",
    "- **Purpose**: Show Mamba-GNN's maximum attack potential\n",
    "- **Loss**: FocalLoss (gamma=2.5)\n",
    "- **Results**: Document optimized performance\n",
    "- **Section in report**: \"Performance Analysis\" or \"Best-Case Scenario\"\n",
    "\n",
    "Toggle loss function below ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1df9b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c1df9b8",
    "outputId": "8ea1109c-6124-47b8-cf94-78341acb8956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1Ô∏è‚É£: FAIR COMPARISON MODE (WITH REGULARIZATION)\n",
      "================================================================================\n",
      "\n",
      "‚úì Loss Function: Cross-Entropy (with label smoothing 0.1)\n",
      "‚úì Regularization: weight_decay=0.01, dropout=0.3\n",
      "‚úì Data Augmentation: noise=0.05, shift=3\n",
      "‚úì Model Capacity: REDUCED (d_model=64, mamba=2, gnn=2)\n",
      "‚úì Early Stopping: After 40 eval periods (~10k steps) without improvement\n",
      "\n",
      "‚ö†Ô∏è Previous training showed SEVERE OVERFITTING:\n",
      "   Train loss ‚Üí 0, Eval loss ‚Üí 22 (massive gap)\n",
      "   These changes should prevent that.\n",
      "\n",
      "‚úì Configuration:\n",
      "  data_path           : data/ASCAD.h5\n",
      "  checkpoint_dir      : checkpoints/mamba_gnn_estranet\n",
      "  target_byte         : 2\n",
      "  train_batch_size    : 256\n",
      "  eval_batch_size     : 32\n",
      "  train_steps         : 50000\n",
      "  learning_rate       : 0.0001\n",
      "  d_model             : 64\n",
      "  mamba_layers        : 2\n",
      "  gnn_layers          : 2\n",
      "  k_neighbors         : 8\n",
      "  eval_steps          : 250\n",
      "  save_steps          : 5000\n",
      "  warmup_steps        : 1000\n",
      "  loss_function       : cross_entropy\n",
      "  clip                : 1.0\n",
      "  dropout             : 0.25\n",
      "  weight_decay        : 0.01\n",
      "  label_smoothing     : 0.1\n",
      "  augment_noise       : 0.03\n",
      "  augment_shift       : 2\n",
      "  early_stopping      : 20\n",
      "  focal_gamma         : 2.5\n",
      "  focal_alpha         : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== SWITCH LOSS FUNCTION HERE ==========\n",
    "LOSS_FUNCTION = \"cross_entropy\"   # ‚Üê start here; switch to \"focal_loss\" for Phase 2\n",
    "# ===============================================\n",
    "\n",
    "# ---- Architecture diagnosis (summary of findings) ----\n",
    "#\n",
    "# ROOT CAUSE 1 ‚Äî FIXED: 14-patch CNN embedding\n",
    "#   AdaptiveAvgPool1d(14) averaged 50 raw samples/patch ‚Üí AES leakage destroyed.\n",
    "#   Fix: --no_patch_embed  ‚Üí  Linear(1, d_model) per step  ‚Üí  700 full-res tokens.\n",
    "#\n",
    "# ROOT CAUSE 2 ‚Äî FIXED: OptimizedMambaBlock was a gated depthwise CNN\n",
    "#   Kernel=7, 4 layers ‚Üí effective receptive field ‚âà 28 samples ONLY.\n",
    "#   A local CNN CANNOT learn \"position 312 correlates with class Y\" because it\n",
    "#   never compares positions that are more than 28 samples apart.\n",
    "#\n",
    "# FIX: --use_ssm_mamba  ‚Üí  SelectiveMambaBlock (REAL Mamba S6 algorithm)\n",
    "#\n",
    "#   The S6 selective SSM at each step t:\n",
    "#     h(t) = exp(Œî(t)¬∑A)¬∑h(t-1) + Œî(t)¬∑B(t)¬∑x(t)   ‚Üê recurrence\n",
    "#     y(t) = C(t)¬∑h(t) + D¬∑x(t)\n",
    "#\n",
    "#   h(T) carries accumulated information from x(0)...x(T) ‚Üí GLOBAL receptive field.\n",
    "#   Complexity: O(n ¬∑ d_inner ¬∑ d_state)  ‚Üê LINEAR  ‚Üê your research novelty\n",
    "#   vs Transformer self-attention:  O(n¬≤)  ‚Üê quadratic\n",
    "#\n",
    "#   Key difference vs TransformerBlock (which was reverted):\n",
    "#   - Transformer: O(n¬≤) attention ‚Äî NOT your novelty, defeats the point\n",
    "#   - Real Mamba S6: O(n) recurrence with global memory ‚Äî THIS is the contribution\n",
    "\n",
    "config = {\n",
    "    'data_path': DATA_PATH,\n",
    "    'checkpoint_dir': CHECKPOINT_DIR,\n",
    "    'target_byte': 2,\n",
    "\n",
    "    # Model architecture ‚Äî real Mamba S6, linear-700 tokens, no GNN\n",
    "    'd_model': 64,\n",
    "    'mamba_layers': 4,          # 4 S6 blocks\n",
    "    'gnn_layers': 0,            # GNN removed (confirmed no benefit with 14 patches)\n",
    "    'k_neighbors': 8,\n",
    "    'd_state': 8,               # SSM state size: 8 halves scan tensor vs 16 (~2x faster on GPU)\n",
    "    'no_patch_embed': True,     # FIX 1: 700-step linear embed\n",
    "    'use_ssm_mamba': True,      # FIX 2: real S6 selective SSM (O(n), global reach)\n",
    "\n",
    "    # Training schedule ‚Äî EstraNet regime\n",
    "    'train_steps': 50000,\n",
    "    'learning_rate': 0.00025,\n",
    "    'warmup_steps': 2000,\n",
    "    'train_batch_size': 256,\n",
    "    'eval_batch_size': 32,\n",
    "    'eval_steps': 1000,\n",
    "    'save_steps': 10000,\n",
    "\n",
    "    # Regularisation\n",
    "    'clip': 1.0,\n",
    "    'dropout': 0.1,\n",
    "    'weight_decay': 0.0,\n",
    "    'label_smoothing': 0.0,\n",
    "    'augment_noise': 0.01,\n",
    "    'augment_shift': 1,\n",
    "    'early_stopping': 0,\n",
    "\n",
    "    # FocalLoss params (Phase 2 only)\n",
    "    'loss_function': LOSS_FUNCTION,\n",
    "    'focal_gamma': 2.5,\n",
    "    'focal_alpha': 1.0,\n",
    "}\n",
    "\n",
    "# Focal overrides\n",
    "if LOSS_FUNCTION in (\"focal\", \"focal_loss\"):\n",
    "    config['learning_rate'] = 1e-4\n",
    "    config['dropout'] = 0.1\n",
    "    config['early_stopping'] = 0\n",
    "    config['train_steps'] = 25000\n",
    "    config['eval_steps'] = 500\n",
    "    config['save_steps'] = 5000\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Real Mamba S6 Config ‚Äî O(n) Selective SSM, 700 Tokens\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  Architecture: d_model=64 | S6-blocks=4 | gnn=0 | embed=linear700\")\n",
    "print()\n",
    "print(\"  Fix 1: --no_patch_embed  ‚Üí  700 full-resolution tokens\")\n",
    "print(\"  Fix 2: --use_ssm_mamba   ‚Üí  real S6 selective SSM\")\n",
    "print()\n",
    "print(\"  SSM recurrence (per step t, per block):\")\n",
    "print(\"    h(t) = exp(Œî(t)¬∑A)¬∑h(t-1) + Œî(t)¬∑B(t)¬∑x(t)   [global memory]\")\n",
    "print(\"    y(t) = C(t)¬∑h(t) + D¬∑x(t)\")\n",
    "print()\n",
    "print(\"  Complexity:   O(n ¬∑ d_inner ¬∑ d_state) = O(n¬∑128¬∑16) ‚Üê LINEAR\")\n",
    "print(\"  Transformer:  O(n¬≤)                                  ‚Üê quadratic\")\n",
    "print(\"  Research novelty: this model vs EstraNet transformer\")\n",
    "print()\n",
    "print(\"  Decision at step 1000:\")\n",
    "print(\"    eval loss < 5.54  ‚Üí  working! run to 50k steps\")\n",
    "print(\"    eval loss ‚â• 5.545 ‚Üí  check training diagnostics\")\n",
    "print()\n",
    "print(\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    if k != 'loss_config':\n",
    "        print(f\"  {k:22s}: {v}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c2455",
   "metadata": {
    "id": "716c2455"
   },
   "source": [
    "## Report Structure: How to Document Results\n",
    "\n",
    "### Section 1: Fair Comparison Analysis (Phase 1Ô∏è‚É£)\n",
    "```markdown\n",
    "## Fair Comparison: Mamba-GNN vs EstraNet\n",
    "\n",
    "### Configuration Match\n",
    "- Loss: Cross-Entropy (same as EstraNet) ‚úì\n",
    "- Learning Rate: 2.5e-4 (same as EstraNet) ‚úì\n",
    "- Batch Size: 256 (same as EstraNet) ‚úì\n",
    "- Optimizer: Adam with Cosine schedule ‚úì\n",
    "- Evaluation: 100-trial Guessing Entropy ‚úì\n",
    "\n",
    "### Results\n",
    "| Model | Key Rank @ 100 traces | Key Rank @ 1000 traces | Recovery Point |\n",
    "|-------|----------------------|----------------------|--------|\n",
    "| EstraNet Transformer | X | Y | Z traces |\n",
    "| Mamba-GNN | X | Y | Z traces |\n",
    "\n",
    "### Conclusion\n",
    "Fair comparison shows [which model is better/comparable]\n",
    "```\n",
    "\n",
    "### Section 2: Performance Analysis (Phase 2Ô∏è‚É£)\n",
    "```markdown\n",
    "## Performance Optimization: Mamba-GNN with FocalLoss\n",
    "\n",
    "### Configuration\n",
    "- Loss: FocalLoss (gamma=2.5) - optimized for attack\n",
    "- Same training setup otherwise ‚úì\n",
    "\n",
    "### Results\n",
    "| Model | Loss | Key Rank @ 100 traces | Key Rank @ 1000 traces |\n",
    "|-------|------|----------------------|----------------------|\n",
    "| Mamba-GNN (Cross-Entropy) | Baseline | X | Y |\n",
    "| Mamba-GNN (FocalLoss) | Optimized | X' | Y' |\n",
    "\n",
    "### Improvement\n",
    "FocalLoss improves performance by [X%] at key recovery\n",
    "```\n",
    "\n",
    "üëá Execute cells below to gather data for both sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell_0006",
   "metadata": {
    "id": "cell_0006"
   },
   "outputs": [],
   "source": [
    "# Base args (include all regularization flags we expect)\n",
    "base_args = {\n",
    "    'data_path': config['data_path'],\n",
    "    'checkpoint_dir': config['checkpoint_dir'],\n",
    "    'target_byte': config['target_byte'],\n",
    "    'train_batch_size': config['train_batch_size'],\n",
    "    'eval_batch_size': config['eval_batch_size'],\n",
    "    'train_steps': config['train_steps'],\n",
    "    'eval_steps': config['eval_steps'],\n",
    "    'save_steps': config['save_steps'],\n",
    "    'learning_rate': config['learning_rate'],\n",
    "    'd_model': config['d_model'],\n",
    "    'mamba_layers': config['mamba_layers'],\n",
    "    'gnn_layers': config['gnn_layers'],\n",
    "    'k_neighbors': config['k_neighbors'],\n",
    "    'dropout': config['dropout'],\n",
    "    'weight_decay': config.get('weight_decay'),\n",
    "    'label_smoothing': config.get('label_smoothing'),\n",
    "    'early_stopping': config.get('early_stopping'),\n",
    "    'augment_noise': config.get('augment_noise'),\n",
    "    'augment_shift': config.get('augment_shift'),\n",
    "    'loss_type': config.get('loss_function'),        # NEW: pass loss type\n",
    "    'focal_gamma': config.get('focal_gamma'),       # NEW: focal params\n",
    "    'focal_alpha': config.get('focal_alpha'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdadd6e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdadd6e5",
    "outputId": "7f650162-04f1-4e05-be8d-38ffd89134a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Running: Phase 1Ô∏è‚É£ (Fair Comparison)\n",
      "================================================================================\n",
      "\n",
      "‚úì Checkpoints saved to: checkpoints/mamba_gnn_estranet\n",
      "‚úì Results saved to: results/cross_entropy_eval.txt\n",
      "\n",
      "Switch between phases by changing: LOSS_FUNCTION = '...'\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup checkpoint directories for both phases\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create phase-specific directories\n",
    "# FIX: Use the existing directory for Phase 1 as requested\n",
    "PHASE1_CKPT = 'checkpoints/mamba_gnn_estranet'  # Cross-Entropy\n",
    "PHASE2_CKPT = 'checkpoints/mamba_gnn_phase2_focal'  # FocalLoss\n",
    "\n",
    "os.makedirs(PHASE1_CKPT, exist_ok=True)\n",
    "os.makedirs(PHASE2_CKPT, exist_ok=True)\n",
    "\n",
    "# Determine which checkpoint dir to use\n",
    "if config['loss_function'] == \"cross_entropy\":\n",
    "    ACTIVE_CKPT = PHASE1_CKPT\n",
    "    PHASE_NAME = \"Phase 1Ô∏è‚É£ (Fair Comparison)\"\n",
    "else:\n",
    "    ACTIVE_CKPT = PHASE2_CKPT\n",
    "    PHASE_NAME = \"Phase 2Ô∏è‚É£ (Performance)\"\n",
    "\n",
    "config['checkpoint_dir'] = ACTIVE_CKPT\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Running: {PHASE_NAME}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n‚úì Checkpoints saved to: {ACTIVE_CKPT}\")\n",
    "print(f\"‚úì Results saved to: {RESULT_DIR}/{config['loss_function']}_eval.txt\")\n",
    "print(f\"\\nSwitch between phases by changing: LOSS_FUNCTION = '...'\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0007",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell_0007",
    "outputId": "d4d81492-4aed-42ea-9f1f-08275c9121a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Mamba-GNN Training (EstraNet-Aligned Configuration)\n",
      "================================================================================\n",
      "\n",
      "‚úì Command: python mamba-gnn-scripts/train_mamba_gnn.py --do_train --data_path=data/ASCAD.h5 --checkpoint_dir=checkpoints/mamba_gnn_estranet --target_byte=2 --train_batch_size=256 --eval_batch_size=32 --train_steps=50000 --learning_rate=0.0001 --d_model=64 --mamba_layers=2 --gnn_layers=2 --k_neighbors=8 --eval_steps=250 --save_steps=5000 --loss_type=cross_entropy --dropout=0.25 --weight_decay=0.01 --label_smoothing=0.1 --augment_noise=0.03 --augment_shift=2 --early_stopping=20\n",
      "‚úì Log file: logs/mamba_gnn_training_20260219_065732.log\n",
      "\n",
      "Monitoring training progress...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONFIGURATION (EstraNet-aligned)\n",
      "================================================================================\n",
      "  data_path             : data/ASCAD.h5\n",
      "  target_byte           : 2\n",
      "  input_length          : 700\n",
      "  do_train              : True\n",
      "  train_batch_size      : 256\n",
      "  eval_batch_size       : 32\n",
      "  train_steps           : 50000\n",
      "  iterations            : 500\n",
      "  eval_steps            : 250\n",
      "  save_steps            : 5000\n",
      "  max_eval_batch        : 312\n",
      "  learning_rate         : 0.0001\n",
      "  clip                  : 1.0\n",
      "  min_lr_ratio          : 0.004\n",
      "  warmup_steps          : 1000\n",
      "  d_model               : 64\n",
      "  mamba_layers          : 2\n",
      "  gnn_layers            : 2\n",
      "  k_neighbors           : 8\n",
      "  dropout               : 0.25\n",
      "  weight_decay          : 0.01\n",
      "  label_smoothing       : 0.1\n",
      "  early_stopping        : 20\n",
      "  augment_noise         : 0.03\n",
      "  augment_shift         : 2\n",
      "  loss_type             : cross_entropy\n",
      "  focal_gamma           : 2.5\n",
      "  focal_alpha           : 1.0\n",
      "  checkpoint_dir        : checkpoints/mamba_gnn_estranet\n",
      "  checkpoint_idx        : 0\n",
      "  warm_start            : False\n",
      "  result_path           : results/mamba_gnn\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MAMBA-GNN TRAINING (EstraNet-aligned configuration)\n",
      "================================================================================\n",
      "\n",
      "Device: cuda\n",
      "Loading ASCAD from: data/ASCAD.h5\n",
      "Training traces:  (50000, 700)\n",
      "Attack traces:    (10000, 700)\n",
      "Target byte:      2\n",
      "\n",
      "=== DATA DIAGNOSTIC ===\n",
      "X_train  mean: -0.0000   (should be ~0.0)\n",
      "X_train  std:  1.0000    (should be ~1.0)\n",
      "X_attack mean: 0.0184\n",
      "X_attack std:  1.0029\n",
      "y_train unique labels: 256\n",
      "y_train first 10: [ 38  47  38  18 100 188 146 199  32 204]\n",
      "Most common labels:  [(103, 244), (38, 239), (90, 234), (133, 232), (251, 231)]\n",
      "Least common labels: [(222, 164), (41, 163), (253, 162), (152, 161), (213, 154)]\n",
      "=== END DIAGNOSTIC ===\n",
      "\n",
      "Optimized Mamba-GNN:\n",
      "  d_model: 64, Mamba layers: 2, GNN layers: 2\n",
      "\n",
      "Model Parameters: 242,388\n",
      "Loss: CrossEntropyLoss  label_smoothing=0.1\n",
      "Gradient clip:  1.0\n",
      "Training batches per iteration: 196\n",
      "Total training steps:           50000\n",
      "Save checkpoints every:         5000 steps\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n",
      "\n",
      "=== FIRST BATCH DIAGNOSTIC ===\n",
      "data shape : torch.Size([256, 700])\n",
      "data mean  : -0.0033  (should be ~0)\n",
      "data std   : 0.9828   (should be ~1)\n",
      "target[:10]: [154, 118, 220, 102, 38, 35, 168, 102, 51, 236]\n",
      "output shape       : torch.Size([256, 256])\n",
      "output mean        : -0.0150\n",
      "output std         : 0.2475\n",
      "max prob (first 5) : [0.008185121230781078, 0.006607864983379841, 0.010584915988147259, 0.006684321444481611, 0.007136364001780748]\n",
      "uniform baseline   : 0.0039\n",
      "=== END FIRST BATCH DIAGNOSTIC ===\n",
      "\n",
      "Train batches[  196]                | loss  5.56\n",
      "Eval  batches[  312]                | loss  5.56\n",
      "‚òÖ New best model saved (eval_loss: 5.56)\n",
      "\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "[   500] | gnorm  0.96 lr  0.000050 | loss  5.56\n",
      "Train batches[  196]                | loss  5.54\n",
      "Eval  batches[  312]                | loss  5.55\n",
      "‚òÖ New best model saved (eval_loss: 5.55)\n",
      "\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "Train batches[  196]                | loss  5.54\n",
      "Eval  batches[  312]                | loss  5.55\n",
      "‚òÖ New best model saved (eval_loss: 5.55)\n",
      "\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "\n",
      "‚úì Checkpoint saved: best_model.pth\n",
      "[  1000] | gnorm  0.65 lr  0.000100 | loss  5.55\n",
      "Train batches[  196]                | loss  5.54\n",
      "Eval  batches[  312]                | loss  5.55\n",
      "Train batches[  196]                | loss  5.54\n",
      "Eval  batches[  312]                | loss  5.55\n",
      "[  1500] | gnorm  0.62 lr  0.000100 | loss  5.54\n",
      "Train batches[  196]                | loss  5.53\n",
      "Eval  batches[  312]                | loss  5.55\n",
      "Train batches[  196]                | loss  5.53\n",
      "Eval  batches[  312]                | loss  5.55\n",
      "[  2000] | gnorm  0.64 lr  0.000100 | loss  5.54\n",
      "Train batches[  196]                | loss  5.53\n",
      "Eval  batches[  312]                | loss  5.55\n",
      "Train batches[  196]                | loss  5.53\n",
      "Eval  batches[  312]                | loss  5.55\n",
      "[  2500] | gnorm  0.74 lr  0.000100 | loss  5.53\n",
      "Train batches[  196]                | loss  5.52\n",
      "Eval  batches[  312]                | loss  5.56\n",
      "Train batches[  196]                | loss  5.52\n",
      "Eval  batches[  312]                | loss  5.56\n",
      "[  3000] | gnorm  0.89 lr  0.000100 | loss  5.53\n",
      "Train batches[  196]                | loss  5.51\n",
      "Eval  batches[  312]                | loss  5.56\n",
      "Train batches[  196]                | loss  5.51\n",
      "Eval  batches[  312]                | loss  5.56\n",
      "[  3500] | gnorm  1.01 lr  0.000099 | loss  5.52\n",
      "Train batches[  196]                | loss  5.50\n",
      "Eval  batches[  312]                | loss  5.56\n",
      "Train batches[  196]                | loss  5.50\n",
      "Eval  batches[  312]                | loss  5.57\n",
      "[  4000] | gnorm  1.19 lr  0.000099 | loss  5.51\n",
      "Train batches[  196]                | loss  5.49\n",
      "Eval  batches[  312]                | loss  5.57\n",
      "Train batches[  196]                | loss  5.49\n",
      "Eval  batches[  312]                | loss  5.57\n",
      "[  4500] | gnorm  1.28 lr  0.000099 | loss  5.51\n",
      "Train batches[  196]                | loss  5.48\n",
      "Eval  batches[  312]                | loss  5.57\n",
      "Train batches[  196]                | loss  5.48\n",
      "Eval  batches[  312]                | loss  5.57\n",
      "[  5000] | gnorm  1.42 lr  0.000098 | loss  5.50\n",
      "Train batches[  196]                | loss  5.47\n",
      "Eval  batches[  312]                | loss  5.58\n",
      "\n",
      "‚úì Checkpoint saved: mamba_gnn-5000.pth\n",
      "Model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-5000.pth\n",
      "\n",
      "‚úì Checkpoint saved: checkpoint_latest.pth\n",
      "Train batches[  196]                | loss  5.47\n",
      "\n",
      "‚úì Checkpoint saved: checkpoint_latest.pth\n",
      "Eval  batches[  312]                | loss  5.58\n",
      "[  5500] | gnorm  1.40 lr  0.000098 | loss  5.49\n",
      "Train batches[  196]                | loss  5.46\n",
      "Eval  batches[  312]                | loss  5.58\n",
      "Train batches[  196]                | loss  5.46\n",
      "Eval  batches[  312]                | loss  5.58\n",
      "\n",
      "‚ö† Early stopping triggered at step 5750\n",
      "  Best eval loss: 5.55\n",
      "Final model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-50000.pth\n",
      "\n",
      "‚úì Checkpoint saved: mamba_gnn-50000.pth\n",
      "\n",
      "================================================================================\n",
      "Training completed!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "‚úì Training completed successfully!\n",
      "================================================================================\n",
      "\n",
      "‚úì Training log saved to: logs/mamba_gnn_training_20260219_065732.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys, os, shutil, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "IS_FOCAL = config['loss_function'] in (\"focal\", \"focal_loss\")\n",
    "\n",
    "# ---- Determine checkpoint dir and warm-start ----\n",
    "if IS_FOCAL:\n",
    "    ce_best = Path(CHECKPOINT_DIR) / 'best_model.pth'\n",
    "    focal_dir = Path(FOCAL_CKPT_DIR)\n",
    "    focal_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if ce_best.exists():\n",
    "        shutil.copy2(ce_best, focal_dir / 'checkpoint_latest.pth')\n",
    "        print(f\"‚úì Copied {ce_best} ‚Üí {focal_dir}/checkpoint_latest.pth\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  CE best_model not found at {ce_best}\")\n",
    "        print(\"   Run Phase 1 (cross_entropy) first, then switch LOSS_FUNCTION.\")\n",
    "        raise SystemExit(\"Phase 1 checkpoint missing ‚Äî cannot start Phase 2.\")\n",
    "    active_ckpt_dir = str(FOCAL_CKPT_DIR)\n",
    "    config['checkpoint_dir'] = active_ckpt_dir\n",
    "else:\n",
    "    active_ckpt_dir = CHECKPOINT_DIR\n",
    "    config['checkpoint_dir'] = active_ckpt_dir\n",
    "\n",
    "# ---- Keys handled manually (not auto-passed from config dict) ----\n",
    "MANUAL_KEYS = {'loss_config', 'iterations', 'loss_function',\n",
    "               'clip', 'warmup_steps', 'focal_gamma', 'focal_alpha',\n",
    "               'no_patch_embed', 'use_ssm_mamba'}\n",
    "\n",
    "train_cmd_parts = [\"python\", \"mamba-gnn-scripts/train_mamba_gnn.py\", \"--do_train\"]\n",
    "\n",
    "if IS_FOCAL:\n",
    "    train_cmd_parts.append(\"--warm_start\")\n",
    "\n",
    "for key, value in config.items():\n",
    "    if key in MANUAL_KEYS:\n",
    "        continue\n",
    "    if value is not None:\n",
    "        train_cmd_parts.append(f\"--{key}={value}\")\n",
    "\n",
    "# Always pass these explicitly\n",
    "train_cmd_parts.append(f\"--clip={config['clip']}\")\n",
    "train_cmd_parts.append(f\"--warmup_steps={config['warmup_steps']}\")\n",
    "train_cmd_parts.append(f\"--loss_type={config['loss_function']}\")\n",
    "\n",
    "# Pass boolean flags when enabled\n",
    "if config.get('no_patch_embed'):\n",
    "    train_cmd_parts.append(\"--no_patch_embed\")\n",
    "if config.get('use_ssm_mamba'):\n",
    "    train_cmd_parts.append(\"--use_ssm_mamba\")\n",
    "\n",
    "if IS_FOCAL:\n",
    "    train_cmd_parts.append(f\"--focal_gamma={config['focal_gamma']}\")\n",
    "    train_cmd_parts.append(f\"--focal_alpha={config['focal_alpha']}\")\n",
    "\n",
    "train_cmd = \" \".join(train_cmd_parts)\n",
    "\n",
    "# ---- Logging ----\n",
    "log_dir = Path(\"logs\")\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "phase_tag = \"phase2_focal\" if IS_FOCAL else \"mamba_ssm_ce\"\n",
    "log_file = log_dir / f\"mamba_{phase_tag}_{timestamp}.log\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'PHASE 2 ‚Äî Focal Finetune' if IS_FOCAL else 'PHASE 1 ‚Äî Real Mamba S6 CE Training'}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Command: {train_cmd}\")\n",
    "print(f\"Log: {log_file}\\n\")\n",
    "\n",
    "checkpoint_dir = Path(active_ckpt_dir)\n",
    "last_ckpt_time = time.time()\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(f\"Command: {train_cmd}\\n\\n\")\n",
    "    for k, v in config.items():\n",
    "        if k != 'loss_config':\n",
    "            f.write(f\"  {k}: {v}\\n\")\n",
    "    f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        \"python -u \" + \" \".join(train_cmd_parts[1:]),\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        line = process.stdout.readline()\n",
    "        if line:\n",
    "            print(line.rstrip())\n",
    "            f.write(line)\n",
    "            f.flush()\n",
    "        if process.poll() is not None:\n",
    "            break\n",
    "        if checkpoint_dir.exists():\n",
    "            ckpts = list(checkpoint_dir.glob(\"*.pth\"))\n",
    "            if ckpts:\n",
    "                latest = max(ckpts, key=os.path.getctime)\n",
    "                t = os.path.getctime(latest)\n",
    "                if t > last_ckpt_time:\n",
    "                    print(f\"\\n‚úì Checkpoint saved: {latest.name}\")\n",
    "                    last_ckpt_time = t\n",
    "\n",
    "    rc = process.returncode\n",
    "    status = \"‚úì Training completed!\" if rc == 0 else f\"‚úó Training failed (exit {rc})\"\n",
    "    print(\"\\n\" + status)\n",
    "    f.write(\"\\n\" + status + \"\\n\")\n",
    "\n",
    "print(f\"\\nLog saved: {log_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c806809",
   "metadata": {
    "id": "8c806809"
   },
   "source": [
    "### Fix `KeyError` in Training Script\n",
    "\n",
    "The previous training run failed because the `loss_history` dictionary was not properly initialized before attempting to log metrics for `global_step=250`. The following patch modifies the `train_mamba_gnn.py` script to ensure that `loss_history[global_step]` is always a dictionary before its `update()` method is called, preventing the `KeyError`.\n",
    "\n",
    "This change uses `loss_history.setdefault(global_step, {})`, which will get the dictionary associated with `global_step` if it exists, or create an empty dictionary and associate it with `global_step` if it doesn't.\n",
    "\n",
    "After applying this patch, you can re-run the training cell (`cell_0007`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf3da8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdf3da8f",
    "outputId": "e36c0b02-8e38-48ab-f55e-42cfccae0ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching script: /content/EstraNet/mamba-gnn-scripts/train_mamba_gnn.py using Python...\n",
      "‚ö† Pattern 'loss_history[global_step].update({' not found on line 396.\n",
      "   The line might be different than expected, or already patched.\n",
      "‚úó Patching operation completed but no changes were applied.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "script_to_patch = '/content/EstraNet/mamba-gnn-scripts/train_mamba_gnn.py'\n",
    "\n",
    "if not os.path.exists(script_to_patch):\n",
    "    print(f\"Error: Script not found at {script_to_patch}\")\n",
    "else:\n",
    "    print(f\"Patching script: {script_to_patch} using Python...\")\n",
    "    try:\n",
    "        with open(script_to_patch, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        patched = False\n",
    "        target_line_num = 396 # 1-indexed\n",
    "\n",
    "        if target_line_num <= len(lines):\n",
    "            line_index = target_line_num - 1 # 0-indexed\n",
    "            current_line = lines[line_index]\n",
    "\n",
    "            # Look for the specific pattern to replace\n",
    "            pattern_to_find = 'loss_history[global_step].update({' # Exact pattern from traceback\n",
    "            if pattern_to_find in current_line:\n",
    "                # Replace it, preserving leading whitespace\n",
    "                leading_whitespace = current_line[:current_line.find(pattern_to_find)]\n",
    "                new_content = leading_whitespace + 'loss_history.setdefault(global_step, {}).update({' # The fix\n",
    "                lines[line_index] = new_content + '\\n' # Ensure newline is preserved\n",
    "                patched = True\n",
    "                print(f\"‚úì Line {target_line_num} patched successfully.\")\n",
    "            else:\n",
    "                print(f\"‚ö† Pattern '{pattern_to_find}' not found on line {target_line_num}.\")\n",
    "                print(\"   The line might be different than expected, or already patched.\")\n",
    "        else:\n",
    "            print(f\"‚ö† Line {target_line_num} does not exist in the script.\")\n",
    "\n",
    "\n",
    "        if patched:\n",
    "            with open(script_to_patch, 'w') as f:\n",
    "                f.writelines(lines)\n",
    "            print(\"‚úì Script changes saved!\")\n",
    "            print(\"You can now re-run the training cell (cell_0007).\")\n",
    "        else:\n",
    "            print(\"‚úó Patching operation completed but no changes were applied.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó An unexpected error occurred during patching: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5970ac",
   "metadata": {
    "id": "ce5970ac"
   },
   "source": [
    "## Monitor Training Progress in Real-Time\n",
    "\n",
    "Run this cell **in a separate terminal** while training is running to watch checkpoints being saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d79605",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59d79605",
    "outputId": "3a80d433-f431-462d-bb9f-2af636ab1d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Progress Monitor\n",
      "================================================================================\n",
      "\n",
      "Watching: checkpoints/mamba_gnn_estranet\n",
      "Press Ctrl+C to stop monitoring\n",
      "\n",
      "‚úì [06:58:35] best_model.pth (1.0 MB)\n",
      "‚úì [07:03:04] checkpoint_latest.pth (2.9 MB)\n",
      "‚úì [07:03:04] mamba_gnn-5000.pth (2.9 MB)\n",
      "‚úì [07:03:51] mamba_gnn-50000.pth (2.9 MB)\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n",
      "\n",
      "‚úì Total checkpoints: 4\n",
      "‚úì Total size: 9.5 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoint_dir = Path(config['checkpoint_dir'])\n",
    "previous_files = set()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Training Progress Monitor\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nWatching: {checkpoint_dir}\")\n",
    "print(\"Press Ctrl+C to stop monitoring\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if checkpoint_dir.exists():\n",
    "            # Get all checkpoint files\n",
    "            current_files = set(checkpoint_dir.glob(\"*.pth\"))\n",
    "            new_files = current_files - previous_files\n",
    "\n",
    "            # Show new checkpoints\n",
    "            for ckpt_file in sorted(new_files):\n",
    "                file_size_mb = ckpt_file.stat().st_size / (1024 * 1024)\n",
    "                timestamp = datetime.fromtimestamp(ckpt_file.stat().st_mtime).strftime(\"%H:%M:%S\")\n",
    "                print(f\"‚úì [{timestamp}] {ckpt_file.name} ({file_size_mb:.1f} MB)\")\n",
    "\n",
    "            previous_files = current_files\n",
    "\n",
    "            # Check total size\n",
    "            if current_files:\n",
    "                total_size = sum(f.stat().st_size for f in current_files) / (1024 * 1024)\n",
    "                print(f\"\\n‚úì Total checkpoints: {len(current_files)}\")\n",
    "                print(f\"‚úì Total size: {total_size:.1f} MB\")\n",
    "            else:\n",
    "                print(\"‚è≥ Waiting for first checkpoint...\")\n",
    "        else:\n",
    "            print(\"‚è≥ Checkpoint directory not created yet...\")\n",
    "\n",
    "        # Wait before checking again\n",
    "        time.sleep(5)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚úì Monitoring stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78a7af",
   "metadata": {
    "id": "cc78a7af"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoint_dir = Path(config['checkpoint_dir'])\n",
    "previous_files = set()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Training Progress Monitor\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nWatching: {checkpoint_dir}\")\n",
    "print(\"Press Ctrl+C to stop monitoring\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if checkpoint_dir.exists():\n",
    "            # Get all checkpoint files\n",
    "            current_files = set(checkpoint_dir.glob(\"*.pth\"))\n",
    "            new_files = current_files - previous_files\n",
    "\n",
    "            # Show new checkpoints\n",
    "            for ckpt_file in sorted(new_files):\n",
    "                file_size_mb = ckpt_file.stat().st_size / (1024 * 1024)\n",
    "                timestamp = datetime.fromtimestamp(ckpt_file.stat().st_mtime).strftime(\"%H:%M:%S\")\n",
    "                print(f\"‚úì [{timestamp}] {ckpt_file.name} ({file_size_mb:.1f} MB)\")\n",
    "\n",
    "            previous_files = current_files\n",
    "\n",
    "            # Check total size\n",
    "            total_size = sum(f.stat().st_size for f in current_files) / (1024 * 1024)\n",
    "            print(f\"\\nTotal checkpoints size: {total_size:.1f} MB\")\n",
    "            print(f\"Number of checkpoints: {len(current_files)}\")\n",
    "        else:\n",
    "            print(\"‚ö† Checkpoint directory not found yet...\")\n",
    "\n",
    "        # Wait before checking again\n",
    "        time.sleep(5)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚úì Monitoring stopped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0008",
   "metadata": {
    "id": "cell_0008"
   },
   "source": [
    "## Option 2: Train Using TensorFlow (for TFLite Conversion)\n",
    "\n",
    "If you need to deploy on mobile/edge devices, use the TensorFlow version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0009",
   "metadata": {
    "id": "cell_0009"
   },
   "outputs": [],
   "source": [
    "# # TensorFlow training command\n",
    "# tf_train_cmd = f\"\"\"python scripts/train_mamba_gnn_tf.py \\\n",
    "#     --data_path={config['data_path']} \\\n",
    "#     --checkpoint_dir=checkpoints/mamba_gnn_tf \\\n",
    "#     --target_byte={config['target_byte']} \\\n",
    "#     --train_batch_size={config['train_batch_size']} \\\n",
    "#     --eval_batch_size={config['eval_batch_size']} \\\n",
    "#     --train_steps={config['train_steps']} \\\n",
    "#     --learning_rate={config['learning_rate']} \\\n",
    "#     --d_model={config['d_model']} \\\n",
    "#     --do_train\n",
    "# \"\"\"\n",
    "\n",
    "# print(\"TensorFlow training command:\")\n",
    "# print(tf_train_cmd)\n",
    "# print(\"\\n‚úì This will automatically export to TFLite after training\")\n",
    "# print(\"  Output: checkpoints/mamba_gnn_tf/mamba_gnn.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0010",
   "metadata": {
    "id": "cell_0010"
   },
   "outputs": [],
   "source": [
    "# Execute TensorFlow training (uncomment to run)\n",
    "# import subprocess\n",
    "# result = subprocess.run(tf_train_cmd.split(), capture_output=False)\n",
    "# print(f\"‚úì Training complete. TFLite model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0011",
   "metadata": {
    "id": "cell_0011"
   },
   "source": [
    "## Monitor Training Progress\n",
    "\n",
    "View training loss, learning rate, and gradient norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0012",
   "metadata": {
    "id": "cell_0012"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Load training history\n",
    "loss_path = Path(CHECKPOINT_DIR) / 'loss.pkl'\n",
    "\n",
    "if loss_path.exists():\n",
    "    with open(loss_path, 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "\n",
    "    steps = sorted(history.keys())\n",
    "    train_losses = [history[s]['train_loss'] for s in steps if 'train_loss' in history[s]]\n",
    "    lrs = [history[s]['lr'] for s in steps if 'lr' in history[s]]\n",
    "    grad_norms = [history[s]['grad_norm'] for s in steps if 'grad_norm' in history[s]]\n",
    "\n",
    "    # Plot training metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Training loss\n",
    "    axes[0].plot(steps[:len(train_losses)], train_losses, 'b-', linewidth=2)\n",
    "    axes[0].set_xlabel('Training Steps', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss (Cross-Entropy)', fontsize=12)\n",
    "    axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning rate\n",
    "    axes[1].plot(steps[:len(lrs)], lrs, 'g-', linewidth=2)\n",
    "    axes[1].set_xlabel('Training Steps', fontsize=12)\n",
    "    axes[1].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[1].set_title('Learning Rate Schedule (Cosine Decay)', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Gradient norm\n",
    "    axes[2].plot(steps[:len(grad_norms)], grad_norms, 'r-', linewidth=2)\n",
    "    axes[2].set_xlabel('Training Steps', fontsize=12)\n",
    "    axes[2].set_ylabel('Gradient Norm', fontsize=12)\n",
    "    axes[2].set_title('Gradient Norm (Clipped at 0.25)', fontsize=14, fontweight='bold')\n",
    "    axes[2].axhline(y=0.25, color='orange', linestyle='--', label='Clip threshold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULT_DIR}/training_progress.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n‚úì Training Progress:\")\n",
    "    print(f\"  Total steps: {max(steps):,}\")\n",
    "    print(f\"  Latest loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Latest LR: {lrs[-1]:.6f}\")\n",
    "    print(f\"  Latest grad norm: {grad_norms[-1]:.4f}\")\n",
    "    print(f\"\\n  Plot saved: {RESULT_DIR}/training_progress.png\")\n",
    "else:\n",
    "    print(\"‚ö† No training history found. Start training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0013",
   "metadata": {
    "id": "cell_0013"
   },
   "source": [
    "## Evaluate Model (Guessing Entropy)\n",
    "\n",
    "Evaluate using 100-trial Guessing Entropy methodology (matches EstraNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0014",
   "metadata": {
    "id": "cell_0014"
   },
   "outputs": [],
   "source": [
    "# Evaluation configuration\n",
    "checkpoint_idx = 100000  # Which checkpoint to evaluate (0 = latest)\n",
    "\n",
    "eval_cmd = f\"\"\"python mamba-gnn-scripts/train_mamba_gnn.py \\\n",
    "    --data_path={config['data_path']} \\\n",
    "    --checkpoint_dir={config['checkpoint_dir']} \\\n",
    "    --target_byte={config['target_byte']} \\\n",
    "    --d_model={config['d_model']} \\\n",
    "    --mamba_layers={config['mamba_layers']} \\\n",
    "    --gnn_layers={config['gnn_layers']} \\\n",
    "    --k_neighbors={config['k_neighbors']} \\\n",
    "    --dropout={config['dropout']} \\\n",
    "    --checkpoint_idx={checkpoint_idx} \\\n",
    "    --result_path={RESULT_DIR}/mamba_gnn_eval\n",
    "\"\"\"\n",
    "\n",
    "print(\"Evaluation command:\")\n",
    "print(eval_cmd)\n",
    "print(\"\\n‚úì This will compute 100-trial Guessing Entropy\")\n",
    "print(\"  (Takes ~10-15 minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0015",
   "metadata": {
    "id": "cell_0015"
   },
   "outputs": [],
   "source": [
    "# Execute evaluation\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Evaluating Mamba-GNN (100-trial Guessing Entropy)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = subprocess.run(eval_cmd.split(), capture_output=False)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úì Evaluation complete!\")\n",
    "else:\n",
    "    print(\"\\n‚úó Evaluation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0016",
   "metadata": {
    "id": "cell_0016"
   },
   "source": [
    "## Plot Guessing Entropy Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0017",
   "metadata": {
    "id": "cell_0017"
   },
   "outputs": [],
   "source": [
    "# Load and plot evaluation results\n",
    "result_file = f'{RESULT_DIR}/mamba_gnn_eval.txt'\n",
    "\n",
    "if Path(result_file).exists():\n",
    "    with open(result_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        mean_ranks = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
    "        std_ranks = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
    "\n",
    "    traces = np.arange(1, len(mean_ranks) + 1)\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Main line\n",
    "    plt.plot(traces, mean_ranks, 'b-', linewidth=2.5, label='Mean Key Rank (GE)')\n",
    "\n",
    "    # Confidence interval\n",
    "    plt.fill_between(traces,\n",
    "                     mean_ranks - std_ranks,\n",
    "                     mean_ranks + std_ranks,\n",
    "                     alpha=0.3, color='blue', label='¬±1 Std Dev')\n",
    "\n",
    "    # Key recovered line\n",
    "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Key Recovered (Rank=0)')\n",
    "\n",
    "    # Labels and formatting\n",
    "    plt.xlabel('Number of Traces', fontsize=14)\n",
    "    plt.ylabel('Key Rank (Guessing Entropy)', fontsize=14)\n",
    "    plt.title('Mamba-GNN: Guessing Entropy Evaluation (100 trials)\\nEstraNet-Aligned Configuration',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12, loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add milestones\n",
    "    milestones = [100, 500, 1000, 2000, 5000]\n",
    "    for m in milestones:\n",
    "        if m < len(mean_ranks):\n",
    "            plt.axvline(x=m, color='gray', linestyle=':', alpha=0.5)\n",
    "            plt.text(m, plt.ylim()[1] * 0.95, f'{m}',\n",
    "                    ha='center', fontsize=9, color='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULT_DIR}/guessing_entropy_curve.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"=\"*80)\n",
    "    print(\"GUESSING ENTROPY RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTarget byte: {config['target_byte']}\")\n",
    "    print(f\"\\nKey Rank (Mean ¬± Std):\")\n",
    "    print(f\"  100 traces:   {mean_ranks[99]:.2f} ¬± {std_ranks[99]:.2f}\")\n",
    "    print(f\"  500 traces:   {mean_ranks[499]:.2f} ¬± {std_ranks[499]:.2f}\")\n",
    "    print(f\"  1000 traces:  {mean_ranks[999]:.2f} ¬± {std_ranks[999]:.2f}\")\n",
    "    if len(mean_ranks) >= 2000:\n",
    "        print(f\"  2000 traces:  {mean_ranks[1999]:.2f} ¬± {std_ranks[1999]:.2f}\")\n",
    "    if len(mean_ranks) >= 5000:\n",
    "        print(f\"  5000 traces:  {mean_ranks[4999]:.2f} ¬± {std_ranks[4999]:.2f}\")\n",
    "    if len(mean_ranks) >= 10000:\n",
    "        print(f\"  10000 traces: {mean_ranks[9999]:.2f} ¬± {std_ranks[9999]:.2f}\")\n",
    "\n",
    "    # Find recovery point\n",
    "    recovered_idx = np.where(mean_ranks == 0)[0]\n",
    "    if len(recovered_idx) > 0:\n",
    "        print(f\"\\n‚úì Key RECOVERED at {recovered_idx[0]+1} traces\")\n",
    "    else:\n",
    "        print(f\"\\n‚úó Key NOT recovered (best rank: {mean_ranks[-1]:.2f})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Plot saved: {RESULT_DIR}/guessing_entropy_curve.png\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö† No evaluation results found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0018",
   "metadata": {
    "id": "cell_0018"
   },
   "source": [
    "## Compare with EstraNet\n",
    "\n",
    "Compare Mamba-GNN with EstraNet Transformer/GNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0019",
   "metadata": {
    "id": "cell_0019"
   },
   "outputs": [],
   "source": [
    "# Compare with EstraNet results\n",
    "estranet_result = 'results/estranet_transformer_eval.txt'  # Update this path\n",
    "\n",
    "compare_cmd = f\"\"\"python scripts/compare_results.py \\\n",
    "    --mamba_results={result_file} \\\n",
    "    --estranet_results={estranet_result} \\\n",
    "    --output={RESULT_DIR}/model_comparison.png\n",
    "\"\"\"\n",
    "\n",
    "print(\"Comparison command:\")\n",
    "print(compare_cmd)\n",
    "print(\"\\n‚úì Make sure you have EstraNet results first:\")\n",
    "print(\"  python scripts/train_trans.py --model_type=transformer --do_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0020",
   "metadata": {
    "id": "cell_0020"
   },
   "outputs": [],
   "source": [
    "# Execute comparison (uncomment when EstraNet results are ready)\n",
    "# import subprocess\n",
    "# result = subprocess.run(compare_cmd.split())\n",
    "# print(\"‚úì Comparison plot created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0021",
   "metadata": {
    "id": "cell_0021"
   },
   "source": [
    "## Load and Use Trained Model\n",
    "\n",
    "Load checkpoint for inference or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0022",
   "metadata": {
    "id": "cell_0022"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to sys.path to allow absolute imports from 'models'\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import model using absolute path from the models directory\n",
    "# This assumes the mamba_gnn_model.py, mamba_block.py, etc., are in the 'models' folder\n",
    "# and that 'EstraNet' is the working directory.\n",
    "# The relative imports within mamba_gnn_model.py will be fixed by the patch below.\n",
    "from models.mamba_gnn_model import OptimizedMambaGNN\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Patch mamba_gnn_model.py for relative import error ---\n",
    "script_to_patch = 'models/mamba_gnn_model.py'\n",
    "if os.path.exists(script_to_patch):\n",
    "    print(f\"Patching imports in {script_to_patch}...\")\n",
    "    with open(script_to_patch, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Replace relative imports with absolute imports within the 'models' context\n",
    "    content = content.replace('from .mamba_block import OptimizedMambaBlock', 'from models.mamba_block import OptimizedMambaBlock')\n",
    "    content = content.replace('from .gat_layer import EnhancedGAT', 'from models.gat_layer import EnhancedGAT')\n",
    "    content = content.replace('from .patch_embedding import CNNPatchEmbedding', 'from models.patch_embedding import CNNPatchEmbedding')\n",
    "\n",
    "    with open(script_to_patch, 'w') as f:\n",
    "        f.write(content)\n",
    "    print(\"‚úì Imports patched successfully!\")\n",
    "else:\n",
    "    print(f\"‚ö† Warning: Model script not found at {script_to_patch}. Cannot patch imports.\")\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create model (EstraNet-aligned configuration)\n",
    "model = OptimizedMambaGNN(\n",
    "    trace_length=700,\n",
    "    d_model=config['d_model'],\n",
    "    mamba_layers=config['mamba_layers'],\n",
    "    gnn_layers=config['gnn_layers'],\n",
    "    num_classes=256,\n",
    "    k_neighbors=config['k_neighbors'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = f\"{CHECKPOINT_DIR}/mamba_gnn-100000.pth\"\n",
    "\n",
    "if Path(ckpt_path).exists():\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"\\n‚úì Model loaded successfully\")\n",
    "    print(f\"  Checkpoint: {ckpt_path}\")\n",
    "    print(f\"  Training step: {checkpoint['global_step']:,}\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Model ready for inference\")\n",
    "else:\n",
    "    print(f\"‚ö† Checkpoint not found: {ckpt_path}\")\n",
    "    print(\"  Train the model first or specify different checkpoint_idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fc456",
   "metadata": {
    "id": "300fc456"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Define paths\n",
    "checkpoint_folder = 'checkpoints/mamba_gnn_estranet'\n",
    "output_filename = 'mamba_gnn_estranet_checkpoints'\n",
    "\n",
    "# Check if folder exists\n",
    "if os.path.exists(checkpoint_folder):\n",
    "    print(f\"Zipping {checkpoint_folder}...\")\n",
    "\n",
    "    # Create zip file\n",
    "    shutil.make_archive(output_filename, 'zip', checkpoint_folder)\n",
    "\n",
    "    # Download\n",
    "    zip_path = f\"{output_filename}.zip\"\n",
    "    print(f\"Downloading {zip_path} ({os.path.getsize(zip_path) / 1e6:.1f} MB)...\")\n",
    "    files.download(zip_path)\n",
    "else:\n",
    "    print(f\"Folder not found: {checkpoint_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0023",
   "metadata": {
    "id": "cell_0023"
   },
   "outputs": [],
   "source": [
    "# Example: Get predictions on attack traces\n",
    "if Path(ckpt_path).exists() and Path(DATA_PATH).exists():\n",
    "    print(\"Running inference on sample traces...\")\n",
    "\n",
    "    # Load attack data\n",
    "    with h5py.File(DATA_PATH, 'r') as f:\n",
    "        X_attack = f['Attack_traces/traces'][:100]  # First 100 traces\n",
    "        m_attack = f['Attack_traces/metadata'][:]\n",
    "\n",
    "    # Normalize (using same scaler as training)\n",
    "    with h5py.File(DATA_PATH, 'r') as f:\n",
    "        X_train_sample = f['Profiling_traces/traces'][:1000]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_sample)\n",
    "    X_attack_norm = scaler.transform(X_attack)\n",
    "\n",
    "    # Get predictions\n",
    "    X_tensor = torch.FloatTensor(X_attack_norm).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "    predicted_classes = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "    print(f\"\\n‚úì Inference complete\")\n",
    "    print(f\"  Traces processed: {len(X_attack)}\")\n",
    "    print(f\"  Prediction shape: {probs.shape}\")\n",
    "    print(f\"  Top predicted classes: {predicted_classes[:10]}\")\n",
    "    print(f\"  Confidence (first trace): {probs[0].max().item():.4f}\")\n",
    "else:\n",
    "    print(\"‚ö† Model or data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0024",
   "metadata": {
    "id": "cell_0024"
   },
   "source": [
    "## Summary & Configuration Verification\n",
    "\n",
    "**‚úÖ This notebook uses the CORRECT configuration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0025",
   "metadata": {
    "id": "cell_0025"
   },
   "outputs": [],
   "source": [
    "# Verify configuration matches EstraNet\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "verification = {\n",
    "    'Loss Function': ('Cross-Entropy', '‚úì'),\n",
    "    'Learning Rate': ('2.5e-4', '‚úì'),\n",
    "    'Train Batch Size': ('256', '‚úì'),\n",
    "    'Eval Batch Size': ('32', '‚úì'),\n",
    "    'Training Steps': ('100,000', '‚úì'),\n",
    "    'Optimizer': ('Adam', '‚úì'),\n",
    "    'LR Schedule': ('Cosine Decay', '‚úì'),\n",
    "    'Model Dimension': ('128', '‚úì'),\n",
    "    'Dropout': ('0.1', '‚úì'),\n",
    "    'Gradient Clipping': ('0.25', '‚úì'),\n",
    "    'Evaluation Method': ('100-trial GE', '‚úì'),\n",
    "}\n",
    "\n",
    "print(\"\\nEstraNet Alignment Check:\")\n",
    "for param, (value, status) in verification.items():\n",
    "    print(f\"  {status} {param:25s}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì ALL PARAMETERS MATCH ESTRANET\")\n",
    "print(\"‚úì FAIR COMPARISON GUARANTEED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è DO NOT USE: final_best_gnn_mamba_teacher.ipynb\")\n",
    "print(\"   That notebook has incompatible configuration:\")\n",
    "print(\"   - FocalLoss instead of Cross-Entropy\")\n",
    "print(\"   - Learning rate 8x too high\")\n",
    "print(\"   - Batch size 4x too small\")\n",
    "print(\"   - Single-trial evaluation instead of 100-trial GE\")\n",
    "\n",
    "print(\"\\n‚úì USE THIS NOTEBOOK for training and evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0026",
   "metadata": {
    "id": "cell_0026"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "### 1. Train Model\n",
    "Run the training cell above to start training with correct configuration.\n",
    "\n",
    "### 2. Monitor Progress\n",
    "Check training loss, learning rate, and gradient norms periodically.\n",
    "\n",
    "### 3. Evaluate Model\n",
    "After training, run evaluation to get Guessing Entropy curves.\n",
    "\n",
    "### 4. Compare with EstraNet\n",
    "Train EstraNet models and compare results using the comparison script.\n",
    "\n",
    "### 5. Expected Results\n",
    "- **After 100k steps**: Loss ~4.8-5.0\n",
    "- **Key recovery**: Within 1000-2000 traces\n",
    "- **Performance**: Comparable to EstraNet Transformer (~1200 traces)\n",
    "\n",
    "### Files Created\n",
    "- **Training script**: `mamba-gnn-scripts/train_mamba_gnn.py`\n",
    "- **TensorFlow script**: `scripts/train_mamba_gnn_tf.py`\n",
    "- **PowerShell runner**: `mamba-gnn-scripts/train_mamba_gnn.ps1`\n",
    "- **Bash runner**: `mamba-gnn-scripts/train_mamba_gnn.sh`\n",
    "- **Comparison tool**: `scripts/compare_results.py`\n",
    "- **This notebook**: `train_mamba_gnn_notebook.ipynb`\n",
    "\n",
    "### Documentation\n",
    "- **Quick start**: `QUICKSTART.md`\n",
    "- **TFLite guide**: `TENSORFLOW_TFLITE_GUIDE.md`\n",
    "- **TF implementation**: `TENSORFLOW_IMPLEMENTATION_SUMMARY.md`\n",
    "- **Config comparison**: `NOTEBOOK_VS_SCRIPT_COMPARISON.md`\n",
    "\n",
    "**Ready to train with fair comparison to EstraNet!** üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070cd09",
   "metadata": {
    "id": "b070cd09"
   },
   "source": [
    "## Compare Both Phases for Report\n",
    "\n",
    "After completing Phase 1Ô∏è‚É£ and Phase 2Ô∏è‚É£, use this to compare results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98c964",
   "metadata": {
    "id": "1e98c964"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load results from both phases\n",
    "phase1_result = f'{RESULT_DIR}/cross_entropy_eval.txt'\n",
    "phase2_result = f'{RESULT_DIR}/focal_loss_eval.txt'\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Phase 1: Fair Comparison\n",
    "if Path(phase1_result).exists():\n",
    "    with open(phase1_result, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        phase1_mean = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
    "        phase1_std = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
    "    results['Phase 1 (Cross-Entropy)'] = phase1_mean\n",
    "    print(\"‚úì Phase 1 results loaded\")\n",
    "else:\n",
    "    print(\"‚ö† Phase 1 results not found. Train Phase 1 first.\")\n",
    "\n",
    "# Phase 2: Performance\n",
    "if Path(phase2_result).exists():\n",
    "    with open(phase2_result, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        phase2_mean = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
    "        phase2_std = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
    "    results['Phase 2 (FocalLoss)'] = phase2_mean\n",
    "    print(\"‚úì Phase 2 results loaded\")\n",
    "else:\n",
    "    print(\"‚ö† Phase 2 results not found. Train Phase 2 first.\")\n",
    "\n",
    "# Compare key milestones\n",
    "if len(results) == 2:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON TABLE FOR REPORT\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    milestones = [100, 500, 1000, 2000, 5000]\n",
    "\n",
    "    print(f\"\\n{'Traces':<10} {'Phase 1 (CE)':<20} {'Phase 2 (Focal)':<20} {'Improvement'}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for m in milestones:\n",
    "        if m <= len(phase1_mean):\n",
    "            p1_rank = phase1_mean[m-1]\n",
    "            p2_rank = phase2_mean[m-1]\n",
    "            improvement = ((p1_rank - p2_rank) / (p1_rank + 1e-6)) * 100\n",
    "\n",
    "            print(f\"{m:<10} {p1_rank:<20.2f} {p2_rank:<20.2f} {improvement:+.1f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    # Find recovery points\n",
    "    p1_recovery = np.where(phase1_mean == 0)[0]\n",
    "    p2_recovery = np.where(phase2_mean == 0)[0]\n",
    "\n",
    "    print(\"\\nKEY RECOVERY SUMMARY:\")\n",
    "    if len(p1_recovery) > 0:\n",
    "        print(f\"  Phase 1 (Cross-Entropy): {p1_recovery[0]+1} traces\")\n",
    "    else:\n",
    "        print(f\"  Phase 1 (Cross-Entropy): No recovery (best: {phase1_mean[-1]:.2f})\")\n",
    "\n",
    "    if len(p2_recovery) > 0:\n",
    "        print(f\"  Phase 2 (FocalLoss):     {p2_recovery[0]+1} traces\")\n",
    "    else:\n",
    "        print(f\"  Phase 2 (FocalLoss):     No recovery (best: {phase2_mean[-1]:.2f})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    # Plot comparison\n",
    "    traces = np.arange(1, min(len(phase1_mean), len(phase2_mean)) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(traces, phase1_mean[:len(traces)], 'b-', linewidth=2.5, label='Phase 1: Cross-Entropy (Fair)')\n",
    "    plt.plot(traces, phase2_mean[:len(traces)], 'r-', linewidth=2.5, label='Phase 2: FocalLoss (Optimized)')\n",
    "\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    plt.xlabel('Number of Traces', fontsize=14)\n",
    "    plt.ylabel('Key Rank (Guessing Entropy)', fontsize=14)\n",
    "    plt.title('Mamba-GNN Comparison: Fair vs Optimized\\nfor Report', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12, loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULT_DIR}/phase_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Comparison plot saved: {RESULT_DIR}/phase_comparison.png\")\n",
    "else:\n",
    "    print(\"‚ö† Complete both Phase 1 and Phase 2 to see comparison\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell_0000",
      "metadata": {
        "id": "cell_0000"
      },
      "source": [
        "# Mamba-GNN Training Notebook (EstraNet-Aligned Configuration)\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT**: This notebook uses the **corrected configuration** that matches EstraNet for fair comparison.\n",
        "\n",
        "## Key Configuration ‚úÖ\n",
        "- **Loss Function**: Cross-Entropy (not Focal Loss)\n",
        "- **Learning Rate**: 2.5e-4 (not 2e-3)\n",
        "- **Batch Size**: 256 train / 32 eval (not 64)\n",
        "- **Training**: 100k steps (not 50-100 epochs)\n",
        "- **Optimizer**: Adam (not AdamW)\n",
        "- **Scheduler**: Cosine Decay (not OneCycleLR)\n",
        "- **Evaluation**: 100-trial Guessing Entropy (not single trial)\n",
        "\n",
        "## DO NOT USE `final_best_gnn_mamba_teacher.ipynb` ‚ùå\n",
        "That notebook has incorrect configuration and cannot be fairly compared with EstraNet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267e0826",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "267e0826",
        "outputId": "9a406334-416b-4124-f557-affa67119db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'EstraNet'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects: 100% (310/310), done.\u001b[K\n",
            "remote: Compressing objects: 100% (215/215), done.\u001b[K\n",
            "remote: Total 310 (delta 132), reused 271 (delta 93), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (310/310), 32.22 MiB | 16.21 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n",
            "‚úÖ Clean! Directory: /content/EstraNet\n"
          ]
        }
      ],
      "source": [
        "import os, shutil\n",
        "\n",
        "os.chdir('/content')\n",
        "if os.path.exists('EstraNet'):\n",
        "    shutil.rmtree('EstraNet')  # Remove nested mess\n",
        "\n",
        "!git clone https://github.com/loshithan/EstraNet.git\n",
        "os.chdir('EstraNet')\n",
        "print(f\"‚úÖ Clean! Directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0001",
      "metadata": {
        "id": "cell_0001"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0002",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_0002",
        "outputId": "089f7f4c-751f-4d05-a74d-6215998c39b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n",
            "\n",
            "‚úì Environment ready\n",
            "  Data path: data/ASCAD.h5\n",
            "  Checkpoint dir: checkpoints/mamba_gnn_estranet\n",
            "  Result dir: results\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Add paths\n",
        "sys.path.append(str(Path.cwd() / 'mamba-gnn-scripts'))\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = 'data/ASCAD.h5'\n",
        "CHECKPOINT_DIR = 'checkpoints/mamba_gnn_estranet'\n",
        "RESULT_DIR = 'results'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\n‚úì Environment ready\")\n",
        "print(f\"  Data path: {DATA_PATH}\")\n",
        "print(f\"  Checkpoint dir: {CHECKPOINT_DIR}\")\n",
        "print(f\"  Result dir: {RESULT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0003",
      "metadata": {
        "id": "cell_0003"
      },
      "source": [
        "## Configuration Comparison\n",
        "\n",
        "### ‚ùå Old Notebook (final_best_gnn_mamba_teacher.ipynb)\n",
        "```python\n",
        "# WRONG CONFIGURATION - DO NOT USE\n",
        "loss = FocalLoss(gamma=2.5)\n",
        "optimizer = AdamW(lr=2e-3)\n",
        "batch_size = 64\n",
        "epochs = 50-100\n",
        "scheduler = OneCycleLR(max_lr=2e-3)\n",
        "evaluation = single_trial_key_rank()\n",
        "```\n",
        "\n",
        "### ‚úÖ New Configuration (This Notebook)\n",
        "```python\n",
        "# CORRECT CONFIGURATION - Matches EstraNet\n",
        "loss = CrossEntropyLoss()\n",
        "optimizer = Adam(lr=2.5e-4)\n",
        "batch_size = 256 (train), 32 (eval)\n",
        "train_steps = 100000\n",
        "scheduler = CosineLRSchedule(max_lr=2.5e-4)\n",
        "evaluation = compute_ge_key_rank(num_trials=100)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0004",
      "metadata": {
        "id": "cell_0004"
      },
      "source": [
        "## Option 1: Train Using PyTorch Script (Recommended)\n",
        "\n",
        "This uses the corrected `train_mamba_gnn.py` script with EstraNet-aligned configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e13c143",
      "metadata": {
        "id": "4e13c143"
      },
      "source": [
        "## Choose Loss Function: Fair Comparison vs Performance\n",
        "\n",
        "**Two-Phase Experiment Strategy for Your Report:**\n",
        "\n",
        "### Phase 1Ô∏è‚É£ (FIRST): Fair Comparison with Cross-Entropy\n",
        "- **Purpose**: Compare Mamba-GNN vs EstraNet objectively (same configuration)\n",
        "- **Loss**: Cross-Entropy\n",
        "- **Results**: Document baseline performance\n",
        "- **Section in report**: \"Fair Comparison Analysis\"\n",
        "\n",
        "### Phase 2Ô∏è‚É£ (THEN): Best Performance with FocalLoss\n",
        "- **Purpose**: Show Mamba-GNN's maximum attack potential\n",
        "- **Loss**: FocalLoss (gamma=2.5)\n",
        "- **Results**: Document optimized performance\n",
        "- **Section in report**: \"Performance Analysis\" or \"Best-Case Scenario\"\n",
        "\n",
        "Toggle loss function below ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1df9b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1df9b8",
        "outputId": "be97c762-c709-4796-fef2-0a53340a8319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PHASE 1Ô∏è‚É£: FAIR COMPARISON MODE\n",
            "================================================================================\n",
            "\n",
            "‚úì Loss Function: Cross-Entropy\n",
            "‚úì Purpose: Compare Mamba-GNN vs EstraNet with same configuration\n",
            "‚úì Results: Document in 'Fair Comparison Analysis' section\n",
            "\n",
            "‚úì Configuration:\n",
            "  data_path           : data/ASCAD.h5\n",
            "  checkpoint_dir      : checkpoints/mamba_gnn_estranet\n",
            "  target_byte         : 2\n",
            "  train_batch_size    : 256\n",
            "  eval_batch_size     : 32\n",
            "  train_steps         : 100000\n",
            "  learning_rate       : 0.00025\n",
            "  d_model             : 128\n",
            "  mamba_layers        : 4\n",
            "  gnn_layers          : 3\n",
            "  k_neighbors         : 8\n",
            "  dropout             : 0.1\n",
            "  iterations          : 500\n",
            "  eval_steps          : 500\n",
            "  save_steps          : 10000\n",
            "  clip                : 0.25\n",
            "  warmup_steps        : 1000\n",
            "  loss_function       : cross_entropy\n"
          ]
        }
      ],
      "source": [
        "# ========== SWITCH LOSS FUNCTION HERE ==========\n",
        "# Choose one: \"cross_entropy\" or \"focal_loss\"\n",
        "LOSS_FUNCTION = \"cross_entropy\"  # ‚Üê Change to \"focal_loss\" for Phase 2\n",
        "# ===============================================\n",
        "\n",
        "# Training configuration (EstraNet-aligned + Anti-Overfitting Regularization)\n",
        "config = {\n",
        "    'data_path': DATA_PATH,\n",
        "    'checkpoint_dir': CHECKPOINT_DIR,\n",
        "    'target_byte': 2,\n",
        "    'train_batch_size': 256,\n",
        "    'eval_batch_size': 32,\n",
        "    'train_steps': 25000,        # ‚úÖ Reduced from 100000 to 25000\n",
        "    'learning_rate': 2.5e-4,\n",
        "    'd_model': 64,              # ‚ö†Ô∏è REDUCED from 128 to prevent overfitting\n",
        "    'mamba_layers': 2,          # ‚ö†Ô∏è REDUCED from 4 to prevent overfitting\n",
        "    'gnn_layers': 2,            # ‚ö†Ô∏è REDUCED from 3 to prevent overfitting\n",
        "    'k_neighbors': 8,\n",
        "    'dropout': 0.3,             # ‚ö†Ô∏è INCREASED from 0.1 to prevent overfitting\n",
        "    'iterations': 500,\n",
        "    'eval_steps': 250,          # ‚úÖ More frequent evaluation to detect overfitting early\n",
        "    'save_steps': 5000,         # ‚úÖ Checkpoints every 5k steps\n",
        "    'clip': 0.25,\n",
        "    'warmup_steps': 1000,\n",
        "    'loss_function': LOSS_FUNCTION,\n",
        "    \n",
        "    # NEW: Anti-overfitting regularization\n",
        "    'weight_decay': 0.01,       # L2 regularization\n",
        "    'label_smoothing': 0.1,     # Soften targets\n",
        "    'early_stopping': 5,        # ‚ö†Ô∏è Reduced patience to react quicker (eval periods)\n",
        "    'augment_noise': 0.05,      # Gaussian noise std\n",
        "    'augment_shift': 3,         # Max random time shift\n",
        "}\n",
        "\n",
        "# Loss function specific settings\n",
        "if LOSS_FUNCTION == \"cross_entropy\":\n",
        "    config['loss_config'] = {\n",
        "        'type': 'CrossEntropyLoss',\n",
        "        'description': 'Fair comparison with EstraNet',\n",
        "    }\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 1Ô∏è‚É£: FAIR COMPARISON MODE (WITH REGULARIZATION)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\n‚úì Loss Function: Cross-Entropy (with label smoothing 0.1)\")\n",
        "    print(\"‚úì Regularization: weight_decay=0.01, dropout=0.3\")\n",
        "    print(\"‚úì Data Augmentation: noise=0.05, shift=3\")\n",
        "    print(\"‚úì Model Capacity: REDUCED (d_model=64, mamba=2, gnn=2)\")\n",
        "    print(\"‚úì Early Stopping: After 5 eval periods without improvement\")\n",
        "    print(\"\\n‚ö†Ô∏è Previous training showed SEVERE OVERFITTING:\")\n",
        "    print(\"   Train loss ‚Üí 0, Eval loss ‚Üí 22 (massive gap)\")\n",
        "    print(\"   These changes should prevent that.\")\n",
        "\n",
        "print(\"\\n‚úì Configuration:\")\n",
        "for key, value in config.items():\n",
        "    if key != 'loss_config':\n",
        "        print(f\"  {key:20s}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716c2455",
      "metadata": {
        "id": "716c2455"
      },
      "source": [
        "## Report Structure: How to Document Results\n",
        "\n",
        "### Section 1: Fair Comparison Analysis (Phase 1Ô∏è‚É£)\n",
        "```markdown\n",
        "## Fair Comparison: Mamba-GNN vs EstraNet\n",
        "\n",
        "### Configuration Match\n",
        "- Loss: Cross-Entropy (same as EstraNet) ‚úì\n",
        "- Learning Rate: 2.5e-4 (same as EstraNet) ‚úì\n",
        "- Batch Size: 256 (same as EstraNet) ‚úì\n",
        "- Optimizer: Adam with Cosine schedule ‚úì\n",
        "- Evaluation: 100-trial Guessing Entropy ‚úì\n",
        "\n",
        "### Results\n",
        "| Model | Key Rank @ 100 traces | Key Rank @ 1000 traces | Recovery Point |\n",
        "|-------|----------------------|----------------------|--------|\n",
        "| EstraNet Transformer | X | Y | Z traces |\n",
        "| Mamba-GNN | X | Y | Z traces |\n",
        "\n",
        "### Conclusion\n",
        "Fair comparison shows [which model is better/comparable]\n",
        "```\n",
        "\n",
        "### Section 2: Performance Analysis (Phase 2Ô∏è‚É£)\n",
        "```markdown\n",
        "## Performance Optimization: Mamba-GNN with FocalLoss\n",
        "\n",
        "### Configuration\n",
        "- Loss: FocalLoss (gamma=2.5) - optimized for attack\n",
        "- Same training setup otherwise ‚úì\n",
        "\n",
        "### Results\n",
        "| Model | Loss | Key Rank @ 100 traces | Key Rank @ 1000 traces |\n",
        "|-------|------|----------------------|----------------------|\n",
        "| Mamba-GNN (Cross-Entropy) | Baseline | X | Y |\n",
        "| Mamba-GNN (FocalLoss) | Optimized | X' | Y' |\n",
        "\n",
        "### Improvement\n",
        "FocalLoss improves performance by [X%] at key recovery\n",
        "```\n",
        "\n",
        "üëá Execute cells below to gather data for both sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0006",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_0006",
        "outputId": "95126683-2be0-4220-8bff-a53f903fa790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training command (cross_entropy):\n",
            "python mamba-gnn-scripts/train_mamba_gnn.py     --data_path=data/ASCAD.h5     --checkpoint_dir=checkpoints/mamba_gnn_estranet     --target_byte=2     --train_batch_size=256     --eval_batch_size=32     --train_steps=100000     --learning_rate=0.00025     --d_model=128     --mamba_layers=4     --gnn_layers=3     --k_neighbors=8     --dropout=0.1     --do_train\n",
            "\n",
            "\n",
            "‚úì Run the cell below to start training\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Use absolute script path to ensure the edited script is executed\n",
        "script_path = os.path.abspath('mamba-gnn-scripts/train_mamba_gnn.py')\n",
        "\n",
        "# Base args (include all regularization flags we expect)\n",
        "base_args = {\n",
        "    'data_path': config['data_path'],\n",
        "    'checkpoint_dir': config['checkpoint_dir'],\n",
        "    'target_byte': config['target_byte'],\n",
        "    'train_batch_size': config['train_batch_size'],\n",
        "    'eval_batch_size': config['eval_batch_size'],\n",
        "    'train_steps': config['train_steps'],\n",
        "    'eval_steps': config['eval_steps'],\n",
        "    'save_steps': config['save_steps'],\n",
        "    'learning_rate': config['learning_rate'],\n",
        "    'd_model': config['d_model'],\n",
        "    'mamba_layers': config['mamba_layers'],\n",
        "    'gnn_layers': config['gnn_layers'],\n",
        "    'k_neighbors': config['k_neighbors'],\n",
        "    'dropout': config['dropout'],\n",
        "    'weight_decay': config.get('weight_decay'),\n",
        "    'label_smoothing': config.get('label_smoothing'),\n",
        "    'early_stopping': config.get('early_stopping'),\n",
        "    'augment_noise': config.get('augment_noise'),\n",
        "    'augment_shift': config.get('augment_shift')\n",
        "}\n",
        "\n",
        "# Generate CLI string helper\n",
        "def build_cli(script, args, allowed_flags=None):\n",
        "    parts = [f'python \"{script}\"']\n",
        "    for k, v in args.items():\n",
        "        flag = f\"--{k}\"\n",
        "        if allowed_flags is not None and flag not in allowed_flags:\n",
        "            continue\n",
        "        if v is None:\n",
        "            continue\n",
        "        parts.append(f\"{flag}={v}\")\n",
        "    parts.append('--do_train')\n",
        "    return ' \\\\\\n    '.join(parts)\n",
        "\n",
        "# Probe script --help to see which flags are supported by the runtime script\n",
        "help_proc = subprocess.run(['python', script_path, '--help'], capture_output=True, text=True)\n",
        "help_text = (help_proc.stdout or '') + (help_proc.stderr or '')\n",
        "\n",
        "# Determine allowed flags from help text\n",
        "expected_flags = [f\"--{k}\" for k in base_args.keys()]\n",
        "allowed_flags = set()\n",
        "for flag in expected_flags:\n",
        "    if flag in help_text:\n",
        "        allowed_flags.add(flag)\n",
        "\n",
        "# If some expected flags are missing, warn and build a cleaned command\n",
        "missing = [f for f in expected_flags if f not in allowed_flags]\n",
        "if missing:\n",
        "    print(\"‚ö†Ô∏è The training script's --help is missing these flags:\", missing)\n",
        "    print(\"   Building command without unsupported flags so training can run.\")\n",
        "\n",
        "train_cmd = build_cli(script_path, base_args, allowed_flags)\n",
        "\n",
        "print(f\"Training command ({config['loss_function']}):\")\n",
        "print(train_cmd)\n",
        "print('\\n‚úì Run the cell below to start training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdadd6e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdadd6e5",
        "outputId": "6b7618c5-7b94-4744-a788-4fcc2cfaeaf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Running: Phase 1Ô∏è‚É£ (Fair Comparison)\n",
            "================================================================================\n",
            "\n",
            "‚úì Checkpoints saved to: checkpoints/mamba_gnn_estranet\n",
            "‚úì Results saved to: results/cross_entropy_eval.txt\n",
            "\n",
            "Switch between phases by changing: LOSS_FUNCTION = '...'\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup checkpoint directories for both phases\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create phase-specific directories\n",
        "# FIX: Use the existing directory for Phase 1 as requested\n",
        "PHASE1_CKPT = 'checkpoints/mamba_gnn_estranet'  # Cross-Entropy\n",
        "PHASE2_CKPT = 'checkpoints/mamba_gnn_phase2_focal'  # FocalLoss\n",
        "\n",
        "os.makedirs(PHASE1_CKPT, exist_ok=True)\n",
        "os.makedirs(PHASE2_CKPT, exist_ok=True)\n",
        "\n",
        "# Determine which checkpoint dir to use\n",
        "if config['loss_function'] == \"cross_entropy\":\n",
        "    ACTIVE_CKPT = PHASE1_CKPT\n",
        "    PHASE_NAME = \"Phase 1Ô∏è‚É£ (Fair Comparison)\"\n",
        "else:\n",
        "    ACTIVE_CKPT = PHASE2_CKPT\n",
        "    PHASE_NAME = \"Phase 2Ô∏è‚É£ (Performance)\"\n",
        "\n",
        "config['checkpoint_dir'] = ACTIVE_CKPT\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Running: {PHASE_NAME}\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\n‚úì Checkpoints saved to: {ACTIVE_CKPT}\")\n",
        "print(f\"‚úì Results saved to: {RESULT_DIR}/{config['loss_function']}_eval.txt\")\n",
        "print(f\"\\nSwitch between phases by changing: LOSS_FUNCTION = '...'\")\n",
        "print(f\"{'='*80}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0007",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cell_0007",
        "outputId": "3af5044a-5ee0-42db-f961-717583dbad7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Starting Mamba-GNN Training (EstraNet-Aligned Configuration)\n",
            "================================================================================\n",
            "\n",
            "‚úì Command: python mamba-gnn-scripts/train_mamba_gnn.py     --data_path=data/ASCAD.h5     --checkpoint_dir=checkpoints/mamba_gnn_estranet     --target_byte=2     --train_batch_size=256     --eval_batch_size=32     --train_steps=100000     --learning_rate=0.00025     --d_model=128     --mamba_layers=4     --gnn_layers=3     --k_neighbors=8     --dropout=0.1     --do_train\n",
            "\n",
            "\n",
            "Monitoring training progress...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "CONFIGURATION (EstraNet-aligned)\n",
            "================================================================================\n",
            "data_path:         data/ASCAD.h5\n",
            "target_byte:       2\n",
            "input_length:      700\n",
            "train_batch_size:  256\n",
            "eval_batch_size:   32\n",
            "train_steps:       100000\n",
            "learning_rate:     0.00025\n",
            "d_model:           128\n",
            "mamba_layers:      4\n",
            "gnn_layers:        3\n",
            "checkpoint_dir:    checkpoints/mamba_gnn_estranet\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "MAMBA-GNN TRAINING (EstraNet-aligned configuration)\n",
            "================================================================================\n",
            "\n",
            "Device: cuda\n",
            "Loading ASCAD from: data/ASCAD.h5\n",
            "Training traces:  (50000, 700)\n",
            "Attack traces:    (10000, 700)\n",
            "Target byte:      2\n",
            "Optimized Mamba-GNN:\n",
            "  d_model: 128, Mamba layers: 4, GNN layers: 3\n",
            "  Input scaling: 0.1\n",
            "\n",
            "Model Parameters: 1,450,399\n",
            "Training batches per iteration: 196\n",
            "Total training steps: 100000\n",
            "Save checkpoints every: 10000 steps\n",
            "\n",
            "================================================================================\n",
            "Starting training...\n",
            "================================================================================\n",
            "\n",
            "[   500] | gnorm  1.12 lr  0.000125 | loss  5.58\n",
            "Train batches[  196]                | loss  5.54\n",
            "Eval  batches[  312]                | loss  5.56\n",
            "[  1000] | gnorm  0.64 lr  0.000250 | loss  5.55\n",
            "Train batches[  196]                | loss  5.52\n",
            "Eval  batches[  312]                | loss  5.56\n",
            "[  1500] | gnorm  1.05 lr  0.000250 | loss  5.52\n",
            "Train batches[  196]                | loss  5.48\n",
            "Eval  batches[  312]                | loss  5.58\n",
            "[  2000] | gnorm  2.10 lr  0.000250 | loss  5.47\n",
            "Train batches[  196]                | loss  5.35\n",
            "Eval  batches[  312]                | loss  5.65\n",
            "[  2500] | gnorm  2.94 lr  0.000250 | loss  5.35\n",
            "Train batches[  196]                | loss  5.15\n",
            "Eval  batches[  312]                | loss  5.68\n",
            "[  3000] | gnorm  4.57 lr  0.000250 | loss  5.17\n",
            "Train batches[  196]                | loss  4.92\n",
            "Eval  batches[  312]                | loss  5.76\n",
            "[  3500] | gnorm  5.58 lr  0.000250 | loss  4.95\n",
            "Train batches[  196]                | loss  4.59\n",
            "Eval  batches[  312]                | loss  5.85\n",
            "[  4000] | gnorm  7.27 lr  0.000249 | loss  4.68\n",
            "Train batches[  196]                | loss  4.16\n",
            "Eval  batches[  312]                | loss  6.04\n",
            "[  4500] | gnorm  9.01 lr  0.000249 | loss  4.40\n",
            "Train batches[  196]                | loss  3.77\n",
            "Eval  batches[  312]                | loss  6.28\n",
            "[  5000] | gnorm 10.96 lr  0.000249 | loss  4.06\n",
            "Train batches[  196]                | loss  3.38\n",
            "Eval  batches[  312]                | loss  6.61\n",
            "[  5500] | gnorm 13.01 lr  0.000249 | loss  3.77\n",
            "Train batches[  196]                | loss  2.94\n",
            "Eval  batches[  312]                | loss  6.96\n",
            "[  6000] | gnorm 14.02 lr  0.000248 | loss  3.44\n",
            "Train batches[  196]                | loss  2.55\n",
            "Eval  batches[  312]                | loss  7.34\n",
            "[  6500] | gnorm 13.88 lr  0.000248 | loss  3.18\n",
            "Train batches[  196]                | loss  2.23\n",
            "Eval  batches[  312]                | loss  7.74\n",
            "[  7000] | gnorm 17.32 lr  0.000248 | loss  2.93\n",
            "Train batches[  196]                | loss  1.94\n",
            "Eval  batches[  312]                | loss  8.11\n",
            "[  7500] | gnorm 19.12 lr  0.000247 | loss  2.69\n",
            "Train batches[  196]                | loss  1.73\n",
            "Eval  batches[  312]                | loss  8.52\n",
            "[  8000] | gnorm 18.52 lr  0.000247 | loss  2.50\n",
            "Train batches[  196]                | loss  1.43\n",
            "Eval  batches[  312]                | loss  8.92\n",
            "[  8500] | gnorm 20.50 lr  0.000246 | loss  2.31\n",
            "Train batches[  196]                | loss  1.31\n",
            "Eval  batches[  312]                | loss  9.31\n",
            "[  9000] | gnorm 19.69 lr  0.000246 | loss  2.15\n",
            "Train batches[  196]                | loss  1.06\n",
            "Eval  batches[  312]                | loss  9.66\n",
            "[  9500] | gnorm 21.84 lr  0.000245 | loss  1.98\n",
            "Train batches[  196]                | loss  0.93\n",
            "Eval  batches[  312]                | loss 10.11\n",
            "[ 10000] | gnorm 19.83 lr  0.000245 | loss  1.88\n",
            "Train batches[  196]                | loss  0.85\n",
            "Eval  batches[  312]                | loss 10.37\n",
            "Model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-10000.pth\n",
            "[ 10500] | gnorm 21.69 lr  0.000244 | loss  1.72\n",
            "Train batches[  196]                | loss  0.70\n",
            "Eval  batches[  312]                | loss 10.72\n",
            "[ 11000] | gnorm 20.87 lr  0.000244 | loss  1.64\n",
            "Train batches[  196]                | loss  0.61\n",
            "Eval  batches[  312]                | loss 11.06\n",
            "[ 11500] | gnorm 23.78 lr  0.000243 | loss  1.52\n",
            "Train batches[  196]                | loss  0.52\n",
            "Eval  batches[  312]                | loss 11.37\n",
            "[ 12000] | gnorm 18.39 lr  0.000242 | loss  1.44\n",
            "Train batches[  196]                | loss  0.46\n",
            "Eval  batches[  312]                | loss 11.64\n",
            "[ 12500] | gnorm 19.10 lr  0.000242 | loss  1.35\n",
            "Train batches[  196]                | loss  0.37\n",
            "Eval  batches[  312]                | loss 11.96\n",
            "[ 13000] | gnorm 20.77 lr  0.000241 | loss  1.28\n",
            "Train batches[  196]                | loss  0.30\n",
            "Eval  batches[  312]                | loss 12.08\n",
            "[ 13500] | gnorm 21.28 lr  0.000240 | loss  1.22\n",
            "Train batches[  196]                | loss  0.28\n",
            "Eval  batches[  312]                | loss 12.40\n",
            "[ 14000] | gnorm 20.69 lr  0.000240 | loss  1.15\n",
            "Train batches[  196]                | loss  0.31\n",
            "Eval  batches[  312]                | loss 12.77\n",
            "[ 14500] | gnorm 19.49 lr  0.000239 | loss  1.11\n",
            "Train batches[  196]                | loss  0.26\n",
            "Eval  batches[  312]                | loss 12.92\n",
            "[ 15000] | gnorm 18.09 lr  0.000238 | loss  1.04\n",
            "Train batches[  196]                | loss  0.21\n",
            "Eval  batches[  312]                | loss 13.14\n",
            "[ 15500] | gnorm 20.57 lr  0.000237 | loss  1.02\n",
            "Train batches[  196]                | loss  0.19\n",
            "Eval  batches[  312]                | loss 13.40\n",
            "[ 16000] | gnorm 21.28 lr  0.000236 | loss  0.95\n",
            "Train batches[  196]                | loss  0.16\n",
            "Eval  batches[  312]                | loss 13.56\n",
            "[ 16500] | gnorm 18.15 lr  0.000235 | loss  0.92\n",
            "Train batches[  196]                | loss  0.12\n",
            "Eval  batches[  312]                | loss 13.68\n",
            "[ 17000] | gnorm 22.14 lr  0.000234 | loss  0.88\n",
            "Train batches[  196]                | loss  0.11\n",
            "Eval  batches[  312]                | loss 13.92\n",
            "[ 17500] | gnorm 16.36 lr  0.000233 | loss  0.84\n",
            "Train batches[  196]                | loss  0.13\n",
            "Eval  batches[  312]                | loss 14.10\n",
            "[ 18000] | gnorm 18.51 lr  0.000232 | loss  0.82\n",
            "Train batches[  196]                | loss  0.10\n",
            "Eval  batches[  312]                | loss 14.12\n",
            "[ 18500] | gnorm 21.63 lr  0.000231 | loss  0.78\n",
            "Train batches[  196]                | loss  0.11\n",
            "Eval  batches[  312]                | loss 14.40\n",
            "[ 19000] | gnorm 20.86 lr  0.000230 | loss  0.77\n",
            "Train batches[  196]                | loss  0.10\n",
            "Eval  batches[  312]                | loss 14.59\n",
            "[ 19500] | gnorm 17.00 lr  0.000229 | loss  0.72\n",
            "Train batches[  196]                | loss  0.09\n",
            "Eval  batches[  312]                | loss 14.79\n",
            "[ 20000] | gnorm 16.50 lr  0.000228 | loss  0.71\n",
            "Train batches[  196]                | loss  0.07\n",
            "Eval  batches[  312]                | loss 14.97\n",
            "Model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-20000.pth\n",
            "[ 20500] | gnorm 19.86 lr  0.000227 | loss  0.68\n",
            "Train batches[  196]                | loss  0.08\n",
            "Eval  batches[  312]                | loss 15.11\n",
            "[ 21000] | gnorm 15.46 lr  0.000226 | loss  0.66\n",
            "Train batches[  196]                | loss  0.06\n",
            "Eval  batches[  312]                | loss 15.18\n",
            "[ 21500] | gnorm 18.40 lr  0.000225 | loss  0.63\n",
            "Train batches[  196]                | loss  0.08\n",
            "Eval  batches[  312]                | loss 15.37\n",
            "[ 22000] | gnorm 17.62 lr  0.000223 | loss  0.62\n",
            "Train batches[  196]                | loss  0.05\n",
            "Eval  batches[  312]                | loss 15.42\n",
            "[ 22500] | gnorm 19.34 lr  0.000222 | loss  0.61\n",
            "Train batches[  196]                | loss  0.04\n",
            "Eval  batches[  312]                | loss 15.58\n",
            "[ 23000] | gnorm 17.54 lr  0.000221 | loss  0.58\n",
            "Train batches[  196]                | loss  0.04\n",
            "Eval  batches[  312]                | loss 15.64\n",
            "[ 23500] | gnorm 18.29 lr  0.000220 | loss  0.57\n",
            "Train batches[  196]                | loss  0.05\n",
            "Eval  batches[  312]                | loss 15.75\n",
            "[ 24000] | gnorm 16.84 lr  0.000218 | loss  0.55\n",
            "Train batches[  196]                | loss  0.04\n",
            "Eval  batches[  312]                | loss 15.81\n",
            "[ 24500] | gnorm 34.88 lr  0.000217 | loss  0.54\n",
            "Train batches[  196]                | loss  0.03\n",
            "Eval  batches[  312]                | loss 16.00\n",
            "[ 25000] | gnorm 16.16 lr  0.000216 | loss  0.52\n",
            "Train batches[  196]                | loss  0.02\n",
            "Eval  batches[  312]                | loss 16.08\n",
            "[ 25500] | gnorm 13.58 lr  0.000214 | loss  0.51\n",
            "Train batches[  196]                | loss  0.03\n",
            "Eval  batches[  312]                | loss 16.21\n",
            "[ 26000] | gnorm 19.10 lr  0.000213 | loss  0.49\n",
            "Train batches[  196]                | loss  0.04\n",
            "Eval  batches[  312]                | loss 16.47\n",
            "[ 26500] | gnorm 17.16 lr  0.000211 | loss  0.48\n",
            "Train batches[  196]                | loss  0.03\n",
            "Eval  batches[  312]                | loss 16.46\n",
            "[ 27000] | gnorm 13.84 lr  0.000210 | loss  0.46\n",
            "Train batches[  196]                | loss  0.03\n",
            "Eval  batches[  312]                | loss 16.62\n",
            "[ 27500] | gnorm 13.58 lr  0.000209 | loss  0.45\n",
            "Train batches[  196]                | loss  0.03\n",
            "Eval  batches[  312]                | loss 16.73\n",
            "[ 28000] | gnorm 16.96 lr  0.000207 | loss  0.44\n",
            "Train batches[  196]                | loss  0.02\n",
            "Eval  batches[  312]                | loss 16.68\n",
            "[ 28500] | gnorm 16.40 lr  0.000206 | loss  0.42\n",
            "Train batches[  196]                | loss  0.02\n",
            "Eval  batches[  312]                | loss 16.88\n",
            "[ 29000] | gnorm 18.10 lr  0.000204 | loss  0.42\n",
            "Train batches[  196]                | loss  0.02\n",
            "Eval  batches[  312]                | loss 16.90\n",
            "[ 29500] | gnorm 13.61 lr  0.000202 | loss  0.41\n",
            "Train batches[  196]                | loss  0.02\n",
            "Eval  batches[  312]                | loss 17.04\n",
            "[ 30000] | gnorm 13.94 lr  0.000201 | loss  0.40\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.14\n",
            "Model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-30000.pth\n",
            "[ 30500] | gnorm 17.84 lr  0.000199 | loss  0.39\n",
            "Train batches[  196]                | loss  0.02\n",
            "Eval  batches[  312]                | loss 17.33\n",
            "[ 31000] | gnorm 14.80 lr  0.000198 | loss  0.38\n",
            "Train batches[  196]                | loss  0.02\n",
            "Eval  batches[  312]                | loss 17.35\n",
            "[ 31500] | gnorm 15.47 lr  0.000196 | loss  0.37\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.40\n",
            "[ 32000] | gnorm 13.22 lr  0.000194 | loss  0.36\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.59\n",
            "[ 32500] | gnorm 13.56 lr  0.000193 | loss  0.36\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.58\n",
            "[ 33000] | gnorm 13.94 lr  0.000191 | loss  0.34\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.68\n",
            "[ 33500] | gnorm 14.86 lr  0.000189 | loss  0.34\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.75\n",
            "[ 34000] | gnorm 15.05 lr  0.000188 | loss  0.33\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.84\n",
            "[ 34500] | gnorm 12.96 lr  0.000186 | loss  0.32\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.96\n",
            "[ 35000] | gnorm 13.65 lr  0.000184 | loss  0.31\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 17.97\n",
            "[ 35500] | gnorm 12.73 lr  0.000183 | loss  0.31\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 18.10\n",
            "[ 36000] | gnorm 14.64 lr  0.000181 | loss  0.30\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 18.19\n",
            "[ 36500] | gnorm 10.71 lr  0.000179 | loss  0.29\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 18.27\n",
            "[ 37000] | gnorm 12.44 lr  0.000177 | loss  0.29\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 18.36\n",
            "[ 37500] | gnorm 10.14 lr  0.000175 | loss  0.28\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 18.42\n",
            "[ 38000] | gnorm 15.00 lr  0.000174 | loss  0.28\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 18.52\n",
            "[ 38500] | gnorm 10.39 lr  0.000172 | loss  0.27\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 18.52\n",
            "[ 39000] | gnorm 12.92 lr  0.000170 | loss  0.27\n",
            "Train batches[  196]                | loss  0.01\n",
            "Eval  batches[  312]                | loss 18.63\n",
            "[ 39500] | gnorm 11.20 lr  0.000168 | loss  0.26\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 18.69\n",
            "[ 40000] | gnorm 13.81 lr  0.000166 | loss  0.25\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 18.69\n",
            "Model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-40000.pth\n",
            "[ 40500] | gnorm 11.91 lr  0.000164 | loss  0.25\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 18.91\n",
            "[ 41000] | gnorm 13.53 lr  0.000162 | loss  0.24\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 18.82\n",
            "[ 41500] | gnorm 10.94 lr  0.000161 | loss  0.24\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.06\n",
            "[ 42000] | gnorm 10.49 lr  0.000159 | loss  0.23\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 18.98\n",
            "[ 42500] | gnorm 13.50 lr  0.000157 | loss  0.23\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.17\n",
            "[ 43000] | gnorm  9.94 lr  0.000155 | loss  0.23\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.12\n",
            "[ 43500] | gnorm 13.73 lr  0.000153 | loss  0.21\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.20\n",
            "[ 44000] | gnorm 13.31 lr  0.000151 | loss  0.21\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.31\n",
            "[ 44500] | gnorm  8.74 lr  0.000149 | loss  0.21\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.49\n",
            "[ 45000] | gnorm 12.65 lr  0.000147 | loss  0.20\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.40\n",
            "[ 45500] | gnorm 12.09 lr  0.000145 | loss  0.20\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.51\n",
            "[ 46000] | gnorm 13.93 lr  0.000143 | loss  0.19\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.66\n",
            "[ 46500] | gnorm 13.34 lr  0.000141 | loss  0.19\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.71\n",
            "[ 47000] | gnorm 10.33 lr  0.000139 | loss  0.19\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.77\n",
            "[ 47500] | gnorm 10.68 lr  0.000137 | loss  0.18\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.83\n",
            "[ 48000] | gnorm 12.24 lr  0.000135 | loss  0.18\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.80\n",
            "[ 48500] | gnorm 12.31 lr  0.000133 | loss  0.17\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 19.96\n",
            "[ 49000] | gnorm 24.57 lr  0.000131 | loss  0.17\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.01\n",
            "[ 49500] | gnorm  9.85 lr  0.000129 | loss  0.16\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.11\n",
            "[ 50000] | gnorm  9.61 lr  0.000127 | loss  0.16\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.23\n",
            "Model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-50000.pth\n",
            "[ 50500] | gnorm 10.46 lr  0.000126 | loss  0.16\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.22\n",
            "[ 51000] | gnorm 10.42 lr  0.000124 | loss  0.16\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.27\n",
            "[ 51500] | gnorm 10.15 lr  0.000122 | loss  0.15\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.29\n",
            "[ 52000] | gnorm  9.19 lr  0.000120 | loss  0.15\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.39\n",
            "[ 52500] | gnorm 13.21 lr  0.000118 | loss  0.15\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.46\n",
            "[ 53000] | gnorm 11.09 lr  0.000116 | loss  0.14\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.43\n",
            "[ 53500] | gnorm  8.83 lr  0.000114 | loss  0.14\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.59\n",
            "[ 54000] | gnorm 11.03 lr  0.000112 | loss  0.13\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.58\n",
            "[ 54500] | gnorm  8.17 lr  0.000110 | loss  0.13\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.71\n",
            "[ 55000] | gnorm  9.48 lr  0.000108 | loss  0.13\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.82\n",
            "[ 55500] | gnorm 11.10 lr  0.000106 | loss  0.13\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.83\n",
            "[ 56000] | gnorm 11.74 lr  0.000104 | loss  0.13\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.88\n",
            "[ 56500] | gnorm  8.26 lr  0.000102 | loss  0.12\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 20.95\n",
            "[ 57000] | gnorm 10.36 lr  0.000100 | loss  0.12\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.04\n",
            "[ 57500] | gnorm  9.49 lr  0.000098 | loss  0.11\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.03\n",
            "[ 58000] | gnorm 10.77 lr  0.000096 | loss  0.11\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.23\n",
            "[ 58500] | gnorm  9.95 lr  0.000094 | loss  0.11\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.16\n",
            "[ 59000] | gnorm  8.91 lr  0.000092 | loss  0.11\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.24\n",
            "[ 59500] | gnorm  8.40 lr  0.000090 | loss  0.11\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.22\n",
            "[ 60000] | gnorm 10.67 lr  0.000089 | loss  0.10\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.33\n",
            "Model saved: checkpoints/mamba_gnn_estranet/mamba_gnn-60000.pth\n",
            "[ 60500] | gnorm  9.41 lr  0.000087 | loss  0.10\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.41\n",
            "[ 61000] | gnorm  5.14 lr  0.000085 | loss  0.10\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.43\n",
            "[ 61500] | gnorm  7.93 lr  0.000083 | loss  0.09\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.49\n",
            "[ 62000] | gnorm  6.50 lr  0.000081 | loss  0.09\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.52\n",
            "[ 62500] | gnorm  5.74 lr  0.000079 | loss  0.09\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.45\n",
            "[ 63000] | gnorm  7.84 lr  0.000077 | loss  0.09\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.59\n",
            "[ 63500] | gnorm  6.17 lr  0.000076 | loss  0.09\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.64\n",
            "[ 64000] | gnorm  8.06 lr  0.000074 | loss  0.08\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.86\n",
            "[ 64500] | gnorm  9.00 lr  0.000072 | loss  0.08\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.79\n",
            "[ 65000] | gnorm  5.63 lr  0.000070 | loss  0.08\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.72\n",
            "[ 65500] | gnorm  7.78 lr  0.000068 | loss  0.08\n",
            "Train batches[  196]                | loss  0.00\n",
            "Eval  batches[  312]                | loss 21.83\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2460448685.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Display output in real-time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Execute training with real-time logging and progress monitoring\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Create log file with timestamp\n",
        "log_dir = Path(\"logs\")\n",
        "log_dir.mkdir(exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = log_dir / f\"mamba_gnn_training_{timestamp}.log\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Starting Mamba-GNN Training (EstraNet-Aligned Configuration)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n‚úì Command: {train_cmd}\")\n",
        "print(f\"‚úì Log file: {log_file}\\n\")\n",
        "\n",
        "# Modify command to use unbuffered Python output\n",
        "unbuffered_cmd = train_cmd.replace(\"python \", \"python -u \")\n",
        "\n",
        "# Monitor checkpoint directory for training progress\n",
        "checkpoint_dir = Path(config['checkpoint_dir'])\n",
        "last_checkpoint_time = time.time()\n",
        "\n",
        "print(\"Monitoring training progress...\\n\")\n",
        "\n",
        "# Open log file for writing\n",
        "with open(log_file, 'w') as f:\n",
        "    # Write header\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(f\"Mamba-GNN Training Log - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(f\"Command: {train_cmd}\\n\\n\")\n",
        "    f.write(\"Configuration:\\n\")\n",
        "    for key, value in config.items():\n",
        "        if key != 'loss_config':\n",
        "            f.write(f\"  {key}: {value}\\n\")\n",
        "    f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "    f.write(\"Training Output:\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.flush()\n",
        "    \n",
        "    try:\n",
        "        # Start training process with unbuffered output\n",
        "        process = subprocess.Popen(\n",
        "            unbuffered_cmd,\n",
        "            shell=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "            bufsize=1,  # Line buffering\n",
        "        )\n",
        "\n",
        "        # Display output in real-time and log to file\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output:\n",
        "                line = output.rstrip()\n",
        "                print(line)\n",
        "                f.write(line + \"\\n\")\n",
        "                f.flush()  # Ensure immediate write\n",
        "\n",
        "            # Check if process finished\n",
        "            if process.poll() is not None:\n",
        "                break\n",
        "\n",
        "            # Monitor checkpoint creation\n",
        "            if checkpoint_dir.exists():\n",
        "                checkpoint_files = list(checkpoint_dir.glob(\"*.pth\"))\n",
        "                if checkpoint_files:\n",
        "                    latest = max(checkpoint_files, key=os.path.getctime)\n",
        "                    current_time = os.path.getctime(latest)\n",
        "                    if current_time > last_checkpoint_time:\n",
        "                        msg = f\"\\n‚úì Checkpoint saved: {latest.name}\"\n",
        "                        print(msg)\n",
        "                        f.write(msg + \"\\n\")\n",
        "                        f.flush()\n",
        "                        last_checkpoint_time = current_time\n",
        "\n",
        "        # Write final status\n",
        "        if process.returncode == 0:\n",
        "            msg = \"\\n\" + \"=\"*80 + \"\\n‚úì Training completed successfully!\\n\" + \"=\"*80\n",
        "            print(msg)\n",
        "            f.write(msg + \"\\n\")\n",
        "        else:\n",
        "            msg = \"\\n\" + \"=\"*80 + f\"\\n‚úó Training failed with error code: {process.returncode}\\n\" + \"=\"*80\n",
        "            print(msg)\n",
        "            f.write(msg + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"‚úó Error running training: {e}\"\n",
        "        print(msg)\n",
        "        f.write(msg + \"\\n\")\n",
        "\n",
        "print(f\"\\n‚úì Training log saved to: {log_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5970ac",
      "metadata": {
        "id": "ce5970ac"
      },
      "source": [
        "## Monitor Training Progress in Real-Time\n",
        "\n",
        "Run this cell **in a separate terminal** while training is running to watch checkpoints being saved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d79605",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59d79605",
        "outputId": "6a1adcc6-7127-4381-deff-67e73de77f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Training Progress Monitor\n",
            "================================================================================\n",
            "\n",
            "Watching: checkpoints/mamba_gnn_estranet\n",
            "Press Ctrl+C to stop monitoring\n",
            "\n",
            "‚úì [12:42:29] checkpoint_latest.pth (16.7 MB)\n",
            "‚úì [10:29:46] mamba_gnn-10000.pth (16.7 MB)\n",
            "‚úì [10:56:19] mamba_gnn-20000.pth (16.7 MB)\n",
            "‚úì [11:22:46] mamba_gnn-30000.pth (16.7 MB)\n",
            "‚úì [11:49:11] mamba_gnn-40000.pth (16.7 MB)\n",
            "‚úì [12:15:47] mamba_gnn-50000.pth (16.7 MB)\n",
            "‚úì [12:42:29] mamba_gnn-60000.pth (16.7 MB)\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "‚úì Total checkpoints: 7\n",
            "‚úì Total size: 117.2 MB\n",
            "\n",
            "\n",
            "‚úì Monitoring stopped\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "checkpoint_dir = Path(config['checkpoint_dir'])\n",
        "previous_files = set()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Training Progress Monitor\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nWatching: {checkpoint_dir}\")\n",
        "print(\"Press Ctrl+C to stop monitoring\\n\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        if checkpoint_dir.exists():\n",
        "            # Get all checkpoint files\n",
        "            current_files = set(checkpoint_dir.glob(\"*.pth\"))\n",
        "            new_files = current_files - previous_files\n",
        "\n",
        "            # Show new checkpoints\n",
        "            for ckpt_file in sorted(new_files):\n",
        "                file_size_mb = ckpt_file.stat().st_size / (1024 * 1024)\n",
        "                timestamp = datetime.fromtimestamp(ckpt_file.stat().st_mtime).strftime(\"%H:%M:%S\")\n",
        "                print(f\"‚úì [{timestamp}] {ckpt_file.name} ({file_size_mb:.1f} MB)\")\n",
        "\n",
        "            previous_files = current_files\n",
        "\n",
        "            # Check total size\n",
        "            if current_files:\n",
        "                total_size = sum(f.stat().st_size for f in current_files) / (1024 * 1024)\n",
        "                print(f\"\\n‚úì Total checkpoints: {len(current_files)}\")\n",
        "                print(f\"‚úì Total size: {total_size:.1f} MB\")\n",
        "            else:\n",
        "                print(\"‚è≥ Waiting for first checkpoint...\")\n",
        "        else:\n",
        "            print(\"‚è≥ Checkpoint directory not created yet...\")\n",
        "\n",
        "        # Wait before checking again\n",
        "        time.sleep(5)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n‚úì Monitoring stopped\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc78a7af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc78a7af",
        "outputId": "2a576f4d-6f0c-4011-f097-cdc89b11ce60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Training Progress Monitor\n",
            "================================================================================\n",
            "\n",
            "Watching: checkpoints/mamba_gnn_estranet\n",
            "Press Ctrl+C to stop monitoring\n",
            "\n",
            "‚úì [12:42:29] checkpoint_latest.pth (16.7 MB)\n",
            "‚úì [10:29:46] mamba_gnn-10000.pth (16.7 MB)\n",
            "‚úì [10:56:19] mamba_gnn-20000.pth (16.7 MB)\n",
            "‚úì [11:22:46] mamba_gnn-30000.pth (16.7 MB)\n",
            "‚úì [11:49:11] mamba_gnn-40000.pth (16.7 MB)\n",
            "‚úì [12:15:47] mamba_gnn-50000.pth (16.7 MB)\n",
            "‚úì [12:42:29] mamba_gnn-60000.pth (16.7 MB)\n",
            "\n",
            "Total checkpoints size: 117.2 MB\n",
            "Number of checkpoints: 7\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Total checkpoints size: 117.2 MB\n",
            "Number of checkpoints: 7\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Total checkpoints size: 117.2 MB\n",
            "Number of checkpoints: 7\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Total checkpoints size: 117.2 MB\n",
            "Number of checkpoints: 7\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Total checkpoints size: 117.2 MB\n",
            "Number of checkpoints: 7\n",
            "\n",
            "‚úì Monitoring stopped\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "checkpoint_dir = Path(config['checkpoint_dir'])\n",
        "previous_files = set()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Training Progress Monitor\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nWatching: {checkpoint_dir}\")\n",
        "print(\"Press Ctrl+C to stop monitoring\\n\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        if checkpoint_dir.exists():\n",
        "            # Get all checkpoint files\n",
        "            current_files = set(checkpoint_dir.glob(\"*.pth\"))\n",
        "            new_files = current_files - previous_files\n",
        "\n",
        "            # Show new checkpoints\n",
        "            for ckpt_file in sorted(new_files):\n",
        "                file_size_mb = ckpt_file.stat().st_size / (1024 * 1024)\n",
        "                timestamp = datetime.fromtimestamp(ckpt_file.stat().st_mtime).strftime(\"%H:%M:%S\")\n",
        "                print(f\"‚úì [{timestamp}] {ckpt_file.name} ({file_size_mb:.1f} MB)\")\n",
        "\n",
        "            previous_files = current_files\n",
        "\n",
        "            # Check total size\n",
        "            total_size = sum(f.stat().st_size for f in current_files) / (1024 * 1024)\n",
        "            print(f\"\\nTotal checkpoints size: {total_size:.1f} MB\")\n",
        "            print(f\"Number of checkpoints: {len(current_files)}\")\n",
        "        else:\n",
        "            print(\"‚ö† Checkpoint directory not found yet...\")\n",
        "\n",
        "        # Wait before checking again\n",
        "        time.sleep(5)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚úì Monitoring stopped\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0008",
      "metadata": {
        "id": "cell_0008"
      },
      "source": [
        "## Option 2: Train Using TensorFlow (for TFLite Conversion)\n",
        "\n",
        "If you need to deploy on mobile/edge devices, use the TensorFlow version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0009",
      "metadata": {
        "id": "cell_0009"
      },
      "outputs": [],
      "source": [
        "# # TensorFlow training command\n",
        "# tf_train_cmd = f\"\"\"python scripts/train_mamba_gnn_tf.py \\\n",
        "#     --data_path={config['data_path']} \\\n",
        "#     --checkpoint_dir=checkpoints/mamba_gnn_tf \\\n",
        "#     --target_byte={config['target_byte']} \\\n",
        "#     --train_batch_size={config['train_batch_size']} \\\n",
        "#     --eval_batch_size={config['eval_batch_size']} \\\n",
        "#     --train_steps={config['train_steps']} \\\n",
        "#     --learning_rate={config['learning_rate']} \\\n",
        "#     --d_model={config['d_model']} \\\n",
        "#     --do_train\n",
        "# \"\"\"\n",
        "\n",
        "# print(\"TensorFlow training command:\")\n",
        "# print(tf_train_cmd)\n",
        "# print(\"\\n‚úì This will automatically export to TFLite after training\")\n",
        "# print(\"  Output: checkpoints/mamba_gnn_tf/mamba_gnn.tflite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0010",
      "metadata": {
        "id": "cell_0010"
      },
      "outputs": [],
      "source": [
        "# Execute TensorFlow training (uncomment to run)\n",
        "# import subprocess\n",
        "# result = subprocess.run(tf_train_cmd.split(), capture_output=False)\n",
        "# print(f\"‚úì Training complete. TFLite model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0011",
      "metadata": {
        "id": "cell_0011"
      },
      "source": [
        "## Monitor Training Progress\n",
        "\n",
        "View training loss, learning rate, and gradient norms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0012",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_0012",
        "outputId": "ac3728e3-ad03-4f9c-ad9a-a117680f296a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö† No training history found. Start training first.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Load training history\n",
        "loss_path = Path(CHECKPOINT_DIR) / 'loss.pkl'\n",
        "\n",
        "if loss_path.exists():\n",
        "    with open(loss_path, 'rb') as f:\n",
        "        history = pickle.load(f)\n",
        "\n",
        "    steps = sorted(history.keys())\n",
        "    train_losses = [history[s]['train_loss'] for s in steps if 'train_loss' in history[s]]\n",
        "    lrs = [history[s]['lr'] for s in steps if 'lr' in history[s]]\n",
        "    grad_norms = [history[s]['grad_norm'] for s in steps if 'grad_norm' in history[s]]\n",
        "\n",
        "    # Plot training metrics\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Training loss\n",
        "    axes[0].plot(steps[:len(train_losses)], train_losses, 'b-', linewidth=2)\n",
        "    axes[0].set_xlabel('Training Steps', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss (Cross-Entropy)', fontsize=12)\n",
        "    axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning rate\n",
        "    axes[1].plot(steps[:len(lrs)], lrs, 'g-', linewidth=2)\n",
        "    axes[1].set_xlabel('Training Steps', fontsize=12)\n",
        "    axes[1].set_ylabel('Learning Rate', fontsize=12)\n",
        "    axes[1].set_title('Learning Rate Schedule (Cosine Decay)', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Gradient norm\n",
        "    axes[2].plot(steps[:len(grad_norms)], grad_norms, 'r-', linewidth=2)\n",
        "    axes[2].set_xlabel('Training Steps', fontsize=12)\n",
        "    axes[2].set_ylabel('Gradient Norm', fontsize=12)\n",
        "    axes[2].set_title('Gradient Norm (Clipped at 0.25)', fontsize=14, fontweight='bold')\n",
        "    axes[2].axhline(y=0.25, color='orange', linestyle='--', label='Clip threshold')\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{RESULT_DIR}/training_progress.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n‚úì Training Progress:\")\n",
        "    print(f\"  Total steps: {max(steps):,}\")\n",
        "    print(f\"  Latest loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"  Latest LR: {lrs[-1]:.6f}\")\n",
        "    print(f\"  Latest grad norm: {grad_norms[-1]:.4f}\")\n",
        "    print(f\"\\n  Plot saved: {RESULT_DIR}/training_progress.png\")\n",
        "else:\n",
        "    print(\"‚ö† No training history found. Start training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0013",
      "metadata": {
        "id": "cell_0013"
      },
      "source": [
        "## Evaluate Model (Guessing Entropy)\n",
        "\n",
        "Evaluate using 100-trial Guessing Entropy methodology (matches EstraNet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0014",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_0014",
        "outputId": "b4eab7e6-1d19-44e3-f7d9-43f512daa026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation command:\n",
            "python mamba-gnn-scripts/train_mamba_gnn.py     --data_path=data/ASCAD.h5     --checkpoint_dir=checkpoints/mamba_gnn_phase1_fair     --target_byte=2     --d_model=128     --mamba_layers=4     --gnn_layers=3     --k_neighbors=8     --dropout=0.1     --checkpoint_idx=100000     --result_path=results/mamba_gnn_eval\n",
            "\n",
            "\n",
            "‚úì This will compute 100-trial Guessing Entropy\n",
            "  (Takes ~10-15 minutes)\n"
          ]
        }
      ],
      "source": [
        "# Evaluation configuration\n",
        "checkpoint_idx = 100000  # Which checkpoint to evaluate (0 = latest)\n",
        "\n",
        "eval_cmd = f\"\"\"python mamba-gnn-scripts/train_mamba_gnn.py \\\n",
        "    --data_path={config['data_path']} \\\n",
        "    --checkpoint_dir={config['checkpoint_dir']} \\\n",
        "    --target_byte={config['target_byte']} \\\n",
        "    --d_model={config['d_model']} \\\n",
        "    --mamba_layers={config['mamba_layers']} \\\n",
        "    --gnn_layers={config['gnn_layers']} \\\n",
        "    --k_neighbors={config['k_neighbors']} \\\n",
        "    --dropout={config['dropout']} \\\n",
        "    --checkpoint_idx={checkpoint_idx} \\\n",
        "    --result_path={RESULT_DIR}/mamba_gnn_eval\n",
        "\"\"\"\n",
        "\n",
        "print(\"Evaluation command:\")\n",
        "print(eval_cmd)\n",
        "print(\"\\n‚úì This will compute 100-trial Guessing Entropy\")\n",
        "print(\"  (Takes ~10-15 minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0015",
      "metadata": {
        "id": "cell_0015"
      },
      "outputs": [],
      "source": [
        "# Execute evaluation\n",
        "import subprocess\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Evaluating Mamba-GNN (100-trial Guessing Entropy)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "result = subprocess.run(eval_cmd.split(), capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n‚úì Evaluation complete!\")\n",
        "else:\n",
        "    print(\"\\n‚úó Evaluation failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0016",
      "metadata": {
        "id": "cell_0016"
      },
      "source": [
        "## Plot Guessing Entropy Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0017",
      "metadata": {
        "id": "cell_0017"
      },
      "outputs": [],
      "source": [
        "# Load and plot evaluation results\n",
        "result_file = f'{RESULT_DIR}/mamba_gnn_eval.txt'\n",
        "\n",
        "if Path(result_file).exists():\n",
        "    with open(result_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        mean_ranks = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
        "        std_ranks = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
        "\n",
        "    traces = np.arange(1, len(mean_ranks) + 1)\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    # Main line\n",
        "    plt.plot(traces, mean_ranks, 'b-', linewidth=2.5, label='Mean Key Rank (GE)')\n",
        "\n",
        "    # Confidence interval\n",
        "    plt.fill_between(traces,\n",
        "                     mean_ranks - std_ranks,\n",
        "                     mean_ranks + std_ranks,\n",
        "                     alpha=0.3, color='blue', label='¬±1 Std Dev')\n",
        "\n",
        "    # Key recovered line\n",
        "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Key Recovered (Rank=0)')\n",
        "\n",
        "    # Labels and formatting\n",
        "    plt.xlabel('Number of Traces', fontsize=14)\n",
        "    plt.ylabel('Key Rank (Guessing Entropy)', fontsize=14)\n",
        "    plt.title('Mamba-GNN: Guessing Entropy Evaluation (100 trials)\\nEstraNet-Aligned Configuration',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12, loc='upper right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add milestones\n",
        "    milestones = [100, 500, 1000, 2000, 5000]\n",
        "    for m in milestones:\n",
        "        if m < len(mean_ranks):\n",
        "            plt.axvline(x=m, color='gray', linestyle=':', alpha=0.5)\n",
        "            plt.text(m, plt.ylim()[1] * 0.95, f'{m}',\n",
        "                    ha='center', fontsize=9, color='gray')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{RESULT_DIR}/guessing_entropy_curve.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"=\"*80)\n",
        "    print(\"GUESSING ENTROPY RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nTarget byte: {config['target_byte']}\")\n",
        "    print(f\"\\nKey Rank (Mean ¬± Std):\")\n",
        "    print(f\"  100 traces:   {mean_ranks[99]:.2f} ¬± {std_ranks[99]:.2f}\")\n",
        "    print(f\"  500 traces:   {mean_ranks[499]:.2f} ¬± {std_ranks[499]:.2f}\")\n",
        "    print(f\"  1000 traces:  {mean_ranks[999]:.2f} ¬± {std_ranks[999]:.2f}\")\n",
        "    if len(mean_ranks) >= 2000:\n",
        "        print(f\"  2000 traces:  {mean_ranks[1999]:.2f} ¬± {std_ranks[1999]:.2f}\")\n",
        "    if len(mean_ranks) >= 5000:\n",
        "        print(f\"  5000 traces:  {mean_ranks[4999]:.2f} ¬± {std_ranks[4999]:.2f}\")\n",
        "    if len(mean_ranks) >= 10000:\n",
        "        print(f\"  10000 traces: {mean_ranks[9999]:.2f} ¬± {std_ranks[9999]:.2f}\")\n",
        "\n",
        "    # Find recovery point\n",
        "    recovered_idx = np.where(mean_ranks == 0)[0]\n",
        "    if len(recovered_idx) > 0:\n",
        "        print(f\"\\n‚úì Key RECOVERED at {recovered_idx[0]+1} traces\")\n",
        "    else:\n",
        "        print(f\"\\n‚úó Key NOT recovered (best rank: {mean_ranks[-1]:.2f})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Plot saved: {RESULT_DIR}/guessing_entropy_curve.png\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"‚ö† No evaluation results found. Run evaluation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0018",
      "metadata": {
        "id": "cell_0018"
      },
      "source": [
        "## Compare with EstraNet\n",
        "\n",
        "Compare Mamba-GNN with EstraNet Transformer/GNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0019",
      "metadata": {
        "id": "cell_0019"
      },
      "outputs": [],
      "source": [
        "# Compare with EstraNet results\n",
        "estranet_result = 'results/estranet_transformer_eval.txt'  # Update this path\n",
        "\n",
        "compare_cmd = f\"\"\"python scripts/compare_results.py \\\n",
        "    --mamba_results={result_file} \\\n",
        "    --estranet_results={estranet_result} \\\n",
        "    --output={RESULT_DIR}/model_comparison.png\n",
        "\"\"\"\n",
        "\n",
        "print(\"Comparison command:\")\n",
        "print(compare_cmd)\n",
        "print(\"\\n‚úì Make sure you have EstraNet results first:\")\n",
        "print(\"  python scripts/train_trans.py --model_type=transformer --do_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0020",
      "metadata": {
        "id": "cell_0020"
      },
      "outputs": [],
      "source": [
        "# Execute comparison (uncomment when EstraNet results are ready)\n",
        "# import subprocess\n",
        "# result = subprocess.run(compare_cmd.split())\n",
        "# print(\"‚úì Comparison plot created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0021",
      "metadata": {
        "id": "cell_0021"
      },
      "source": [
        "## Load and Use Trained Model\n",
        "\n",
        "Load checkpoint for inference or further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0022",
      "metadata": {
        "id": "cell_0022"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import h5py\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import model\n",
        "sys.path.append('models')\n",
        "from mamba_gnn_model import OptimizedMambaGNN\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model (EstraNet-aligned configuration)\n",
        "model = OptimizedMambaGNN(\n",
        "    trace_length=700,\n",
        "    d_model=config['d_model'],\n",
        "    mamba_layers=config['mamba_layers'],\n",
        "    gnn_layers=config['gnn_layers'],\n",
        "    num_classes=256,\n",
        "    k_neighbors=config['k_neighbors'],\n",
        "    dropout=config['dropout']\n",
        ").to(device)\n",
        "\n",
        "# Load checkpoint\n",
        "ckpt_path = f\"{CHECKPOINT_DIR}/mamba_gnn-100000.pth\"\n",
        "\n",
        "if Path(ckpt_path).exists():\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    print(f\"\\n‚úì Model loaded successfully\")\n",
        "    print(f\"  Checkpoint: {ckpt_path}\")\n",
        "    print(f\"  Training step: {checkpoint['global_step']:,}\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Model ready for inference\")\n",
        "else:\n",
        "    print(f\"‚ö† Checkpoint not found: {ckpt_path}\")\n",
        "    print(\"  Train the model first or specify different checkpoint_idx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "300fc456",
      "metadata": {
        "id": "300fc456"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Define paths\n",
        "checkpoint_folder = 'checkpoints/mamba_gnn_estranet'\n",
        "output_filename = 'mamba_gnn_estranet_checkpoints'\n",
        "\n",
        "# Check if folder exists\n",
        "if os.path.exists(checkpoint_folder):\n",
        "    print(f\"Zipping {checkpoint_folder}...\")\n",
        "\n",
        "    # Create zip file\n",
        "    shutil.make_archive(output_filename, 'zip', checkpoint_folder)\n",
        "\n",
        "    # Download\n",
        "    zip_path = f\"{output_filename}.zip\"\n",
        "    print(f\"Downloading {zip_path} ({os.path.getsize(zip_path) / 1e6:.1f} MB)...\")\n",
        "    files.download(zip_path)\n",
        "else:\n",
        "    print(f\"Folder not found: {checkpoint_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0023",
      "metadata": {
        "id": "cell_0023"
      },
      "outputs": [],
      "source": [
        "# Example: Get predictions on attack traces\n",
        "if Path(ckpt_path).exists() and Path(DATA_PATH).exists():\n",
        "    print(\"Running inference on sample traces...\")\n",
        "\n",
        "    # Load attack data\n",
        "    with h5py.File(DATA_PATH, 'r') as f:\n",
        "        X_attack = f['Attack_traces/traces'][:100]  # First 100 traces\n",
        "        m_attack = f['Attack_traces/metadata'][:]\n",
        "\n",
        "    # Normalize (using same scaler as training)\n",
        "    with h5py.File(DATA_PATH, 'r') as f:\n",
        "        X_train_sample = f['Profiling_traces/traces'][:1000]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train_sample)\n",
        "    X_attack_norm = scaler.transform(X_attack)\n",
        "\n",
        "    # Get predictions\n",
        "    X_tensor = torch.FloatTensor(X_attack_norm).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_tensor)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "    predicted_classes = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "\n",
        "    print(f\"\\n‚úì Inference complete\")\n",
        "    print(f\"  Traces processed: {len(X_attack)}\")\n",
        "    print(f\"  Prediction shape: {probs.shape}\")\n",
        "    print(f\"  Top predicted classes: {predicted_classes[:10]}\")\n",
        "    print(f\"  Confidence (first trace): {probs[0].max().item():.4f}\")\n",
        "else:\n",
        "    print(\"‚ö† Model or data not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0024",
      "metadata": {
        "id": "cell_0024"
      },
      "source": [
        "## Summary & Configuration Verification\n",
        "\n",
        "**‚úÖ This notebook uses the CORRECT configuration:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0025",
      "metadata": {
        "id": "cell_0025"
      },
      "outputs": [],
      "source": [
        "# Verify configuration matches EstraNet\n",
        "print(\"=\"*80)\n",
        "print(\"CONFIGURATION VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "verification = {\n",
        "    'Loss Function': ('Cross-Entropy', '‚úì'),\n",
        "    'Learning Rate': ('2.5e-4', '‚úì'),\n",
        "    'Train Batch Size': ('256', '‚úì'),\n",
        "    'Eval Batch Size': ('32', '‚úì'),\n",
        "    'Training Steps': ('100,000', '‚úì'),\n",
        "    'Optimizer': ('Adam', '‚úì'),\n",
        "    'LR Schedule': ('Cosine Decay', '‚úì'),\n",
        "    'Model Dimension': ('128', '‚úì'),\n",
        "    'Dropout': ('0.1', '‚úì'),\n",
        "    'Gradient Clipping': ('0.25', '‚úì'),\n",
        "    'Evaluation Method': ('100-trial GE', '‚úì'),\n",
        "}\n",
        "\n",
        "print(\"\\nEstraNet Alignment Check:\")\n",
        "for param, (value, status) in verification.items():\n",
        "    print(f\"  {status} {param:25s}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úì ALL PARAMETERS MATCH ESTRANET\")\n",
        "print(\"‚úì FAIR COMPARISON GUARANTEED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è DO NOT USE: final_best_gnn_mamba_teacher.ipynb\")\n",
        "print(\"   That notebook has incompatible configuration:\")\n",
        "print(\"   - FocalLoss instead of Cross-Entropy\")\n",
        "print(\"   - Learning rate 8x too high\")\n",
        "print(\"   - Batch size 4x too small\")\n",
        "print(\"   - Single-trial evaluation instead of 100-trial GE\")\n",
        "\n",
        "print(\"\\n‚úì USE THIS NOTEBOOK for training and evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0026",
      "metadata": {
        "id": "cell_0026"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "### 1. Train Model\n",
        "Run the training cell above to start training with correct configuration.\n",
        "\n",
        "### 2. Monitor Progress\n",
        "Check training loss, learning rate, and gradient norms periodically.\n",
        "\n",
        "### 3. Evaluate Model\n",
        "After training, run evaluation to get Guessing Entropy curves.\n",
        "\n",
        "### 4. Compare with EstraNet\n",
        "Train EstraNet models and compare results using the comparison script.\n",
        "\n",
        "### 5. Expected Results\n",
        "- **After 100k steps**: Loss ~4.8-5.0\n",
        "- **Key recovery**: Within 1000-2000 traces\n",
        "- **Performance**: Comparable to EstraNet Transformer (~1200 traces)\n",
        "\n",
        "### Files Created\n",
        "- **Training script**: `mamba-gnn-scripts/train_mamba_gnn.py`\n",
        "- **TensorFlow script**: `scripts/train_mamba_gnn_tf.py`\n",
        "- **PowerShell runner**: `mamba-gnn-scripts/train_mamba_gnn.ps1`\n",
        "- **Bash runner**: `mamba-gnn-scripts/train_mamba_gnn.sh`\n",
        "- **Comparison tool**: `scripts/compare_results.py`\n",
        "- **This notebook**: `train_mamba_gnn_notebook.ipynb`\n",
        "\n",
        "### Documentation\n",
        "- **Quick start**: `QUICKSTART.md`\n",
        "- **TFLite guide**: `TENSORFLOW_TFLITE_GUIDE.md`\n",
        "- **TF implementation**: `TENSORFLOW_IMPLEMENTATION_SUMMARY.md`\n",
        "- **Config comparison**: `NOTEBOOK_VS_SCRIPT_COMPARISON.md`\n",
        "\n",
        "**Ready to train with fair comparison to EstraNet!** üéØ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b070cd09",
      "metadata": {
        "id": "b070cd09"
      },
      "source": [
        "## Compare Both Phases for Report\n",
        "\n",
        "After completing Phase 1Ô∏è‚É£ and Phase 2Ô∏è‚É£, use this to compare results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e98c964",
      "metadata": {
        "id": "1e98c964"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results from both phases\n",
        "phase1_result = f'{RESULT_DIR}/cross_entropy_eval.txt'\n",
        "phase2_result = f'{RESULT_DIR}/focal_loss_eval.txt'\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Phase 1: Fair Comparison\n",
        "if Path(phase1_result).exists():\n",
        "    with open(phase1_result, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        phase1_mean = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
        "        phase1_std = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
        "    results['Phase 1 (Cross-Entropy)'] = phase1_mean\n",
        "    print(\"‚úì Phase 1 results loaded\")\n",
        "else:\n",
        "    print(\"‚ö† Phase 1 results not found. Train Phase 1 first.\")\n",
        "\n",
        "# Phase 2: Performance\n",
        "if Path(phase2_result).exists():\n",
        "    with open(phase2_result, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        phase2_mean = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
        "        phase2_std = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
        "    results['Phase 2 (FocalLoss)'] = phase2_mean\n",
        "    print(\"‚úì Phase 2 results loaded\")\n",
        "else:\n",
        "    print(\"‚ö† Phase 2 results not found. Train Phase 2 first.\")\n",
        "\n",
        "# Compare key milestones\n",
        "if len(results) == 2:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON TABLE FOR REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    milestones = [100, 500, 1000, 2000, 5000]\n",
        "\n",
        "    print(f\"\\n{'Traces':<10} {'Phase 1 (CE)':<20} {'Phase 2 (Focal)':<20} {'Improvement'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for m in milestones:\n",
        "        if m <= len(phase1_mean):\n",
        "            p1_rank = phase1_mean[m-1]\n",
        "            p2_rank = phase2_mean[m-1]\n",
        "            improvement = ((p1_rank - p2_rank) / (p1_rank + 1e-6)) * 100\n",
        "\n",
        "            print(f\"{m:<10} {p1_rank:<20.2f} {p2_rank:<20.2f} {improvement:+.1f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    # Find recovery points\n",
        "    p1_recovery = np.where(phase1_mean == 0)[0]\n",
        "    p2_recovery = np.where(phase2_mean == 0)[0]\n",
        "\n",
        "    print(\"\\nKEY RECOVERY SUMMARY:\")\n",
        "    if len(p1_recovery) > 0:\n",
        "        print(f\"  Phase 1 (Cross-Entropy): {p1_recovery[0]+1} traces\")\n",
        "    else:\n",
        "        print(f\"  Phase 1 (Cross-Entropy): No recovery (best: {phase1_mean[-1]:.2f})\")\n",
        "\n",
        "    if len(p2_recovery) > 0:\n",
        "        print(f\"  Phase 2 (FocalLoss):     {p2_recovery[0]+1} traces\")\n",
        "    else:\n",
        "        print(f\"  Phase 2 (FocalLoss):     No recovery (best: {phase2_mean[-1]:.2f})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    # Plot comparison\n",
        "    traces = np.arange(1, min(len(phase1_mean), len(phase2_mean)) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(traces, phase1_mean[:len(traces)], 'b-', linewidth=2.5, label='Phase 1: Cross-Entropy (Fair)')\n",
        "    plt.plot(traces, phase2_mean[:len(traces)], 'r-', linewidth=2.5, label='Phase 2: FocalLoss (Optimized)')\n",
        "\n",
        "    plt.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    plt.xlabel('Number of Traces', fontsize=14)\n",
        "    plt.ylabel('Key Rank (Guessing Entropy)', fontsize=14)\n",
        "    plt.title('Mamba-GNN Comparison: Fair vs Optimized\\nfor Report', fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12, loc='upper right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{RESULT_DIR}/phase_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Comparison plot saved: {RESULT_DIR}/phase_comparison.png\")\n",
        "else:\n",
        "    print(\"‚ö† Complete both Phase 1 and Phase 2 to see comparison\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aacfe15",
   "metadata": {},
   "source": [
    "# Fine‑tune Mamba‑GNN with FocalLoss (warm‑start)\n",
    "\n",
    "This notebook fine‑tunes an existing Cross‑Entropy (CE) checkpoint with FocalLoss.\n",
    "- Warm‑start from your best CE checkpoint\n",
    "- Quick (10‑trial) GE for fast feedback and full (100‑trial) GE for final validation\n",
    "- Plots, checkpoint monitor and export utilities\n",
    "\n",
    "Run cells in order. Cells that actually start training/evaluation are disabled by default — change the RUN_* flags to True when ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports & environment\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd()\n",
    "CHECKPOINTS = ROOT / 'checkpoints'\n",
    "RESULTS = ROOT / 'results'\n",
    "LOGS = ROOT / 'logs'\n",
    "for p in (CHECKPOINTS, RESULTS, LOGS):\n",
    "    p.mkdir(exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Make sure local scripts are importable\n",
    "sys.path.append(str(ROOT / 'mamba-gnn-scripts'))\n",
    "sys.path.append(str(ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58982cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Fine‑tune configuration (edit as needed)\n",
    "SOURCE_CKPT = 'checkpoints/mamba_gnn_estranet/mamba_gnn-50000.pth'  # your best CE checkpoint\n",
    "TARGET_DIR  = 'checkpoints/mamba_gnn_finetune_from_ce'\n",
    "EXTRA_STEPS = 25000\n",
    "LEARNING_RATE = 1e-4\n",
    "FOCAL_GAMMA = 2.5\n",
    "FOCAL_ALPHA = 1.0\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "EVAL_BATCH_SIZE = 32\n",
    "SAVE_STEPS = 5000\n",
    "EVAL_STEPS = 250\n",
    "DROPOUT = 0.3\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Safety flags (cells are inert unless you flip these)\n",
    "RUN_PREPARE = False   # copy checkpoint + infer arch\n",
    "RUN_FINETUNE = False  # actually launch the fine‑tune job\n",
    "RUN_QUICK_GE = False  # 10‑trial GE check after first fine‑tuned ckpt\n",
    "RUN_FULL_GE = False   # 100‑trial GE for final validation\n",
    "\n",
    "print('Config ready — change flags to run steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Prepare target folder and infer architecture (dry run)\n",
    "import importlib.util\n",
    "\n",
    "spec = importlib.util.spec_from_file_location('finetune_wrapper', Path('mamba-gnn-scripts') / 'finetune_mamba_focal.py')\n",
    "fw = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(fw)\n",
    "\n",
    "src = Path(SOURCE_CKPT)\n",
    "tgt = Path(TARGET_DIR)\n",
    "\n",
    "print('Source exists:', src.exists())\n",
    "if RUN_PREPARE:\n",
    "    ck = fw.copy_ckpt_to_target(src, tgt)\n",
    "    print(f'Copied to {tgt}/checkpoint_latest.pth')\n",
    "    src_step = int(ck.get('global_step', 0) or 0)\n",
    "    print('Source global_step:', src_step)\n",
    "    try:\n",
    "        d_model, mamba_layers, gnn_layers = fw.infer_arch_from_state(ck.get('model_state_dict') or ck)\n",
    "        print('Inferred arch ->', d_model, mamba_layers, gnn_layers)\n",
    "    except Exception as e:\n",
    "        print('Arch inference failed — using defaults (d_model=64,mamba=2,gnn=2)')\n",
    "else:\n",
    "    print('RUN_PREPARE is False — set True to copy checkpoint and infer architecture')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Launch fine‑tune (warm_start -> continues from source global_step)\n",
    "import subprocess\n",
    "\n",
    "if RUN_FINETUNE:\n",
    "    # build command using wrapper helper to ensure consistent args\n",
    "    total_steps = None  # let wrapper compute from source + EXTRA_STEPS\n",
    "    cmd = [\n",
    "        sys.executable, 'mamba-gnn-scripts/finetune_mamba_focal.py',\n",
    "        f'--source_ckpt={SOURCE_CKPT}',\n",
    "        f'--target_dir={TARGET_DIR}',\n",
    "        f'--extra_steps={EXTRA_STEPS}',\n",
    "        f'--learning_rate={LEARNING_RATE}',\n",
    "        f'--focal_gamma={FOCAL_GAMMA}',\n",
    "        f'--focal_alpha={FOCAL_ALPHA}',\n",
    "        f'--train_batch_size={TRAIN_BATCH_SIZE}',\n",
    "        f'--eval_batch_size={EVAL_BATCH_SIZE}',\n",
    "        f'--save_steps={SAVE_STEPS}',\n",
    "        f'--eval_steps={EVAL_STEPS}',\n",
    "        f'--dropout={DROPOUT}',\n",
    "        f'--weight_decay={WEIGHT_DECAY}'\n",
    "    ]\n",
    "\n",
    "    print('Launching fine‑tune command:')\n",
    "    print(' '.join(map(str, cmd)))\n",
    "    # run as subprocess (streams to notebook stdout)\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    try:\n",
    "        for line in proc.stdout:\n",
    "            print(line, end='')\n",
    "    except KeyboardInterrupt:\n",
    "        proc.terminate()\n",
    "        print('\\nCancelled by user')\n",
    "    print('\\nFine‑tune process exited with code', proc.returncode)\n",
    "else:\n",
    "    print('RUN_FINETUNE is False — set True to run the fine‑tune job from this cell')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Checkpoint monitor (one-shot)\n",
    "from pathlib import Path\n",
    "\n",
    "def latest_ckpt_info(ckpt_dir):\n",
    "    p = Path(ckpt_dir)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    ckpts = sorted(p.glob('*.pth'), key=lambda x: x.stat().st_mtime)\n",
    "    if not ckpts:\n",
    "        return None\n",
    "    latest = ckpts[-1]\n",
    "    size_mb = latest.stat().st_size / (1024*1024)\n",
    "    return latest.name, size_mb, latest.stat().st_mtime\n",
    "\n",
    "info = latest_ckpt_info(TARGET_DIR)\n",
    "if info:\n",
    "    print('Latest checkpoint:', info)\n",
    "else:\n",
    "    print('No checkpoints found in', TARGET_DIR)\n",
    "\n",
    "print('\\nTip: re-run this cell while training to refresh the latest checkpoint info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1506be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Quick GE (10‑trial) or Full GE (100‑trial) evaluator\n",
    "# This imports training utilities and runs GE evaluation on the latest checkpoint in TARGET_DIR\n",
    "import importlib.util\n",
    "\n",
    "spec = importlib.util.spec_from_file_location('train_mod', Path('mamba-gnn-scripts') / 'train_mamba_gnn.py')\n",
    "train_mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(train_mod)\n",
    "\n",
    "from train_mod import OptimizedMambaGNN, ASCADDataset, load_ascad_data, evaluate_model_ge\n",
    "\n",
    "def run_ge(ckpt_path, num_trials=10, max_traces=10000):\n",
    "    print('Loading checkpoint:', ckpt_path)\n",
    "    ck = torch.load(str(ckpt_path), map_location='cpu')\n",
    "    msd = ck.get('model_state_dict') or ck\n",
    "\n",
    "    # infer d_model (classifier weight or pos_encoding)\n",
    "    if 'classifier.0.weight' in msd:\n",
    "        d_model = msd['classifier.0.weight'].shape[1]\n",
    "    elif 'pos_encoding' in msd:\n",
    "        d_model = msd['pos_encoding'].shape[2]\n",
    "    else:\n",
    "        d_model = 64\n",
    "\n",
    "    # simple layer inference (best‑effort)\n",
    "    mamba_layers = len({k.split('.')[1] for k in msd.keys() if k.startswith('mamba_blocks.')}) or 2\n",
    "    gnn_layers = len({k.split('.')[1] for k in msd.keys() if k.startswith('gnn_layers.')}) or 2\n",
    "\n",
    "    print('Instantiating model ->', d_model, mamba_layers, gnn_layers)\n",
    "    model = OptimizedMambaGNN(trace_length=700, d_model=d_model, mamba_layers=mamba_layers, gnn_layers=gnn_layers, num_classes=256, k_neighbors=8, dropout=0.3)\n",
    "    model.load_state_dict(msd)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # load attack data\n",
    "    _, _, X_attack, y_attack, m_attack = load_ascad_data('data/ASCAD.h5', target_byte=2)\n",
    "    attack_dataset = ASCADDataset(X_attack, y_attack)\n",
    "    attack_loader = torch.utils.data.DataLoader(attack_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    mean_ranks, std_ranks = evaluate_model_ge(model, attack_loader, m_attack, 2, device, num_trials=num_trials, max_traces=max_traces)\n",
    "    return mean_ranks, std_ranks\n",
    "\n",
    "# Run quick/full GE when flags set\n",
    "from pathlib import Path\n",
    "ckpt = Path(TARGET_DIR) / 'checkpoint_latest.pth'\n",
    "if not ckpt.exists():\n",
    "    # fallback: pick latest saved .pth\n",
    "    ckpts = sorted(Path(TARGET_DIR).glob('mamba_gnn-*.pth'), key=lambda p: p.stat().st_mtime)\n",
    "    ckpt = ckpts[-1] if ckpts else None\n",
    "\n",
    "if ckpt is None:\n",
    "    print('No checkpoint found to evaluate. Run fine‑tune first or ensure TARGET_DIR is correct.')\n",
    "else:\n",
    "    if RUN_QUICK_GE:\n",
    "        print('\\nRunning QUICK GE (10 trials)')\n",
    "        mean10, std10 = run_ge(ckpt, num_trials=10)\n",
    "        out = Path('results') / f'finetune_quick_{ckpt.stem}.txt'\n",
    "        with open(out,'w') as f:\n",
    "            f.write('\\t'.join(map(str,mean10)) + '\\n')\n",
    "            f.write('\\t'.join(map(str,std10)) + '\\n')\n",
    "        print('Saved quick GE ->', out)\n",
    "\n",
    "    if RUN_FULL_GE:\n",
    "        print('\\nRunning FULL GE (100 trials) — this will take time')\n",
    "        mean100, std100 = run_ge(ckpt, num_trials=100)\n",
    "        out = Path('results') / f'finetune_full_{ckpt.stem}.txt'\n",
    "        with open(out,'w') as f:\n",
    "            f.write('\\t'.join(map(str,mean100)) + '\\n')\n",
    "            f.write('\\t'.join(map(str,std100)) + '\\n')\n",
    "        print('Saved full GE ->', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df965c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Plot GE results (if present)\n",
    "from pathlib import Path\n",
    "\n",
    "# prefer full GE result if available\n",
    "full_files = sorted(Path('results').glob('finetune_full_*.txt'))\n",
    "quick_files = sorted(Path('results').glob('finetune_quick_*.txt'))\n",
    "\n",
    "res_file = full_files[-1] if full_files else (quick_files[-1] if quick_files else None)\n",
    "\n",
    "if res_file:\n",
    "    with open(res_file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        mean = np.array([float(x) for x in lines[0].strip().split('\\t')])\n",
    "        std  = np.array([float(x) for x in lines[1].strip().split('\\t')])\n",
    "    traces = np.arange(1, len(mean)+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(traces, mean, label='Mean GE')\n",
    "    plt.fill_between(traces, mean-std, mean+std, alpha=0.25)\n",
    "    plt.axhline(0, color='red', linestyle='--', label='Recovered')\n",
    "    plt.xlabel('Traces'); plt.ylabel('Key rank (GE)')\n",
    "    plt.title(f'Guessing Entropy — {res_file.name}')\n",
    "    plt.legend(); plt.grid(True)\n",
    "    out = Path('results') / f'ge_plot_{res_file.stem}.png'\n",
    "    plt.savefig(out, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved plot ->', out)\n",
    "else:\n",
    "    print('No GE result files found in results/. Run quick/full GE first.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01503bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Export checkpoints (zip)\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "zip_path = Path.cwd() / 'mamba_gnn_finetune_from_ce_checkpoints'\n",
    "if Path(TARGET_DIR).exists():\n",
    "    shutil.make_archive(str(zip_path), 'zip', root_dir=str(Path(TARGET_DIR)))\n",
    "    print('Created:', str(zip_path)+'.zip')\n",
    "else:\n",
    "    print('Target checkpoint folder not found:', TARGET_DIR)\n",
    "\n",
    "print('Use VS Code or Colab file downloader to fetch the zip file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b93135",
   "metadata": {},
   "source": [
    "# 9) Notes & recommended workflow\n",
    "\n",
    "# Recommended quick flow:\n",
    "# 1) Set RUN_PREPARE=True and run the \"Prepare\" cell to copy the CE checkpoint\n",
    "# 2) Set RUN_FINETUNE=True and run the \"Launch fine‑tune\" cell to start warm‑start training (use GPU)\n",
    "# 3) After first save_steps complete, set RUN_QUICK_GE=True and run the \"Quick GE\" cell\n",
    "# 4) If quick GE is promising, set RUN_FULL_GE=True and run the full GE cell\n",
    "\n",
    "print('Notebook ready — edit config flags above and run cells in order')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
